#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass book
\use_default_options false
\maintain_unincluded_children false
\language spanish
\language_package default
\inputencoding utf8
\fontencoding T1
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 12
\spacing single
\use_hyperref true
\pdf_bookmarks false
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref section
\pdf_pdfusetitle false
\papersize a4paper
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 2
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style unsrtnat
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\headheight 16pt
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style french
\dynamic_quotes 0
\papercolumns 1
\papersides 2
\paperpagestyle empty
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Chapter
Estado del Arte
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "chap:estado-del-arte"

\end_inset


\end_layout

\begin_layout Standard
En este capítulo se expondrá el estado actual de la detección de tumores
 en ecografías mamarias, desde el prisma de las arquitecturas de las redes
 neuronales artificiales.
 Para obtener una visión de conjunto holística que permita el entendimiento
 de lo general y lo particular se realiza un estudio sobre los avances en
 el campo más global (
\emph on
dentro del campo de la Inteligencia Artificial
\emph default
), el denominado como Visión Artificial, hasta el correspondiente al trabajo,
 detección de tumores en ecografías mamarias.
 
\end_layout

\begin_layout Section
Visión Artificial
\begin_inset CommandInset label
LatexCommand label
name "sec:Visión-Artificial"

\end_inset


\end_layout

\begin_layout Standard
En el campo de la Visión Artificial se pueden establecer tres principales
 vías de desarrollo en cuestión de arquitecturas aplicables a multitud de
 casos de uso.
 Desde los primeros hitos de las arquitecturas de redes neuronales como
 fueron las redes convolucionales (
\begin_inset CommandInset citation
LatexCommand citet
key "LeCun1989,LeCun1995,Krizhevsky2012"
literal "false"

\end_inset

) y la arquitectura tipo U-Net (
\begin_inset CommandInset citation
LatexCommand citet
key "Ronneberger2015"
literal "false"

\end_inset

) los métodos han ido actualizándose de varias formas.
 
\end_layout

\begin_layout Standard
En primer lugar, se debe mencionar la utilización de los bloques de atención
 que desde su invención en 2017 (
\begin_inset CommandInset citation
LatexCommand citet
key "Vaswani2017"
literal "false"

\end_inset

) se han propagado a todos los campos de la Inteligencia Artificial.
\end_layout

\begin_layout Standard
En el contexto de la Visión Artificial hay multitud de usos de esta metodología,
 desde transformers de visión que asemejan los utilizados en texto (
\begin_inset CommandInset citation
LatexCommand citet
key "Dosovitskiy2020,Xie2021,Cao2023,Zhang2023a"
literal "false"

\end_inset

) a otros con estructura piramidal o jerárquico (
\begin_inset CommandInset citation
LatexCommand citet
key "Li2018,Cao2023,Hatamizadeh2023,Wu2024"
literal "false"

\end_inset

).
 El mecanismo de atención utilizado por estas arquitecturas favorece una
 convergencia de gran precisión debido a la capacidad de enfocarse en regiones
 de interés y tener un campo receptivo que cubre la imagen al completo,
 además añade interpretabilidad a la arquitectura por medio de los 
\begin_inset Quotes eld
\end_inset

pesos de la atención
\begin_inset Quotes erd
\end_inset

.
 En contraposición, aumenta en gran medida el coste computacional necesario,
 restringiendo el uso de estas redes e incrementando la huella de carbono.
 
\end_layout

\begin_layout Standard
En segundo lugar, se debe comentar la multimodalidad y los modelos 
\begin_inset Quotes eld
\end_inset


\emph on
para todo
\emph default

\begin_inset Quotes erd
\end_inset

.
 
\end_layout

\begin_layout Standard
Por una parte, la multimodalidad consiste en implementar arquitecturas capaces
 de consumir y predecir diferentes tipos de datos (imagen, texto, audio,
 ...) por medio de la unión de 
\begin_inset Quotes eld
\end_inset


\emph on
Encoders
\emph default

\begin_inset Quotes erd
\end_inset

 y 
\begin_inset Quotes eld
\end_inset


\emph on
Decoders
\emph default

\begin_inset Quotes erd
\end_inset

 pre-entrenados de cada tipo en un mapa de características intermedio (
\begin_inset CommandInset citation
LatexCommand citet
key "Akkus2023,Park2023,Shang2024"
literal "false"

\end_inset

).
\end_layout

\begin_layout Standard
Por otra parte, los modelos 
\begin_inset Quotes eld
\end_inset


\emph on
para todo
\emph default

\begin_inset Quotes erd
\end_inset

 comúnmente denominados modelos fundacionales que tratan de abordar todos
 los problemas de un mismo caso de uso, i.e.
 
\emph on
Segment Anything
\emph default
 (
\begin_inset CommandInset citation
LatexCommand citet
key "Kirillov2023"
literal "false"

\end_inset

).
 
\end_layout

\begin_layout Standard
Estas vías de desarrollo quedan fuera del alcance del individuo medio y
 quedan restringidas al uso por grandes empresas con todo tipo de recursos
 (
\emph on
datos, computación, personal
\emph default
) debido a que son los modelos más pesados y costosos de entrenar.
 La finalidad de estos modelos es obtener una herramienta multi-propósito,
 pero a costa de esta generalidad disminuyen la precisión en casos concretos
 por lo que requieren de metodologías adicionales como el Ajuste Fino (
\emph on
Fine Tuning
\emph default
).
 
\end_layout

\begin_layout Standard
Finalmente, se está avanzando hacia la eficiencia -
\emph on
en este y todos los demás campos de la Inteligencia Artificial
\emph default
- de las arquitecturas para reducir el coste computacional sin (mucha) pérdida
 de efectividad.
 
\end_layout

\begin_layout Standard
Esta vía es la que más tiempo lleva en desarrollo pues desde los inicios
 se ha querido optimizar la solución (
\begin_inset CommandInset citation
LatexCommand citet
key "Szegedy2015,Szegedy2016,Szegedy2017,8578814"
literal "false"

\end_inset

), llegando al punto actual en el que las redes neuronales eficientes pueden
 ser utilizadas en dispositivos móviles , 
\emph on
MobileNet
\emph default
 (
\begin_inset CommandInset citation
LatexCommand citet
key "Howard2017"
literal "false"

\end_inset

), y otros con bajo nivel de recursos.
 
\end_layout

\begin_layout Standard
El trabajo actual hace uso de una de las últimas actualizaciones de esta
 vía, EFSNet (
\begin_inset CommandInset citation
LatexCommand citet
key "Hu2020"
literal "false"

\end_inset

), que sigue la línea de ENet (
\begin_inset CommandInset citation
LatexCommand citet
key "Paszke2016"
literal "false"

\end_inset

) y ShuffleNet (
\begin_inset CommandInset citation
LatexCommand citet
key "8578814,Ma2018"
literal "false"

\end_inset

) para obtener una arquitectura con 
\begin_inset Formula $\simeq0.18$
\end_inset

 millones de parámetros y 
\begin_inset Formula $\simeq143$
\end_inset

 Mb de peso en inferencia.
 
\end_layout

\begin_layout Standard
Evidentemente, hay multitud de otras vías de desarrollo como el uso de grafos
 (
\begin_inset CommandInset citation
LatexCommand citet
key "Bretto2005,Yang2020"
literal "false"

\end_inset

), los modelos de mezcla de expertos, 
\emph on
MoE,
\emph default
 (
\begin_inset CommandInset citation
LatexCommand citet
key "Pavlitskaya2020"
literal "false"

\end_inset

); arquitecturas piramidales (
\begin_inset CommandInset citation
LatexCommand citet
key "He2019"
literal "false"

\end_inset

) o las metodologías 
\emph on
profesor-aprendiz
\emph default
 (
\begin_inset CommandInset citation
LatexCommand citet
key "Guo2023,Huo2023"
literal "false"

\end_inset

) en las que no se hace hincapié dado que no han presentado mejoras evidentes
 en los últimos años.
 Independientemente, son abordajes dignos de mención ya que pueden conformar
 la nueva punta de lanza en un futuro inmediato.
\end_layout

\begin_layout Standard
De igual forma, se deben mencionar los avances en algoritmos de optimización
 e inferencia de las arquitecturas de red neuronal.
 Estos avances cuentan con menor grado de eco mediático y, en multitud de
 ocasiones, son confundidos con la arquitectura.
 
\end_layout

\begin_layout Standard
Primeramente, se debe mencionar la difusión guiada (
\begin_inset CommandInset citation
LatexCommand citet
key "Ho2020"
literal "false"

\end_inset

).
 Esta metodología (compatible con otras) constituye un algoritmo iterativo
 de eliminación de ruido que, partiendo de una imagen semilla compuesta
 en su totalidad por ruido blanco, elimina, paso a paso -
\emph on
inferencia a inferencia
\emph default
-, el ruido para obtener la imagen final.
 Esta eliminación de ruido está guiada por un conjunto de predictores que
 evaluarán si el ruido eliminado se acerca o no al objetivo.
 Comúnmente se hace uso de bloques de atención.
 En consecuencia se obtiene una imagen nítida y acorde con el objetivo pero
 se incrementa el coste computacional tantas veces como pasos de eliminación
 de ruido se utilicen.
 Generalmente, esta vía se restringe a la Inteligencia Artificial Generativa
 (
\begin_inset CommandInset citation
LatexCommand citet
key "Betker2023,Esser2024"
literal "false"

\end_inset

).
 
\end_layout

\begin_layout Standard
En segundo lugar, se continúa investigando y mejorando en la metodología
 
\emph on

\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "GAN"
description "Generative Adversarial Networks"
literal "false"

\end_inset

GAN
\emph default
 (
\begin_inset CommandInset citation
LatexCommand citet
key "Goodfellow2014"
literal "false"

\end_inset

), redes generativas adversarias, que se componen de una red 
\begin_inset Quotes eld
\end_inset

generadora
\begin_inset Quotes erd
\end_inset

 y una 
\begin_inset Quotes eld
\end_inset

discriminadora
\begin_inset Quotes erd
\end_inset

 de forma que la primera debe generar la imagen y la segunda debe decidir
 si ésta es una imagen correcta o no.
 Es un buen método para subrogar la función de pérdida por medio de la utilizaci
ón de una red neuronal simple 
\emph on
-discriminadora-
\emph default
, pero complejiza la implementación de soluciones al tener que optimizar
 simultáneamente dos modelos.
 Las 
\emph on
GANs
\emph default
 (
\begin_inset CommandInset citation
LatexCommand citet
key "Goodfellow2014"
literal "false"

\end_inset

) se han utilizado para todo tipo de casos y es una de las metodologías
 más comunes en problemas de visión artificial.
 Si bien la red 
\begin_inset Quotes eld
\end_inset

discriminadora
\begin_inset Quotes erd
\end_inset

 es simple, la 
\begin_inset Quotes eld
\end_inset

generadora
\begin_inset Quotes erd
\end_inset

 puede ser todo lo compleja que se quiera por lo que utilizar esta vía para
 subrogar la pérdida aumenta el coste de la solución para una misma arquitectura
 
\begin_inset Quotes eld
\end_inset

generadora
\begin_inset Quotes erd
\end_inset

.
 
\end_layout

\begin_layout Standard
En tercer lugar, la utilización de pasos de aprendizaje variables (
\begin_inset CommandInset citation
LatexCommand citet
key "Defazio2023"
literal "false"

\end_inset

) y/o decaimiento en los pesos (
\begin_inset CommandInset citation
LatexCommand citet
key "Krogh1991"
literal "false"

\end_inset

) añade flexibilidad en el entrenamiento que evita que la optimización converja
 a un mínimo local aunque, ciertas configuraciones pueden impactar negativamente
 en el tiempo requerido para el entrenamiento.
 
\end_layout

\begin_layout Standard
En cuarto lugar, mencionar el Aprendizaje Federado (
\begin_inset CommandInset citation
LatexCommand citet
key "Bharati2022"
literal "false"

\end_inset

, que se alza como una metodología novedosa en el entrenamiento de redes
 neuronales descentralizado, distribuido y colaborativo.
 Esta metodología permite a un conjunto de entidades compartir un modelo
 sin compartir los datos y, en los últimos tiempos, ha permitido implementar
 modelos de alta precisión en campos en los que las entidades por sí mismas
 no cuentan con la cantidad de datos necesaria para el entrenamiento pero
 sí en conjunto.
 Esta vía de desarrollo es prometedora ya que añade una capa de seguridad
 en los datos (
\begin_inset CommandInset citation
LatexCommand citet
key "BuenestadoCortes2022"
literal "false"

\end_inset

) y reduce la huella de carbono al deduplicar modelos.
 
\end_layout

\begin_layout Standard
Por último, una metodología de optimización altamente utilizada es el ajuste
 fino, 
\emph on
Fine Tuning
\emph default
 (
\begin_inset CommandInset citation
LatexCommand citet
key "Hinton2006"
literal "false"

\end_inset

), que permite ajustar modelos fundacionales de propósito general -
\emph on
y
\emph default
 
\emph on
otros
\emph default
- a casos específicos como en el caso de MedSAM (
\begin_inset CommandInset citation
LatexCommand citet
key "Ma2024"
literal "false"

\end_inset

).
 El ajuste fino no reduce la complejidad de la arquitectura, es decir, su
 coste computacional.
 La ventaja es que la computación de la propagación hacia atrás únicamente
 se realiza sobre la/las últimas capas de la red neuronal, pero igualmente
 se debe contar con los recursos para realizar la inferencia del modelo
 original.
\end_layout

\begin_layout Standard
Las líneas de investigación comentadas pueden combinarse entre sí.
 Varias arquitecturas del estado del arte hacen uso de atención en la arquitectu
ra U-Net (
\begin_inset CommandInset citation
LatexCommand citet
key "Oktay2018"
literal "false"

\end_inset

), o utilizan la difusión para optimizar los resultados de arquitecturas
 ya conocidas (
\begin_inset CommandInset citation
LatexCommand citet
key "Tan2023"
literal "false"

\end_inset

), o reducen los parámetros de bloques convolucionales en el 
\begin_inset Quotes eld
\end_inset

generador
\begin_inset Quotes erd
\end_inset

 de una 
\emph on
GAN
\emph default
 (
\begin_inset CommandInset citation
LatexCommand citet
key "Sarker2021"
literal "false"

\end_inset

).
 Es decir, estos avances no son independientes y es necesario obtener una
 visión general del estado del arte para escoger con tino la metodología
 para un proyecto concreto.
 Entendiéndose el proyecto como el conjunto de datos, modelo y despliegue,
 se deberán ajustar todas las metodologías basándose en los avances actuales.
\end_layout

\begin_layout Section
Segmentación semántica en aplicaciones biomédicas
\begin_inset CommandInset label
LatexCommand label
name "sec:Segmentación-semántica-en"

\end_inset


\end_layout

\begin_layout Standard
El campo de la biomedicina es tremendamente extenso así como lo son las
 fuentes de datos.
\end_layout

\begin_layout Standard
Los principales campos de investigación están relacionados con Tomografía
 Computerizada (
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "CT"
description "Computerized Tomography"
literal "false"

\end_inset

CT), Imágenes de Resonancia Magnética (
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "MRI"
description "Magnetic Resonance Imaging"
literal "false"

\end_inset

MRI), Imágenes de Ultrasonido (
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "USI"
description "UltraSound Imaging"
literal "false"

\end_inset

USI), Rayos X, Tomografía de Coherencia Óptica (OCT
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "OCT"
description "Optical Coherence Tomography"
literal "false"

\end_inset

), Tomografía por Emisión de Positrones (
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "PET"
description "Positron Emission Tomography"
literal "false"

\end_inset

PET) (
\begin_inset CommandInset citation
LatexCommand citet
key "Du2020,Siddique2021"
literal "false"

\end_inset

).
 
\end_layout

\begin_layout Standard
Las principales arquitecturas utilizadas en estos campos son aquellas relacionad
as con la U-Net (
\begin_inset CommandInset citation
LatexCommand citet
key "Ronneberger2015"
literal "false"

\end_inset

), la 
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "CNN"
description "Convolutional Neural Network"
literal "false"

\end_inset

CNN clásica (
\begin_inset CommandInset citation
LatexCommand citet
key "LeCun1995"
literal "false"

\end_inset

), las 
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "FCN"
description "Fully Convolutional Network"
literal "false"

\end_inset

FCN (
\begin_inset CommandInset citation
LatexCommand citet
key "Long2015"
literal "false"

\end_inset

) y las soluciones tipo 
\emph on
GAN
\emph default
 (
\begin_inset CommandInset citation
LatexCommand citet
key "Goodfellow2014"
literal "false"

\end_inset

).
 Estas arquitecturas presentan las mejores métricas de segmentación en problemas
 como detección de: masa blanca en el cerebro, tumores cerebrales, tumores
 en mamografías, lesiones por infarto, cancer prostático, cancer de hígado,
 cancer de pulmón, entre otros muchos (
\begin_inset CommandInset citation
LatexCommand citet
key "Siddique2021"
literal "false"

\end_inset

).
 Resaltar la arquitectura U-Net que sigue siendo la base más comúnmente
 utilizada.
\end_layout

\begin_layout Standard
Si se restringe el campo a la línea de investigación del trabajo, se puede
 comprobar la prevalencia de las U-Nets frente a las demás arquitecturas
 y un reciente desarrollo vertiginoso basado en la atención por dos motivos:
 su interpretabilidad y su potencia predictiva.
 
\end_layout

\begin_layout Standard
En la detección de tumores en ecografías mamarias se pueden observar tres
 principales soluciones: 
\end_layout

\begin_layout Enumerate
Introducción de la atención: Un gran número de artículos actuales hacen
 uso de la atención para mejorar los modelos recientes.
 El objetivo principal de esta línea es mejorar la precisión a cualquier
 costo, es decir, obtener las mejores métricas en las tablas comparativas.
\end_layout

\begin_layout Enumerate
Variaciones de las arquitecturas clásicas: Una línea recurrente es la utilizació
n de arquitecturas conocidas como U-Net, 
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "ResNet"
description "Residual Network"
literal "false"

\end_inset

ResNet o Inception e implementar variaciones y combinaciones.
 Esta línea trata de variar las arquitecturas actuales añadiendo complejidad,
 paralelizando tareas o introduciendo algoritmos de optimización que mejoren
 un resultado previo.
 De esta manera, se observa una progresión cuasi-estática cuyo objetivo
 no es ser el mejor modelo si no ser mejor que el anterior por medio de
 la actualización de puntos específicos.
\end_layout

\begin_layout Enumerate
Reducción de parámetros: La línea de optimización de recursos existe desde
 el inicio del campo y propone métodos que simulan las transformaciones
 de otras arquitecturas pero utilizando métodos de menor coste computacional.
 El objetivo de estas arquitecturas no es mejorar métricas sino mantenerlas.
 La reducción de parámetros implica directamente la reducción de información
 disponible por lo que el logro consiste en obtener resultados razonablemente
 parecidos con un número de parámetros significativamente menor.
 Esta vía promueve la accesibilidad, reduce la huella de carbono y desliga
 la técnica de grandes corporaciones.
 En el trabajo actual se ha optado por esta vía de desarrollo.
\end_layout

\begin_layout Standard
Si se enfoca la búsqueda a la tercera vía presentada, la reducción de parámetros
, se comprueban avances en pequeñas secciones como el uso de modelos pre-entrena
dos para subrogar la pérdida vía algoritmos semi-supervisados estilo 
\emph on
profesor-aprendiz
\emph default
 (
\begin_inset CommandInset citation
LatexCommand citet
key "Guo2023"
literal "false"

\end_inset

) y uso de bloques de exprimido y excitación (
\begin_inset CommandInset citation
LatexCommand citet
key "Hu2018,Yuan2023,Zhang2023"
literal "false"

\end_inset

).
 Mientras que los algoritmos semi-supervisados sí se encuentran entre los
 últimos avances generales, la utilización de bloques de exprimido y excitación
 como en sustitución de otros más pesados están obsoletos.
 Por este motivo, se ha decidido realizar una actualización de la sustitución
 de bloques convolucionales pesados.
 
\end_layout

\begin_layout Section
Arquitecturas eficientes
\begin_inset CommandInset label
LatexCommand label
name "sec:Arquitecturas-eficientes."

\end_inset


\end_layout

\begin_layout Standard
Se ha comentado previamente (secciones 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Visión-Artificial"
plural "false"
caps "false"
noprefix "false"

\end_inset

, 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Segmentación-semántica-en"
plural "false"
caps "false"
noprefix "false"

\end_inset

) líneas de investigación en arquitecturas de redes neuronales.
 En esta sección se detalla la arquitectura utilizada en este trabajo que
 apuesta por la eficiencia de redes neuronales, haciendo hincapié en los
 puntos clave necesarios para obtener el modelo EFSNet (subsección 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:EFSNet."
plural "false"
caps "false"
noprefix "false"

\end_inset

) utilizado.
 Esta red neuronal se fundamenta en las arquitecturas U-Net (
\begin_inset CommandInset citation
LatexCommand citet
key "Ronneberger2015"
literal "false"

\end_inset

), ResNet (
\begin_inset CommandInset citation
LatexCommand citet
key "7780459"
literal "false"

\end_inset

) e Inception (
\begin_inset CommandInset citation
LatexCommand citet
key "Szegedy2017"
literal "false"

\end_inset

), reduciendo cada bloque de capas al mínimo número de parámetros posible.
\end_layout

\begin_layout Standard
Las principales aportaciones de esta arquitectura y las arquitecturas de
 las que hereda se exponen a continuación y se verá su aplicación en más
 detalle en la sección de Métodos (
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Métodos"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
\end_layout

\begin_layout Enumerate
Normalización por lote, 
\emph on

\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "BatchNorm"
description "Batch Normalization"
literal "false"

\end_inset

BatchNorm
\emph default
, con activación 
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "PReLU"
description "Parametrized Rectified Linear Unit"
literal "false"

\end_inset

PReLU: La normalización por lote, como su nombre indica, normaliza los valores
 resultantes de una capa por conjunto de datos de inferencia.
 
\begin_inset Newline newline
\end_inset

Este tipo de normalización permite eliminar el 
\emph on
bias
\emph default
, o sesgo, de la capa previa ya que el resultado tras esta normalización
 es el mismo (sección 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:BatchNorm-y-sesgo"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 
\begin_inset Newline newline
\end_inset

Con esta combinación se reduce el número de parámetros de tantos como neuronas
 tenga la capa convolucional previa a uno.
\end_layout

\begin_layout Enumerate
Capas de convolución desagregadas: Se intercambia una capa de convolución
 usual de tamaño de kernel 
\begin_inset Formula $k_{s}$
\end_inset

 y stride 
\begin_inset Formula $s$
\end_inset

 por tres capas de convolución consecutivas.
 
\begin_inset Newline newline
\end_inset

La primera aplica el stride, 
\begin_inset Formula $s$
\end_inset

, con 
\begin_inset Formula $k_{0}=(sxs)$
\end_inset

 para variar la dimensión, la segunda realiza la extracción de características
 utilizando 
\begin_inset Formula $k_{1}=k_{s}$
\end_inset

 y, finalmente, la tercera aumenta la dimensión de los canales utilizando
 
\begin_inset Formula $k_{2}=(1x1)$
\end_inset

.
\begin_inset Newline newline
\end_inset

Como se verá más adelante, existe una variación denominada bloque factorizado
 que intercambia la segunda convolución de kernel 
\begin_inset Formula $k=(3x3)$
\end_inset

 por dos convoluciones consecutivas de kernels 
\begin_inset Formula $k_{0}=(1x3)$
\end_inset

 y 
\begin_inset Formula $k_{1}=(3x1)$
\end_inset

 reduciendo aún más el número de parámetros.
 
\begin_inset Newline newline
\end_inset

La justificación matemática de la reducción de parámetros obtenida se puede
 comprobar en la sección 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Convoluciones-desagregadas"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Enumerate
Convolución por grupo y barajado de canales: De ShuffleNet (
\begin_inset CommandInset citation
LatexCommand citet
key "Ma2018"
literal "false"

\end_inset

) se adopta la idea de realizar convoluciones sobre grupos de canales que
 posteriormente son barajados y convolucionados en profundidad para relacionar
 los grupos.
 Esta configuración, similar a la anterior, se utiliza para aumentar la
 riqueza del mapa de características manteniendo un número de parámetros
 reducido.
\end_layout

\begin_layout Enumerate
Regularización dropout en los canales: Habitualmente se utiliza la regularizació
n Dropout sobre las neuronas que conforman una capa.
 En las arquitecturas eficientes se promueve la utilización de dropout sobre
 los canales, 
\emph on
Spatial Dropout
\emph default
, (
\begin_inset CommandInset citation
LatexCommand citet
key "Lee2020"
literal "false"

\end_inset

) del mapa de características.
 De esta forma se impide la especialización de los canales en características
 concretas forzando a extraer características generales de la imagen.
\end_layout

\begin_layout Enumerate
Bloques residuales y 
\emph on
skip-connections
\emph default
: Los bloques residuales utilizan una rama lineal que propaga la información
 sin perturbar a lo largo de un mismo bloque de transformación de forma
 que se genere un contexto global de información a lo largo de la red neuronal.
 Por otro lado, las 
\emph on
skip-connections
\emph default
 que conectan bloques del 
\emph on
Encoder
\emph default
 con bloques del 
\emph on
Decoder
\emph default
 de las arquitecturas U-Net son utilizados con el mismo propósito pero en
 un nivel mayor.
\begin_inset Newline newline
\end_inset

La combinación de ambas técnicas reduce la pérdida de información y dota
 de contexto a las diferentes partes de la arquitectura.
 Utilizando estas técnicas se reduce la carga de computación en los diferentes
 bloques y partes de la red permitiendo reducir los parámetros sin impactar
 en la precisión.
 Adicionalmente, estas conexiones sirven de atajo en la propagación de la
 pérdida durante el entrenamiento previniendo problemas relacionados con
 el desvanecimiento del gradiente.
\end_layout

\begin_layout Standard
Con base en estos 5 puntos principales se estructura la arquitectura utilizada
 EFSNet (
\begin_inset CommandInset citation
LatexCommand citet
key "Hu2020"
literal "false"

\end_inset

).
\end_layout

\end_body
\end_document
