@IEEEtranBSTCTL{Hu2020,
  author   = {Hu, Xuegang and Wang, Haibo},
  comment  = {EFSNet.},
  doi      = {10.1109/ACCESS.2020.2987080},
  journal  = {IEEE Access},
  keywords = {Semantics;Image segmentation;Feature extraction;Decoding;Real-time systems;Image resolution;Performance evaluation;Resource-constrained;semantic segmentation;continuous shuffle dilated convolution;real-time},
  pages    = {70913-70924},
  title    = {Efficient Fast Semantic Segmentation Using Continuous Shuffle Dilated Convolutions},
  volume   = {8},
  year     = {2020},
}

@IEEEtranBSTCTL{paszke2016enet,
  author  = {Paszke, Adam and Chaurasia, Abhishek and Kim, Sangpil and Culurciello, Eugenio},
  comment = {ENet},
  journal = {arXiv preprint arXiv:1606.02147},
  title   = {Enet: A deep neural network architecture for real-time semantic segmentation},
  year    = {2016},
}

@IEEEtranBSTCTL{Zhang2019,
  abstract = {Semantic segmentation is a challenging problem in computer vision. Many applications, such as autonomous driving and robot navigation with urban road scene, need accurate and efficient segmentation. Most state-of-the-art methods focus on accuracy, rather than efficiency. In this paper, we propose a more efficient neural network architecture, which has fewer parameters, for semantic segmentation in the urban road scene. An asymmetric encoder-decoder structure based on ResNet is used in our model. In the first stage of encoder, we use continuous factorized block to extract low-level features. Continuous dilated block is applied in the second stage, which ensures that the model has a larger view field, while keeping the model small-scale and shallow. The down sampled features from encoder are up sampled with decoder to the same-size output as the input image and the details refined. Our model can achieve end-to-end and pixel-to-pixel training without pretraining from scratch. The parameters of our model are only 0.2M, 100× less than those of others such as SegNet, etc. Experiments are conducted on five public road scene datasets (CamVid, CityScapes, Gatech, KITTI Road Detection, and KITTI Semantic Segmentation), and the results demonstrate that our model can achieve better performance.},
  author   = {Zhang, Xuetao and Chen, Zhenxue and Wu, Q. M. Jonathan and Cai, Lei and Lu, Dan and Li, Xianming},
  comment  = {FSSNet},
  doi      = {10.1109/TII.2018.2849348},
  issn     = {1941-0050},
  journal  = {IEEE Transactions on Industrial Informatics},
  keywords = {Semantics;Convolution;Image segmentation;Standards;Informatics;Computer vision;Computer architecture;Convolutional neural network (CNN);real-time;ResNet;scene perception;semantic segmentation},
  month    = {Feb},
  number   = {2},
  pages    = {1183-1192},
  title    = {Fast Semantic Segmentation for Scene Perception},
  volume   = {15},
  year     = {2019},
}

@Article{7803544,
  author   = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title    = {SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation},
  year     = {2017},
  issn     = {1939-3539},
  month    = {Dec},
  number   = {12},
  pages    = {2481-2495},
  volume   = {39},
  abstract = {We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1] . The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN [2] and also with the well known DeepLab-LargeFOV [3] , DeconvNet [4] architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures and can be trained end-to-end using stochastic gradient descent. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. These quantitative assessments show that SegNet provides good performance with competitive inference time and most efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.},
  comment  = {SegNet. Benchmark. History.},
  doi      = {10.1109/TPAMI.2016.2644615},
  keywords = {Decoding;Neural networks;Training;Computer architecture;Image segmentation;Semantics;Convolutional codes;Deep convolutional neural networks;semantic pixel-wise segmentation;indoor scenes;road scenes;encoder;decoder;pooling;upsampling},
}

@InProceedings{7780459,
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Deep Residual Learning for Image Recognition},
  year      = {2016},
  month     = {June},
  pages     = {770-778},
  abstract  = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  comment   = {Bottleneck unit},
  doi       = {10.1109/CVPR.2016.90},
  issn      = {1063-6919},
  keywords  = {Training;Degradation;Complexity theory;Image recognition;Neural networks;Visualization;Image segmentation},
}

@InProceedings{8578814,
  author    = {Zhang, Xiangyu and Zhou, Xinyu and Lin, Mengxiao and Sun, Jian},
  booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  title     = {ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices},
  year      = {2018},
  month     = {June},
  pages     = {6848-6856},
  abstract  = {We introduce an extremely computation-efficient CNN architecture named ShuffleNet, which is designed specially for mobile devices with very limited computing power (e.g., 10-150 MFLOPs). The new architecture utilizes two new operations, pointwise group convolution and channel shuffle, to greatly reduce computation cost while maintaining accuracy. Experiments on ImageNet classification and MS COCO object detection demonstrate the superior performance of ShuffleNet over other structures, e.g. lower top-1 error (absolute 7.8%) than recent MobileNet [12] on ImageNet classification task, under the computation budget of 40 MFLOPs. On an ARM-based mobile device, ShuffleNet achieves ~13× actual speedup over AlexNet while maintaining comparable accuracy.},
  doi       = {10.1109/CVPR.2018.00716},
  issn      = {2575-7075},
  keywords  = {Convolution;Complexity theory;Computer architecture;Mobile handsets;Computational modeling;Task analysis;Neural networks},
}

@InProceedings{Ma2018,
  author    = {Ma, Ningning and Zhang, Xiangyu and Zheng, Hai-Tao and Sun, Jian},
  booktitle = {Computer Vision -- ECCV 2018},
  title     = {ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design},
  year      = {2018},
  address   = {Cham},
  editor    = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
  pages     = {122--138},
  publisher = {Springer International Publishing},
  abstract  = {Currently, the neural network architecture design is mostly guided by the indirect metric of computation complexity, i.e., FLOPs. However, the direct metric, e.g., speed, also depends on the other factors such as memory access cost and platform characterics. Thus, this work proposes to evaluate the direct metric on the target platform, beyond only considering FLOPs. Based on a series of controlled experiments, this work derives several practical guidelines for efficient network design. Accordingly, a new architecture is presented, called ShuffleNet V2. Comprehensive ablation experiments verify that our model is the state-of-the-art in terms of speed and accuracy tradeoff.},
  isbn      = {978-3-030-01264-9},
}

@InProceedings{Loni2019,
  author    = {Mohammad Loni and Ali Zoljodi and Sima Sinaei and Masoud Daneshtalab and Mikael Sj{\"o}din},
  booktitle = {International Conference on Artificial Neural Networks},
  title     = {NeuroPower: Designing Energy Efficient Convolutional Neural Network Architecture for Embedded Systems},
  year      = {2019},
  comment   = {Sobre consumo de energía por tamaño de los modelos},
  url       = {https://api.semanticscholar.org/CorpusID:202402304},
}

@Article{Shin2016,
  author        = {Hoo{-}Chang Shin and Holger R. Roth and Mingchen Gao and Le Lu and Ziyue Xu and Isabella Nogues and Jianhua Yao and Daniel J. Mollura and Ronald M. Summers},
  journal       = {CoRR},
  title         = {Deep Convolutional Neural Networks for Computer-Aided Detection: {CNN} Architectures, Dataset Characteristics and Transfer Learning},
  year          = {2016},
  volume        = {abs/1602.03409},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/ShinRGLXNYMS16.bib},
  eprint        = {1602.03409},
  timestamp     = {Fri, 07 Oct 2022 14:19:05 +0200},
  url           = {http://arxiv.org/abs/1602.03409},
}

@Article{Chenna2023,
  author  = {Chenna, Dwith},
  journal = {arXiv preprint arXiv:2311.12816},
  title   = {Evolution of Convolutional Neural Network (CNN): Compute vs Memory bandwidth for Edge AI},
  year    = {2023},
  comment = {Ojo preprint!
Evolucion de las CNNs},
}

@Article{Krizhevsky2012,
  author  = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal = {Advances in neural information processing systems},
  title   = {Imagenet classification with deep convolutional neural networks},
  year    = {2012},
  volume  = {25},
  comment = {AlexNet. History DCNN},
}

@Article{Network1989,
  author    = {Network, Back-Propagation},
  title     = {Handwritten digit recognition with},
  year      = {1989},
  comment   = {Densa clasificacion. Historia.},
  publisher = {Citeseer},
}

@Article{turaga2010convolutional,
  author    = {Turaga, Srinivas C and Murray, Joseph F and Jain, Viren and Roth, Fabian and Helmstaedter, Moritz and Briggman, Kevin and Denk, Winfried and Seung, H Sebastian},
  journal   = {Neural computation},
  title     = {Convolutional networks can learn to generate affinity graphs for image segmentation},
  year      = {2010},
  number    = {2},
  pages     = {511--538},
  volume    = {22},
  comment   = {Image Segmentation},
  publisher = {MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…},
}

@Article{Siddique2021,
  author   = {Siddique, Nahian and Paheding, Sidike and Elkin, Colin P. and Devabhaktuni, Vijay},
  journal  = {IEEE Access},
  title    = {U-Net and Its Variants for Medical Image Segmentation: A Review of Theory and Applications},
  year     = {2021},
  pages    = {82031-82057},
  volume   = {9},
  comment  = {U-Net history in Image segmentation},
  doi      = {10.1109/ACCESS.2021.3086020},
  keywords = {Image segmentation;Convolution;Biomedical imaging;Three-dimensional displays;Logic gates;Deep learning;Computer architecture;Biomedical imaging;deep learning;neural network architecture;segmentation;U-net},
}

@Article{Du2020,
  author  = {Du, Getao and Cao, Xu and Liang, Jimin and Chen, Xueli and Zhan, Yonghua},
  journal = {Journal of Imaging Science \& Technology},
  title   = {Medical Image Segmentation based on U-Net: A Review.},
  year    = {2020},
  number  = {2},
  volume  = {64},
  comment = {U-Net Medical Image Segmentation},
}

@Article{Guo2018,
  author    = {Guo, Yanming and Liu, Yu and Georgiou, Theodoros and Lew, Michael S},
  journal   = {International journal of multimedia information retrieval},
  title     = {A review of semantic segmentation using deep neural networks},
  year      = {2018},
  pages     = {87--93},
  volume    = {7},
  comment   = {Semantic Segmentation History},
  publisher = {Springer},
}

@Article{Islam2022,
  author  = {Islam, Khawar},
  journal = {arXiv preprint arXiv:2203.01536},
  title   = {Recent advances in vision transformer: A survey and outlook of recent work},
  year    = {2022},
  comment = {Vision Transformer History},
}

@Article{Jiang2022,
  author  = {Baode Jiang and Xiaoya An and Shaofen Xu and Zhanlong Chen},
  journal = {Journal of the Indian Society of Remote Sensing},
  title   = {Intelligent Image Semantic Segmentation: A Review Through Deep Learning Techniques for Remote Sensing Image Analysis},
  year    = {2022},
  pages   = {1865-1878},
  volume  = {51},
  url     = {https://api.semanticscholar.org/CorpusID:246075398},
}

@Article{Yang2022,
  author  = {Cheng Yang and Hongjun Guo},
  journal = {Mathematical Problems in Engineering},
  title   = {A Method of Image Semantic Segmentation Based on PSPNet},
  year    = {2022},
  comment = {Pyramid Semantic Segmentation History},
  url     = {https://api.semanticscholar.org/CorpusID:251511103},
}

@InBook{Rojas1996,
  author    = {Rojas, Ra{\'u}l},
  pages     = {149--182},
  publisher = {Springer Berlin Heidelberg},
  title     = {The Backpropagation Algorithm},
  year      = {1996},
  address   = {Berlin, Heidelberg},
  isbn      = {978-3-642-61068-4},
  abstract  = {We saw in the last chapter that multilayered networks are capable of computing a wider range of Boolean functions than networks with a single layer of computing units. However the computational effort needed for finding the correct combination of weights increases substantially when more parameters and more complicated topologies are considered. In this chapter we discuss a popular learning method capable of handling such large learning problems---the backpropagation algorithm. This numerical method was used by different research communities in different contexts, was discovered and rediscovered, until in 1985 it found its way into connectionist AI mainly through the work of the PDP group [382]. It has been one of the most studied and used algorithms for neural networks learning ever since.},
  booktitle = {Neural Networks: A Systematic Introduction},
  comment   = {History NNs},
  doi       = {10.1007/978-3-642-61068-4_7},
  url       = {https://doi.org/10.1007/978-3-642-61068-4_7},
}

@Article{LeCun1989,
  author  = {LeCun, Yann and Boser, Bernhard and Denker, John and Henderson, Donnie and Howard, Richard and Hubbard, Wayne and Jackel, Lawrence},
  journal = {Advances in neural information processing systems},
  title   = {Handwritten digit recognition with a back-propagation network},
  year    = {1989},
  volume  = {2},
  comment = {First CNN

This 1989 paper, published in the proceedings of the Neural Information Processing Systems (NIPS) conference, is considered the first work to use backpropagation to train the convolution kernels of a CNN for image recognition tasks like handwritten digit classification

The CNN architecture used in this paper was later named LeNet-5 and published in 1995.},
}

@Article{LeCun1995,
  author  = {LeCun, Yann and Jackel, Lawrence D and Bottou, L{\'e}on and Cortes, Corinna and Denker, John S and Drucker, Harris and Guyon, Isabelle and Muller, Urs A and Sackinger, Eduard and Simard, Patrice and others},
  journal = {Neural networks: the statistical mechanics perspective},
  title   = {Learning algorithms for classification: A comparison on handwritten digit recognition},
  year    = {1995},
  number  = {276},
  pages   = {2},
  volume  = {261},
  comment = {LeNet primeras CNNs.},
}

@InProceedings{Ronneberger2015,
  author       = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  booktitle    = {Medical image computing and computer-assisted intervention--MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18},
  title        = {U-net: Convolutional networks for biomedical image segmentation},
  year         = {2015},
  organization = {Springer},
  pages        = {234--241},
  comment      = {Primera U-Net en medicina. History DCNN.

U-Net benchmark},
}

@Article{Krogh1991,
  author  = {Krogh, Anders and Hertz, John},
  journal = {Advances in neural information processing systems},
  title   = {A simple weight decay can improve generalization},
  year    = {1991},
  volume  = {4},
  comment = {Weight Decay},
}

@InProceedings{Nair2010,
  author    = {Nair, Vinod and Hinton, Geoffrey E},
  booktitle = {Proceedings of the 27th international conference on machine learning (ICML-10)},
  title     = {Rectified linear units improve restricted boltzmann machines},
  year      = {2010},
  pages     = {807--814},
  comment   = {ReLU activation},
}

@Article{Lee2020,
  author    = {Lee, Sanghun and Lee, Chulhee},
  journal   = {Multimedia Tools and Applications},
  title     = {Revisiting spatial dropout for regularizing convolutional neural networks},
  year      = {2020},
  number    = {45},
  pages     = {34195--34207},
  volume    = {79},
  comment   = {Spatial Dropout paper.

"...dropout between channels in the CNNs can be functionally similar to dropout in the FCNNs, and spatial dropout can be an effective way to take advantage of the dropout technique for regularizing."},
  publisher = {Springer},
}

@InProceedings{Ioffe2015,
  author       = {Ioffe, Sergey and Szegedy, Christian},
  booktitle    = {International conference on machine learning},
  title        = {Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  year         = {2015},
  organization = {pmlr},
  pages        = {448--456},
  comment      = {BatchNormalization paper},
}

@Article{Ba2016,
  author  = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal = {arXiv preprint arXiv:1607.06450},
  title   = {Layer normalization},
  year    = {2016},
  comment = {Layer Normalization},
}

@InProceedings{He2015,
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle = {Proceedings of the IEEE international conference on computer vision},
  title     = {Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
  year      = {2015},
  pages     = {1026--1034},
  comment   = {Parametrized Rectified Linear Unit, Kaiming Uniform, PReLU},
}

@Article{Kingma2014,
  author  = {Kingma, Diederik P and Ba, Jimmy},
  journal = {arXiv preprint arXiv:1412.6980},
  title   = {Adam: A method for stochastic optimization},
  year    = {2014},
}

@InProceedings{Redmon2016,
  author    = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  title     = {You only look once: Unified, real-time object detection},
  year      = {2016},
  pages     = {779--788},
  comment   = {YOLO history},
}

@InProceedings{Long2015,
  author    = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  title     = {Fully convolutional networks for semantic segmentation},
  year      = {2015},
  pages     = {3431--3440},
  comment   = {Semantic Segmentation History.

FCN.},
}

@Article{Ren2015,
  author  = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  journal = {Advances in neural information processing systems},
  title   = {Faster r-cnn: Towards real-time object detection with region proposal networks},
  year    = {2015},
  volume  = {28},
  comment = {Faster-R-CNN. Real time object detection.},
}

@InProceedings{Girshick2015,
  author    = {Girshick, Ross},
  booktitle = {Proceedings of the IEEE international conference on computer vision},
  title     = {Fast r-cnn},
  year      = {2015},
  pages     = {1440--1448},
  comment   = {Fast R-CNN. History.},
}

@InProceedings{Girshick2014,
  author    = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  title     = {Rich feature hierarchies for accurate object detection and semantic segmentation},
  year      = {2014},
  pages     = {580--587},
  comment   = {R-CNN. History.},
}

@Article{He2015a,
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  journal   = {IEEE transactions on pattern analysis and machine intelligence},
  title     = {Spatial pyramid pooling in deep convolutional networks for visual recognition},
  year      = {2015},
  number    = {9},
  pages     = {1904--1916},
  volume    = {37},
  comment   = {Spatial Pyramid Pooling. HIstory.},
  publisher = {IEEE},
}

@Article{Chen2014,
  author  = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L},
  journal = {arXiv preprint arXiv:1412.7062},
  title   = {Semantic image segmentation with deep convolutional nets and fully connected crfs},
  year    = {2014},
  comment = {Semantic Image Segmentation DCNN. History.},
}

@Article{Farabet2012,
  author    = {Farabet, Clement and Couprie, Camille and Najman, Laurent and LeCun, Yann},
  journal   = {IEEE transactions on pattern analysis and machine intelligence},
  title     = {Learning hierarchical features for scene labeling},
  year      = {2012},
  number    = {8},
  pages     = {1915--1929},
  volume    = {35},
  comment   = {Learning hierarchical features.},
  publisher = {IEEE},
}

@Article{Srivastava2014,
  author    = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal   = {The journal of machine learning research},
  title     = {Dropout: a simple way to prevent neural networks from overfitting},
  year      = {2014},
  number    = {1},
  pages     = {1929--1958},
  volume    = {15},
  comment   = {Dropout first paper. History.},
  publisher = {JMLR. org},
}

@InProceedings{Szegedy2016,
  author    = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  title     = {Rethinking the inception architecture for computer vision},
  year      = {2016},
  pages     = {2818--2826},
  comment   = {Rethinking Inception. Influencia clave para la EFSNet.},
}

@InProceedings{Szegedy2017,
  author    = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander},
  booktitle = {Proceedings of the AAAI conference on artificial intelligence},
  title     = {Inception-v4, inception-resnet and the impact of residual connections on learning},
  year      = {2017},
  number    = {1},
  volume    = {31},
  comment   = {Inception v-4. Influencia para las residual connections de EFSNet.},
}

@InProceedings{He2016,
  author       = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle    = {Computer Vision--ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11--14, 2016, Proceedings, Part IV 14},
  title        = {Identity mappings in deep residual networks},
  year         = {2016},
  organization = {Springer},
  pages        = {630--645},
  comment      = {Identity mappings. Influencia para los residual blocks de EFSNet.},
}

@Article{Jaderberg2015,
  author  = {Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and others},
  journal = {Advances in neural information processing systems},
  title   = {Spatial transformer networks},
  year    = {2015},
  volume  = {28},
  comment = {Spatial transformers. Vía de desarrollo descartada.},
}

@InProceedings{Szegedy2015,
  author    = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  title     = {Going deeper with convolutions},
  year      = {2015},
  pages     = {1--9},
  comment   = {GoogLeNet. Residual Blocks influence.},
}

@Article{Simonyan2014,
  author  = {Simonyan, Karen and Zisserman, Andrew},
  journal = {arXiv preprint arXiv:1409.1556},
  title   = {Very deep convolutional networks for large-scale image recognition},
  year    = {2014},
  comment = {ConvNet. History. VGGNet},
}

@InProceedings{Mehta2018,
  author    = {Mehta, Sachin and Rastegari, Mohammad and Caspi, Anat and Shapiro, Linda and Hajishirzi, Hannaneh},
  booktitle = {Proceedings of the european conference on computer vision (ECCV)},
  title     = {Espnet: Efficient spatial pyramid of dilated convolutions for semantic segmentation},
  year      = {2018},
  pages     = {552--568},
  comment   = {ESPNet. Influencia EFSNet.},
}

@Article{Howard2017,
  author  = {Howard, Andrew G and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  journal = {arXiv preprint arXiv:1704.04861},
  title   = {Mobilenets: Efficient convolutional neural networks for mobile vision applications},
  year    = {2017},
  comment = {MobileNet. Real-time history.},
}

@Article{Chen2024,
  author  = {Chen, Dong and Liu, Ning and Zhu, Yichen and Che, Zhengping and Ma, Rui and Zhang, Fachao and Mou, Xiaofeng and Chang, Yi and Tang, Jian},
  journal = {arXiv preprint arXiv:2402.00084},
  title   = {EPSD: Early Pruning with Self-Distillation for Efficient Model Compression},
  year    = {2024},
  comment = {EPSD. Introducción.},
}

@InProceedings{Zhou2022,
  author    = {Zhou, ZhaoJing and Zhou, Yun and Jiang, Zhuqing and Men, Aidong and Wang, Haiying},
  booktitle = {ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  title     = {An Efficient Method for Model Pruning Using Knowledge Distillation with Few Samples},
  year      = {2022},
  pages     = {2515-2519},
  comment   = {PFDD. Introducción},
  doi       = {10.1109/ICASSP43922.2022.9746024},
  keywords  = {Training;Measurement;Deep learning;Image coding;Convolution;Conferences;Neural networks;Network compression;knowledge distillation;few samples},
}

@InProceedings{Park2022,
  author       = {Park, Jinhyuk and No, Albert},
  booktitle    = {European Conference on Computer Vision},
  title        = {Prune your model before distill it},
  year         = {2022},
  organization = {Springer},
  pages        = {120--136},
  comment      = {Prune before distill. Introduction.},
}

@Article{Liao2023,
  author  = {Liao, Baohao and Meng, Yan and Monz, Christof},
  journal = {arXiv preprint arXiv:2305.16742},
  title   = {Parameter-efficient fine-tuning without introducing new latency},
  year    = {2023},
  comment = {PaFi. Introducción.},
}

@Article{Hu2021,
  author  = {Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal = {arXiv preprint arXiv:2106.09685},
  title   = {Lora: Low-rank adaptation of large language models},
  year    = {2021},
  comment = {LoRA. Introducción},
}

@Article{Dettmers2024,
  author  = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal = {Advances in Neural Information Processing Systems},
  title   = {Qlora: Efficient finetuning of quantized llms},
  year    = {2024},
  volume  = {36},
  comment = {QLoRA. Introducción.},
}

@Article{Wang2023,
  author  = {Wang, Hongyu and Ma, Shuming and Dong, Li and Huang, Shaohan and Wang, Huaijie and Ma, Lingxiao and Yang, Fan and Wang, Ruiping and Wu, Yi and Wei, Furu},
  journal = {arXiv preprint arXiv:2310.11453},
  title   = {Bitnet: Scaling 1-bit transformers for large language models},
  year    = {2023},
  comment = {BitNet. Introducción.},
}

@Article{AlDhabyani2020,
  author    = {Al-Dhabyani, Walid and Gomaa, Mohammed and Khaled, Hussien and Fahmy, Aly},
  journal   = {Data in brief},
  title     = {Dataset of breast ultrasound images},
  year      = {2020},
  pages     = {104863},
  volume    = {28},
  comment   = {BUSI Dataset},
  publisher = {Elsevier},
}

@Article{Pawlowska2024,
  author   = {Pawłowska, Anna and Ćwierz-Pieńkowska, Anna and Domalik, Agnieszka and Jaguś, Dominika and Kasprzak, Piotr and Matkowski, Rafał and Fura, Łukasz and Nowicki, Andrzej and Żołek, Norbert},
  journal  = {Scientific Data},
  title    = {Curated benchmark dataset for ultrasound based breast lesion analysis},
  year     = {2024},
  issn     = {2052-4463},
  number   = {1},
  pages    = {148},
  volume   = {11},
  abstract = {A new detailed dataset of breast ultrasound scans (BrEaST) containing images of benign and malignant lesions as well as normal tissue examples, is presented. The dataset consists of 256 breast scans collected from 256 patients. Each scan was manually annotated and labeled by a radiologist experienced in breast ultrasound examination. In particular, each tumor was identified in the image using a freehand annotation and labeled according to BIRADS features and lexicon. The histopathological classification of the tumor was also provided for patients who underwent a biopsy. The BrEaST dataset is the first breast ultrasound dataset containing patient-level labels, image-level annotations, and tumor-level labels with all cases confirmed by follow-up care or core needle biopsy result. To enable research into breast disease detection, tumor segmentation and classification, the BrEaST dataset is made publicly available with the CC-BY 4.0 license.},
  comment  = {BrEaST},
  doi      = {10.1038/s41597-024-02984-z},
  refid    = {Pawłowska2024},
  url      = {https://doi.org/10.1038/s41597-024-02984-z},
}

@Article{Pawlowska2023,
  author    = {Paw{\l}owska, Anna and Karwat, Piotr and {\.Z}o{\l}ek, Norbert},
  journal   = {Data in Brief},
  title     = {re:“[dataset of breast ultrasound images by w. al-dhabyani, m. gomaa, h. khaled \& a. fahmy, data in brief, 2020, 28, 104863]”},
  year      = {2023},
  volume    = {48},
  comment   = {BUSI-BrEaST beef},
  publisher = {Elsevier},
}

@Article{PiotrzkowskaWroblewska2017,
  author   = {Piotrzkowska-Wróblewska, Hanna and Dobruch-Sobczak, Katarzyna and Byra, Michał and Nowicki, Andrzej},
  journal  = {Medical Physics},
  title    = {Open access database of raw ultrasonic signals acquired from malignant and benign breast lesions},
  year     = {2017},
  number   = {11},
  pages    = {6105-6109},
  volume   = {44},
  abstract = {Purpose The aim of this paper is to provide access to a database consisting of the raw radio-frequency ultrasonic echoes acquired from malignant and benign breast lesions. The database is freely available for study and signal analysis. Acquisition and validation methods The ultrasonic radio-frequency echoes were recorded from breast focal lesions of patients of the Institute of Oncology in Warsaw. The data were collected between 11/2013 and 10/2015. Patients were examined by a radiologist with 18 yr' experience in the ultrasonic examination of breast lesions. The set of data includes scans from 52 malignant and 48 benign breast lesions recorded in a group of 78 women. For each lesion, two individual orthogonal scans from the pathological region were acquired with the Ultrasonix SonixTouch Research ultrasound scanner using the L14-5/38 linear array transducer. All malignant lesions were histologically assessed by core needle biopsy. In the case of benign lesions, part of them was histologically assessed and another part was observed over a 2-year period. Data format and usage notes The radio-frequency echoes were stored in Matlab file format. For each scan, the region of interest was provided to correctly indicate the lesion area. Moreover, for each lesion, the BI-RADS category and the lesion class were included. Two code examples of data manipulation are presented. The data can be downloaded via the Zenodo repository (https://doi.org/10.5281/zenodo.545928) or the website http://bluebox.ippt.gov.pl/~hpiotrzk. Potential applications The database can be used to test quantitative ultrasound techniques and ultrasound image processing algorithms, or to develop computer-aided diagnosis systems.},
  comment  = {OASBUD},
  doi      = {https://doi.org/10.1002/mp.12538},
  eprint   = {https://aapm.onlinelibrary.wiley.com/doi/pdf/10.1002/mp.12538},
  keywords = {breast lesions, dataset, ultrasonic signals, ultrasonography},
  url      = {https://aapm.onlinelibrary.wiley.com/doi/abs/10.1002/mp.12538},
}

@InProceedings{Ribeiro2016,
  author    = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle = {Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},
  title     = {" Why should i trust you?" Explaining the predictions of any classifier},
  year      = {2016},
  pages     = {1135--1144},
  comment   = {Wolf-Husky image prediction snow bias.},
}

@InProceedings{Shah2016,
  author    = {Shah, Anish and Kadam, Eashan and Shah, Hena and Shinde, Sameer and Shingade, Sandip},
  booktitle = {Proceedings of the third international symposium on computer vision and the internet},
  title     = {Deep residual networks with exponential linear unit},
  year      = {2016},
  pages     = {59--65},
  comment   = {BatchNorm ELU. Vanishing Gradients.},
}

@Article{Yu2015,
  author  = {Yu, Fisher and Koltun, Vladlen},
  journal = {arXiv preprint arXiv:1511.07122},
  title   = {Multi-scale context aggregation by dilated convolutions},
  year    = {2015},
  comment = {Dilated convolution.},
}

@InProceedings{Chollet2017,
  author    = {Chollet, Fran{\c{c}}ois},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  title     = {Xception: Deep learning with depthwise separable convolutions},
  year      = {2017},
  pages     = {1251--1258},
  comment   = {DepthWise Convolution},
}

@InProceedings{Mannor2005,
  author  = {Mannor, Shie and Peleg, Dori and Rubinstein, Reuven},
  title   = {The cross entropy method for classification},
  year    = {2005},
  month   = {01},
  pages   = {561-568},
  comment = {CrossEntropy Loss},
  doi     = {10.1145/1102351.1102422},
}

@Article{Reddi2019,
  author  = {Reddi, Sashank J and Kale, Satyen and Kumar, Sanjiv},
  journal = {arXiv preprint arXiv:1904.09237},
  title   = {On the convergence of adam and beyond},
  year    = {2019},
  comment = {AMSGRAD. Adam Variant.},
}

@Article{Bansal2023,
  author  = {Bansal, Satish},
  journal = {International Journal on Recent and Innovation Trends in Computing and Communication},
  title   = {Breast Tumor Recognition by Semantic Segmentation of Multiclass Ultrasound Images},
  year    = {2023},
  month   = {10},
  pages   = {938-946},
  volume  = {11},
  comment = {V-Net. Mal paper. Decir que no es fiable.},
  doi     = {10.17762/ijritcc.v11i9.8986},
}

@Article{Oktay2018,
  author  = {Oktay, Ozan and Schlemper, Jo and Folgoc, Loic Le and Lee, Matthew and Heinrich, Mattias and Misawa, Kazunari and Mori, Kensaku and McDonagh, Steven and Hammerla, Nils Y and Kainz, Bernhard and others},
  journal = {arXiv preprint arXiv:1804.03999},
  title   = {Attention u-net: Learning where to look for the pancreas},
  year    = {2018},
  comment = {Attention U-Net},
}

@Article{Chen2017,
  author  = {Chen, Liang-Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig},
  journal = {arXiv preprint arXiv:1706.05587},
  title   = {Rethinking atrous convolution for semantic image segmentation},
  year    = {2017},
  comment = {DeepLabv3+},
}

@Article{Xu2023,
  author   = {Xu, Meng and Huang, Kuan and Qi, Xiaojun},
  journal  = {IEEE Access},
  title    = {A Regional-Attentive Multi-Task Learning Framework for Breast Ultrasound Image Segmentation and Classification},
  year     = {2023},
  pages    = {5377-5392},
  volume   = {11},
  comment  = {Comparación de métricas BUSI},
  doi      = {10.1109/ACCESS.2023.3236693},
  keywords = {Tumors;Image segmentation;Multitasking;Ultrasonic imaging;Feature extraction;Breast;Biomedical imaging;Multitasking;Regional attention;multi-task learning;segmentation;classification;breast ultrasound},
}

@Article{Amiri2020,
  author    = {Amiri, Mina and Brooks, Rupert and Behboodi, Bahareh and Rivaz, Hassan},
  journal   = {International journal of computer assisted radiology and surgery},
  title     = {Two-stage ultrasound image segmentation using U-Net and test time augmentation},
  year      = {2020},
  pages     = {981--988},
  volume    = {15},
  comment   = {Two Stage. State of the art. Historia.},
  publisher = {Springer},
}

@InProceedings{Xu2021,
  author       = {Xu, Meng and Huang, Kuan and Chen, Qiuxiao and Qi, Xiaojun},
  booktitle    = {2021 IEEE 18th international symposium on biomedical imaging (ISBI)},
  title        = {Mssa-net: Multi-scale self-attention network for breast ultrasound image segmentation},
  year         = {2021},
  organization = {IEEE},
  pages        = {827--831},
  comment      = {MSSA-Net Comparison},
}

@Article{Zhang2023,
  author         = {Zhang, Shuai and Niu, Yanmin},
  journal        = {Bioengineering},
  title          = {LcmUNet: A Lightweight Network Combining CNN and MLP for Real-Time Medical Image Segmentation},
  year           = {2023},
  issn           = {2306-5354},
  number         = {6},
  volume         = {10},
  abstract       = {In recent years, UNet and its improved variants have become the main methods for medical image segmentation. Although these models have achieved excellent results in segmentation accuracy, their large number of network parameters and high computational complexity make it difficult to achieve medical image segmentation in real-time therapy and diagnosis rapidly. To address this problem, we introduce a lightweight medical image segmentation network (LcmUNet) based on CNN and MLP. We designed LcmUNet’s structure in terms of model performance, parameters, and computational complexity. The first three layers are convolutional layers, and the last two layers are MLP layers. In the convolution part, we propose an LDA module that combines asymmetric convolution, depth-wise separable convolution, and an attention mechanism to reduce the number of network parameters while maintaining a strong feature-extraction capability. In the MLP part, we propose an LMLP module that helps enhance contextual information while focusing on local information and improves segmentation accuracy while maintaining high inference speed. This network also covers skip connections between the encoder and decoder at various levels. Our network achieves real-time segmentation results accurately in extensive experiments. With only 1.49 million model parameters and without pre-training, LcmUNet demonstrated impressive performance on different datasets. On the ISIC2018 dataset, it achieved an IoU of 85.19%, 92.07% recall, and 92.99% precision. On the BUSI dataset, it achieved an IoU of 63.99%, 79.96% recall, and 76.69% precision. Lastly, on the Kvasir-SEG dataset, LcmUNet achieved an IoU of 81.89%, 88.93% recall, and 91.79% precision.},
  article-number = {712},
  comment        = {LCM Unet.},
  doi            = {10.3390/bioengineering10060712},
  pubmedid       = {37370643},
  url            = {https://www.mdpi.com/2306-5354/10/6/712},
}

@Article{Byra2020,
  author    = {Byra, Michal and Jarosik, Piotr and Szubert, Aleksandra and Galperin, Michael and Ojeda-Fournier, Haydee and Olson, Linda and O’Boyle, Mary and Comstock, Christopher and Andre, Michael},
  journal   = {Biomedical Signal Processing and Control},
  title     = {Breast mass segmentation in ultrasound with selective kernel U-Net convolutional neural network},
  year      = {2020},
  pages     = {102027},
  volume    = {61},
  comment   = {Sk-U-Net},
  publisher = {Elsevier},
}

@Article{Shareef2022,
  author         = {Shareef, Bryar and Vakanski, Aleksandar and Freer, Phoebe E. and Xian, Min},
  journal        = {Healthcare},
  title          = {ESTAN: Enhanced Small Tumor-Aware Network for Breast Ultrasound Image Segmentation},
  year           = {2022},
  issn           = {2227-9032},
  number         = {11},
  volume         = {10},
  abstract       = {Breast tumor segmentation is a critical task in computer-aided diagnosis (CAD) systems for breast cancer detection because accurate tumor size, shape, and location are important for further tumor quantification and classification. However, segmenting small tumors in ultrasound images is challenging due to the speckle noise, varying tumor shapes and sizes among patients, and the existence of tumor-like image regions. Recently, deep learning-based approaches have achieved great success in biomedical image analysis, but current state-of-the-art approaches achieve poor performance for segmenting small breast tumors. In this paper, we propose a novel deep neural network architecture, namely the Enhanced Small Tumor-Aware Network (ESTAN), to accurately and robustly segment breast tumors. The Enhanced Small Tumor-Aware Network introduces two encoders to extract and fuse image context information at different scales, and utilizes row-column-wise kernels to adapt to the breast anatomy. We compare ESTAN and nine state-of-the-art approaches using seven quantitative metrics on three public breast ultrasound datasets, i.e., BUSIS, Dataset B, and BUSI. The results demonstrate that the proposed approach achieves the best overall performance and outperforms all other approaches on small tumor segmentation. Specifically, the Dice similarity coefficient (DSC) of ESTAN on the three datasets is 0.92, 0.82, and 0.78, respectively; and the DSC of ESTAN on the three datasets of small tumors is 0.89, 0.80, and 0.81, respectively.},
  article-number = {2262},
  comment        = {ESTAN Net},
  doi            = {10.3390/healthcare10112262},
  pubmedid       = {36421586},
  url            = {https://www.mdpi.com/2227-9032/10/11/2262},
}

@Article{Zhang2023a,
  author   = {Zhang, Jie and Zhang, Zhichao and Liu, Hua and Xu, Shiqiang},
  journal  = {IET Image Processing},
  title    = {SaTransformer: Semantic-aware transformer for breast cancer classification and segmentation},
  year     = {2023},
  number   = {13},
  pages    = {3789-3800},
  volume   = {17},
  abstract = {Abstract Breast cancer classification and segmentation play an important role in identifying and detecting benign and malignant breast lesions. However, segmentation and classification still face many challenges: 1) The characteristics of cancer itself, such as fuzzy edges, complex backgrounds, and significant changes in size, shape, and intensity distribution make accurate segment and classification challenges. 2) Existing methods ignore the potential relationship between classification and segmentation tasks, due to the classification and segmentation being treated as two separate tasks. To overcome these challenges, in this paper, a novel Semantic-aware transformer (SaTransformer) for breast cancer classification and segmentation is proposed. Specifically, the SaTransformer enables doing the two takes simultaneously through one unified framework. Unlike existing well-known methods, the segmentation and classification information are semantically interactive, reinforcing each other during feature representation learning and improving the ability of feature representation learning while consuming less memory and computational complexity. The SaTransformer is validated on two publicly available breast cancer datasets – BUSI and UDIAT. Experimental results and quantitative evaluations (accuracy: 97.97\%, precision: 98.20\%, DSC: 86.34\%) demonstrate that the SaTransformer outperforms other state-of-the-art methods.},
  comment  = {SaTransformer benchmark},
  doi      = {https://doi.org/10.1049/ipr2.12897},
  eprint   = {https://ietresearch.onlinelibrary.wiley.com/doi/pdf/10.1049/ipr2.12897},
  keywords = {biomedical imaging, cancer, computer vision, convolutional neural nets, diseases, image classification, image segmentation},
  url      = {https://ietresearch.onlinelibrary.wiley.com/doi/abs/10.1049/ipr2.12897},
}

@Article{Cho2022,
  author   = {Se Woon Cho and Na Rae Baek and Kang Ryoung Park},
  journal  = {Journal of King Saud University - Computer and Information Sciences},
  title    = {Deep Learning-based Multi-stage segmentation method using ultrasound images for breast cancer diagnosis},
  year     = {2022},
  issn     = {1319-1578},
  number   = {10, Part B},
  pages    = {10273-10292},
  volume   = {34},
  abstract = {Globally, breast cancer occurs frequently in women and has the highest mortality rate. Owing to the increased need for a rapid and reliable initial diagnosis of breast cancer, several breast tumor segmentation methods based on ultrasound images have attracted research attention. Most conventional methods use a single network and demonstrate high performance by accurately classifying tumor-containing and normal image pixels. However, tests performed using normal images have revealed the occurrence of many false-positive errors. To address this limitation, this study proposes a multistage-based breast tumor segmentation technique based on the classification and segmentation of ultrasound images. In our method, a breast tumor ensemble classification network (BTEC-Net) is designed to classify whether an ultrasound image contains breast tumors or not. In the segmentation stage, a residual feature selection UNet (RFS-UNet) is used to exclusively segment images classified as abnormal by the BTEC-Net. The proposed multistage segmentation method can be adopted as a fully automated diagnosis system because it can classify images as tumor-containing or normal and effectively specify the breast tumor regions.},
  comment  = {Attention U-Net. Benchmark},
  doi      = {https://doi.org/10.1016/j.jksuci.2022.10.020},
  keywords = {Breast cancer, Ultrasound image, Breast tumor segmentation, BTEC-Net, RFS-UNet},
  url      = {https://www.sciencedirect.com/science/article/pii/S1319157822003718},
}

@InProceedings{Chen2018,
  author    = {Chen, Liang-Chieh and Zhu, Yukun and Papandreou, George and Schroff, Florian and Adam, Hartwig},
  booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
  title     = {Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation},
  year      = {2018},
  month     = {September},
  comment   = {DeepLab v3+},
}

@InProceedings{Zhao2017,
  author    = {Zhao, Hengshuang and Shi, Jianping and Qi, Xiaojuan and Wang, Xiaogang and Jia, Jiaya},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  title     = {Pyramid scene parsing network},
  year      = {2017},
  pages     = {2881--2890},
  comment   = {PSP Net.},
}

@Article{Cho2022a,
  author    = {Cho, Se Woon and Baek, Na Rae and Park, Kang Ryoung},
  journal   = {Journal of King Saud University-Computer and Information Sciences},
  title     = {Deep Learning-based Multi-stage segmentation method using ultrasound images for breast cancer diagnosis},
  year      = {2022},
  number    = {10},
  pages     = {10273--10292},
  volume    = {34},
  comment   = {Benchmark base. RFS-UNet.},
  publisher = {Elsevier},
}

@InProceedings{Kirillov2023,
  author    = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C and Lo, Wan-Yen and others},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
  title     = {Segment anything},
  year      = {2023},
  pages     = {4015--4026},
  comment   = {Segment Anything Model. SAM},
}

@InProceedings{Guo2023,
  author       = {Guo, Hongjiang and Wang, Shengwen and Dang, Hao and Xiao, Kangle and Yang, Yaru and Liu, Wenpei and Liu, Tongtong and Wan, Yiying},
  booktitle    = {2023 China Automation Congress (CAC)},
  title        = {LightBTSeg: A lightweight breast tumor segmentation model using ultrasound images via dual-path joint knowledge distillation},
  year         = {2023},
  organization = {IEEE},
  pages        = {3841--3847},
  comment      = {Lightweight. Student Teacher model.},
}

@InProceedings{Han2020,
  author    = {Han, Kai and Wang, Yunhe and Tian, Qi and Guo, Jianyuan and Xu, Chunjing and Xu, Chang},
  booktitle = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  title     = {Ghostnet: More features from cheap operations},
  year      = {2020},
  pages     = {1580--1589},
}

@InProceedings{Hu2018,
  author    = {Hu, Jie and Shen, Li and Sun, Gang},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  title     = {Squeeze-and-excitation networks},
  year      = {2018},
  pages     = {7132--7141},
  comment   = {Reducción de parámetros.},
}

@Article{Huang2022,
  author  = {Huang, Huimin and Xie, Shiao and Lin, Lanfen and Iwamoto, Yutaro and Han, Xianhua and Chen, Yen-Wei and Tong, Ruofeng},
  journal = {arXiv preprint arXiv:2207.14552},
  title   = {ScaleFormer: revisiting the transformer-based backbones from a scale-wise perspective for medical image segmentation},
  year    = {2022},
  comment = {Transformers en bio medicina. Atención},
}

@Article{Yuan2023,
  author         = {Yuan, Sheng and Qiu, Zhao and Li, Peipei and Hong, Yuqi},
  journal        = {Applied Sciences},
  title          = {RMAU-Net: Breast Tumor Segmentation Network Based on Residual Depthwise Separable Convolution and Multiscale Channel Attention Gates},
  year           = {2023},
  issn           = {2076-3417},
  number         = {20},
  volume         = {13},
  abstract       = {Breast cancer is one of the most common female diseases, posing a great threat to women’s health, and breast ultrasound imaging is a common method for breast cancer diagnosis. In recent years, U-Net and its variants have dominated the medical image segmentation field with their excellent performance. However, the existing U-type segmentation networks have the following problems: (1) the design of the feature extractor is complicated, and the calculation difficulty is increased; (2) the skip connection operation simply combines the features of the encoder and the decoder, without considering both spatial and channel dimensions; (3) during the downsampling phase, the pooling operation results in the loss of feature information. To address the above deficiencies, this paper proposes a breast tumor segmentation network, RMAU-Net, that combines residual depthwise separable convolution and a multi-scale channel attention gate. Specifically, we designed the RDw block, which has a simple structure and a larger sensory field, to overcome the localization problem of convolutional operations. Meanwhile, the MCAG module is designed to correct the low-level features in both spatial and channel dimensions and assist the high-level features to recover the up-sampling and pinpoint non-regular breast tumor features. In addition, this paper used the Patch Merging operation instead of the pooling method to prevent the loss of breast ultrasound image information. Experiments were conducted on two breast ultrasound datasets, Dataset B and BUSI, and the results show that the method in this paper has superior segmentation performance and better generalization.},
  article-number = {11362},
  comment        = {Reducción de parámetros.},
  doi            = {10.3390/app132011362},
  url            = {https://www.mdpi.com/2076-3417/13/20/11362},
}

@Article{Xiao2023,
  author   = {Xiao, Peng and Qin, Zhen and Chen, Dajiang and Zhang, Ning and Ding, Yi and Deng, Fuhu and Qin, Zhiguang and Pang, Minghui},
  journal  = {IEEE Internet of Things Journal},
  title    = {FastNet: A Lightweight Convolutional Neural Network for Tumors Fast Identification in Mobile-Computer-Assisted Devices},
  year     = {2023},
  number   = {11},
  pages    = {9878-9891},
  volume   = {10},
  comment  = {FastNet. Histopatología.},
  doi      = {10.1109/JIOT.2023.3235651},
  keywords = {Histopathology;Tumors;Neural networks;Internet of Things;Deep learning;Computational modeling;Transformers;Histopathology;Internet of Things (IoT);lightweight;mobile-computer-assisted devices;tumor detection},
}

@Article{Siegel2019,
  author   = {Siegel, Rebecca L. and Miller, Kimberly D. and Jemal, Ahmedin},
  journal  = {CA: A Cancer Journal for Clinicians},
  title    = {Cancer statistics, 2019},
  year     = {2019},
  number   = {1},
  pages    = {7-34},
  volume   = {69},
  abstract = {Abstract Each year, the American Cancer Society estimates the numbers of new cancer cases and deaths that will occur in the United States and compiles the most recent data on cancer incidence, mortality, and survival. Incidence data, available through 2015, were collected by the Surveillance, Epidemiology, and End Results Program; the National Program of Cancer Registries; and the North American Association of Central Cancer Registries. Mortality data, available through 2016, were collected by the National Center for Health Statistics. In 2019, 1,762,450 new cancer cases and 606,880 cancer deaths are projected to occur in the United States. Over the past decade of data, the cancer incidence rate (2006-2015) was stable in women and declined by approximately 2\% per year in men, whereas the cancer death rate (2007-2016) declined annually by 1.4\% and 1.8\%, respectively. The overall cancer death rate dropped continuously from 1991 to 2016 by a total of 27\%, translating into approximately 2,629,200 fewer cancer deaths than would have been expected if death rates had remained at their peak. Although the racial gap in cancer mortality is slowly narrowing, socioeconomic inequalities are widening, with the most notable gaps for the most preventable cancers. For example, compared with the most affluent counties, mortality rates in the poorest counties were 2-fold higher for cervical cancer and 40\% higher for male lung and liver cancers during 2012-2016. Some states are home to both the wealthiest and the poorest counties, suggesting the opportunity for more equitable dissemination of effective cancer prevention, early detection, and treatment strategies. A broader application of existing cancer control knowledge with an emphasis on disadvantaged groups would undoubtedly accelerate progress against cancer.},
  comment  = {Cancer stats.},
  doi      = {https://doi.org/10.3322/caac.21551},
  eprint   = {https://acsjournals.onlinelibrary.wiley.com/doi/pdf/10.3322/caac.21551},
  keywords = {cancer cases, cancer statistics, death rates, incidence, mortality},
  url      = {https://acsjournals.onlinelibrary.wiley.com/doi/abs/10.3322/caac.21551},
}

@Article{RegistrosdelCancer2019,
  author  = {Red Espa{\~n}ola de Registros del C{\'a}ncer},
  journal = {Redecan},
  title   = {Estimaciones de la incidencia del c{\'a}ncer en Espa{\~n}a 2019},
  year    = {2019},
  pages   = {1--14},
  volume  = {19},
  comment = {Cancer statistics},
}

@Article{Eigen2013,
  author  = {Eigen, David and Ranzato, Marc'Aurelio and Sutskever, Ilya},
  journal = {arXiv preprint arXiv:1312.4314},
  title   = {Learning factored representations in a deep mixture of experts},
  year    = {2013},
  comment = {Mixture of Experts MoE},
}

@Article{Seele2022,
  author    = {Seele, Peter and Schultz, Mario D},
  journal   = {Journal of Business Ethics},
  title     = {From greenwashing to machinewashing: a model and future directions derived from reasoning by analogy},
  year      = {2022},
  number    = {4},
  pages     = {1063--1089},
  volume    = {178},
  comment   = {greenwashing},
  publisher = {Springer},
}

@Article{Paszke2016,
  author  = {Paszke, Adam and Chaurasia, Abhishek and Kim, Sangpil and Culurciello, Eugenio},
  journal = {arXiv preprint arXiv:1606.02147},
  title   = {Enet: A deep neural network architecture for real-time semantic segmentation},
  year    = {2016},
}

@Article{Vaswani2017,
  author  = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal = {Advances in neural information processing systems},
  title   = {Attention is all you need},
  year    = {2017},
  volume  = {30},
}

@Article{Dosovitskiy2020,
  author  = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal = {arXiv preprint arXiv:2010.11929},
  title   = {An image is worth 16x16 words: Transformers for image recognition at scale},
  year    = {2020},
  comment = {Vision Transformer, ViT},
}

@Article{Hassani2022,
  author  = {Hassani, Ali and Shi, Humphrey},
  journal = {arXiv preprint arXiv:2209.15001},
  title   = {Dilated neighborhood attention transformer},
  year    = {2022},
}

@InProceedings{Cao2023,
  author    = {Cao, Hu and Wang, Yueyue and Chen, Joy and Jiang, Dongsheng and Zhang, Xiaopeng and Tian, Qi and Wang, Manning},
  booktitle = {Computer Vision -- ECCV 2022 Workshops},
  title     = {Swin-Unet: Unet-Like Pure Transformer for Medical Image Segmentation},
  year      = {2023},
  address   = {Cham},
  editor    = {Karlinsky, Leonid and Michaeli, Tomer and Nishino, Ko},
  pages     = {205--218},
  publisher = {Springer Nature Switzerland},
  abstract  = {In the past few years, convolutional neural networks (CNNs) have achieved milestones in medical image analysis. In particular, deep neural networks based on U-shaped architecture and skip-connections have been widely applied in various medical image tasks. However, although CNN has achieved excellent performance, it cannot learn global semantic information interaction well due to the locality of convolution operation. In this paper, we propose Swin-Unet, which is an Unet-like pure Transformer for medical image segmentation. The tokenized image patches are fed into the Transformer-based U-shaped Encoder-Decoder architecture with skip-connections for local-global semantic feature learning. Specifically, we use a hierarchical Swin Transformer with shifted windows as the encoder to extract context features. And a symmetric Swin Transformer-based decoder with a patch expanding layer is designed to perform the up-sampling operation to restore the spatial resolution of the feature maps. Under the direct down-sampling and up-sampling of the inputs and outputs by {\$}{\$}4{\{}{\backslash}times {\}}{\$}{\$}4{\texttimes}, experiments on multi-organ and cardiac segmentation tasks demonstrate that the pure Transformer-based U-shaped Encoder-Decoder network outperforms those methods with full-convolution or the combination of transformer and convolution. The codes have been publicly available at the link (https://github.com/HuCaoFighting/Swin-Unet).},
  isbn      = {978-3-031-25066-8},
}

@Article{Xie2021,
  author  = {Xie, Enze and Wang, Wenhai and Yu, Zhiding and Anandkumar, Anima and Alvarez, Jose M and Luo, Ping},
  journal = {Advances in neural information processing systems},
  title   = {SegFormer: Simple and efficient design for semantic segmentation with transformers},
  year    = {2021},
  pages   = {12077--12090},
  volume  = {34},
}

@Article{Li2018,
  author  = {Li, Hanchao and Xiong, Pengfei and An, Jie and Wang, Lingxue},
  journal = {arXiv preprint arXiv:1805.10180},
  title   = {Pyramid attention network for semantic segmentation},
  year    = {2018},
  comment = {Pyramid Vit},
}

@Article{Wu2024,
  author  = {Wu, Weiyi and Gao, Chongyang and Xu, Xinwen and Li, Siting and Gui, Jiang},
  journal = {arXiv preprint arXiv:2406.09333},
  title   = {Memory-Efficient Sparse Pyramid Attention Networks for Whole Slide Image Analysis},
  year    = {2024},
}

@Article{Hatamizadeh2023,
  author  = {Hatamizadeh, Ali and Heinrich, Greg and Yin, Hongxu and Tao, Andrew and Alvarez, Jose M and Kautz, Jan and Molchanov, Pavlo},
  journal = {arXiv preprint arXiv:2306.06189},
  title   = {Fastervit: Fast vision transformers with hierarchical attention},
  year    = {2023},
  comment = {Vit. Efficient. Fast. Attention},
}

@Article{Akkus2023,
  author  = {Akkus, Cem and Chu, Luyang and Djakovic, Vladana and Jauch-Walser, Steffen and Koch, Philipp and Loss, Giacomo and Marquardt, Christopher and Moldovan, Marco and Sauter, Nadja and Schneider, Maximilian and others},
  journal = {arXiv preprint arXiv:2301.04856},
  title   = {Multimodal deep learning},
  year    = {2023},
}

@Article{Shang2024,
  author  = {Shang, Chenming and Zhang, Hengyuan and Wen, Hao and Yang, Yujiu},
  journal = {arXiv preprint arXiv:2404.08964},
  title   = {Understanding Multimodal Deep Neural Networks: A Concept Selection View},
  year    = {2024},
}

@Article{Park2023,
  author    = {Park, Eunil},
  journal   = {Journal of big Data},
  title     = {CRNet: a multimodal deep convolutional neural network for customer revisit prediction},
  year      = {2023},
  number    = {1},
  pages     = {1},
  volume    = {10},
  publisher = {Springer},
}

@Article{Camilus2012,
  author    = {Camilus, K Santle and Govindan, VK},
  journal   = {International Journal of Image, Graphics and Signal Processing},
  title     = {A review on graph based segmentation},
  year      = {2012},
  number    = {5},
  pages     = {1},
  volume    = {4},
  publisher = {Modern Education and Computer Science Press},
}

@InProceedings{Yang2020,
  author    = {Yang, Fengting and Sun, Qian and Jin, Hailin and Zhou, Zihan},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Superpixel Segmentation With Fully Convolutional Networks},
  year      = {2020},
  month     = {June},
}

@InProceedings{Pavlitskaya2020,
  author    = {Pavlitskaya, Svetlana and Hubschneider, Christian and Weber, Michael and Moritz, Ruby and Huger, Fabian and Schlicht, Peter and Zollner, Marius},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops},
  title     = {Using mixture of expert models to gain insights into semantic segmentation},
  year      = {2020},
  pages     = {342--343},
  comment   = {Segmentation MoE},
}

@InProceedings{He2019,
  author    = {He, Junjun and Deng, Zhongying and Zhou, Lei and Wang, Yali and Qiao, Yu},
  booktitle = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  title     = {Adaptive pyramid context network for semantic segmentation},
  year      = {2019},
  pages     = {7519--7528},
}

@InProceedings{Huo2023,
  author    = {Huo, Xinyue and Xie, Lingxi and Zhou, Wengang and Li, Houqiang and Tian, Qi},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  title     = {Focus on Your Target: A Dual Teacher-Student Framework for Domain-Adaptive Semantic Segmentation},
  year      = {2023},
  month     = {October},
  pages     = {19027-19038},
}

@Article{Ho2020,
  author  = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  journal = {Advances in neural information processing systems},
  title   = {Denoising diffusion probabilistic models},
  year    = {2020},
  pages   = {6840--6851},
  volume  = {33},
}

@InProceedings{Esser2024,
  author    = {Esser, Patrick and Kulal, Sumith and Blattmann, Andreas and Entezari, Rahim and M{\"u}ller, Jonas and Saini, Harry and Levi, Yam and Lorenz, Dominik and Sauer, Axel and Boesel, Frederic and others},
  booktitle = {Forty-first International Conference on Machine Learning},
  title     = {Scaling rectified flow transformers for high-resolution image synthesis},
  year      = {2024},
  comment   = {Stable-Diffusion},
}

@Article{Betker2023,
  author  = {Betker, James and Goh, Gabriel and Jing, Li and Brooks, Tim and Wang, Jianfeng and Li, Linjie and Ouyang, Long and Zhuang, Juntang and Lee, Joyce and Guo, Yufei and others},
  journal = {Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf},
  title   = {Improving image generation with better captions},
  year    = {2023},
  number  = {3},
  pages   = {8},
  volume  = {2},
  comment = {Dalle-3},
}

@Article{Goodfellow2014,
  author  = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  journal = {Advances in neural information processing systems},
  title   = {Generative adversarial nets},
  year    = {2014},
  volume  = {27},
}

@Article{Loshchilov2017,
  author  = {Loshchilov, Ilya and Hutter, Frank},
  journal = {arXiv preprint arXiv:1711.05101},
  title   = {Decoupled weight decay regularization},
  year    = {2017},
}

@Article{Defazio2023,
  author  = {Defazio, Aaron and Cutkosky, Ashok and Mehta, Harsh and Mishchenko, Konstantin},
  journal = {arXiv preprint arXiv:2310.07831},
  title   = {When, why and how much? adaptive learning rate scheduling by refinement},
  year    = {2023},
}

@Article{Bharati2022,
  author    = {Bharati, Subrato and Mondal, M and Podder, Prajoy and Prasath, VB},
  journal   = {International Journal of Hybrid Intelligent Systems},
  title     = {Federated learning: Applications, challenges and future directions},
  year      = {2022},
  number    = {1-2},
  pages     = {19--35},
  volume    = {18},
  publisher = {IOS Press},
}

@InProceedings{Bretto2005,
  author  = {Bretto, Alain and Gillibert, Luc},
  title   = {Hypergraph-Based Image Representation},
  year    = {2005},
  month   = {04},
  pages   = {1-11},
  volume  = {3434},
  doi     = {10.1007/978-3-540-31988-7_1},
  isbn    = {978-3-540-25270-2},
  journal = {Lecture Notes in Computer Science},
}

@Article{BuenestadoCortes2022,
  author    = {Buenestado Cort{\'e}s, Miguel},
  journal   = {UNED. Trabajo Fin de Master.},
  title     = {Aprendizaje Federado Aplicado al Diagn{\'o}stico de Tumores Mamarios En Im{\'a}genes de Ultrasonido},
  year      = {2022},
  month     = feb,
  abstract  = {En el campo de la medicina, el aprendizaje autom{\'a}tico se erige como una herramienta poderosa y eficaz en la automatizaci{\'o}n de la tarea del diagn{\'o}stico por imagen, mediante la creaci{\'o}n de modelos inform{\'a}ticos predictivos. Para lograr resultados con un alto grado de fiabilidad, dichos modelos requieren de grandes vol{\'u}menes de datos para su aprendizaje, caracter{\'i}stica poco frecuente en contextos reales, pues organismos e instituciones m{\'e}dicas, individualmente, no suelen disponer de tales cantidades de informaci{\'o}n. Idealmente, {\'e}sta podr{\'i}a ser compartida y cedida mutuamente en busca de un fin com{\'u}n, pero esto resulta altamente improbable, dadas las regulaciones imperantes en la actualidad en torno a la propiedad intelectual y la privacidad de datos m{\'e}dicos de pacientes. A fin de superar tales escollos, surge el paradigma del aprendizaje federado (federated learning). En este Trabajo Fin de Master, aplicamos dicho paradigma en un caso de aplicaci{\'o}n concreto (detecci{\'o}n de tumores mamarios en im{\'a}genes de ultrasonidos), empleando tecnolog{\'i}a y t{\'e}cnicas espec{\'i}ficas de federaci{\'o}n (la librer{\'i}a TensorFlow Federated; la agregaci{\'o}n federada est{\'a}ndar, Federated Average, y variantes de la misma), con las que entrenamos modelos predictivos sobre diferentes combinaciones de datasets. Evaluaremos la idoneidad y eficacia de dichos modelos federados, tanto por s{\'i} mismos como en comparaci{\'o}n con resultados an{\'a}logos derivados de un aprendizaje cl{\'a}sico. Exponemos que el aprendizaje federado puede alcanzar, bajo ciertas configuraciones, cotas de eficacia similares a las de un aprendizaje cl{\'a}sico. El presente documento se articula, primero, con la introducci{\'o}n de los conceptos clave tocantes al propio caso de estudio: el problema a resolver en cuesti{\'o}n (segmentaci{\'o}n de im{\'a}genes de ultrasonido), los aspectos concretos del aprendizaje autom{\'a}tico que han ayudado a resolverlo (modelo de aprendizaje profundo, una red neuronal convolutiva U-Net) y profundizaremos en los fundamentos del paradigma de aprendizaje federado. Posteriormente, se detallar{\'a} la metodolog{\'i}a aplicada durante el desarrollo del proyecto y se expondr{\'a}n los resultados derivados del mismo. Finalmente, abordaremos las conclusiones de dichos resultados, as{\'i} como las limitaciones y posibles mejoras en el {\'a}mbito del presente trabajo y del aprendizaje federado, en particular.},
  publisher = {Universidad Nacional de Educaci{\'o}n a Distancia (Espa{\~n}a). Escuela T{\'e}cnica Superior de Ingenier{\'i}a Inform{\'a}tica. Departamento de Inteligencia Artificial},
  urldate   = {2022-11-20},
}

@Article{Ma2024,
  author    = {Ma, Jun and He, Yuting and Li, Feifei and Han, Lin and You, Chenyu and Wang, Bo},
  journal   = {Nature Communications},
  title     = {Segment anything in medical images},
  year      = {2024},
  number    = {1},
  pages     = {654},
  volume    = {15},
  comment   = {MedSAM},
  publisher = {Nature Publishing Group UK London},
}

@Article{Hinton2006,
  author    = {Hinton, G. E. and Salakhutdinov, R. R.},
  journal   = {Science},
  title     = {Reducing the Dimensionality of Data with Neural Networks},
  year      = {2006},
  issn      = {1095-9203},
  month     = jul,
  number    = {5786},
  pages     = {504--507},
  volume    = {313},
  comment   = {Fine-tuning},
  doi       = {10.1126/science.1127647},
  publisher = {American Association for the Advancement of Science (AAAS)},
}

@Article{Tan2023,
  author  = {Tan, Weimin and Chen, Siyuan and Yan, Bo},
  journal = {arXiv preprint arXiv:2307.00773},
  title   = {Diffss: Diffusion model for few-shot semantic segmentation},
  year    = {2023},
  comment = {Diffusion in segmentation},
}

@Article{Sarker2021,
  author    = {Sarker, Md Mostafa Kamal and Rashwan, Hatem A and Akram, Farhan and Singh, Vivek Kumar and Banu, Syeda Furruka and Chowdhury, Forhad UH and Choudhury, Kabir Ahmed and Chambon, Sylvie and Radeva, Petia and Puig, Domenec and others},
  journal   = {Expert Systems with Applications},
  title     = {SLSNet: Skin lesion segmentation using a lightweight generative adversarial network},
  year      = {2021},
  pages     = {115433},
  volume    = {183},
  comment   = {GAN lightweight},
  publisher = {Elsevier},
}

@Article{Brostow2009,
  author   = {Gabriel J. Brostow and Julien Fauqueur and Roberto Cipolla},
  journal  = {Pattern Recognition Letters},
  title    = {Semantic object classes in video: A high-definition ground truth database},
  year     = {2009},
  issn     = {0167-8655},
  note     = {Video-based Object and Event Analysis},
  number   = {2},
  pages    = {88-97},
  volume   = {30},
  abstract = {Visual object analysis researchers are increasingly experimenting with video, because it is expected that motion cues should help with detection, recognition, and other analysis tasks. This paper presents the Cambridge-driving Labeled Video Database (CamVid) as the first collection of videos with object class semantic labels, complete with metadata. The database provides ground truth labels that associate each pixel with one of 32 semantic classes. The database addresses the need for experimental data to quantitatively evaluate emerging algorithms. While most videos are filmed with fixed-position CCTV-style cameras, our data was captured from the perspective of a driving automobile. The driving scenario increases the number and heterogeneity of the observed object classes. Over 10min of high quality 30Hz footage is being provided, with corresponding semantically labeled images at 1Hz and in part, 15Hz. The CamVid Database offers four contributions that are relevant to object analysis researchers. First, the per-pixel semantic segmentation of over 700 images was specified manually, and was then inspected and confirmed by a second person for accuracy. Second, the high-quality and large resolution color video images in the database represent valuable extended duration digitized footage to those interested in driving scenarios or ego-motion. Third, we filmed calibration sequences for the camera color response and intrinsics, and computed a 3D camera pose for each frame in the sequences. Finally, in support of expanding this or other databases, we present custom-made labeling software for assisting users who wish to paint precise class-labels for other images and videos. We evaluate the relevance of the database by measuring the performance of an algorithm from each of three distinct domains: multi-class object recognition, pedestrian detection, and label propagation.},
  comment  = {CamVid},
  doi      = {https://doi.org/10.1016/j.patrec.2008.04.005},
  keywords = {Object recognition, Video database, Video understanding, Semantic segmentation, Label propagation},
  url      = {https://www.sciencedirect.com/science/article/pii/S0167865508001220},
}

@InProceedings{Cordts2016,
  author    = {Cordts, Marius and Omran, Mohamed and Ramos, Sebastian and Rehfeld, Timo and Enzweiler, Markus and Benenson, Rodrigo and Franke, Uwe and Roth, Stefan and Schiele, Bernt},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  title     = {The cityscapes dataset for semantic urban scene understanding},
  year      = {2016},
  pages     = {3213--3223},
  comment   = {CityScapes},
}

@Article{Hung2018,
  author  = {Hung, Wei-Chih and Tsai, Yi-Hsuan and Liou, Yan-Ting and Lin, Yen-Yu and Yang, Ming-Hsuan},
  journal = {arXiv preprint arXiv:1802.07934},
  title   = {Adversarial learning for semi-supervised semantic segmentation},
  year    = {2018},
}

@Article{Yap2018,
  author    = {Yap, Moi Hoon and Pons, Gerard and Martí, Joan and Ganau, Sergi and Sentís, Melcior and Zwiggelaar, Reyer and Davison, Adrian K. and Martí, Robert},
  journal   = {IEEE Journal of Biomedical and Health Informatics},
  title     = {Automated Breast Ultrasound Lesions Detection Using Convolutional Neural Networks},
  year      = {2018},
  issn      = {2168-2208},
  month     = jul,
  number    = {4},
  pages     = {1218--1226},
  volume    = {22},
  comment   = {UDIAT},
  doi       = {10.1109/jbhi.2017.2731873},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
}





@Article{Hochreiter1998,
  author   = {Hochreiter, Sepp},
  journal  = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
  title    = {The Vanishing Gradient Problem During Learning Recurrent Neural Nets and Problem Solutions},
  year     = {1998},
  number   = {02},
  pages    = {107-116},
  volume   = {06},
  abstract = {Recurrent nets are in principle capable to store past inputs to produce the currently desired output. Because of this property recurrent nets are used in time series prediction and process control. Practical applications involve temporal dependencies spanning many time steps, e.g. between relevant inputs and desired outputs. In this case, however, gradient based learning methods take too much time. The extremely increased learning time arises because the error vanishes as it gets propagated back. In this article the de-caying error flow is theoretically analyzed. Then methods trying to overcome vanishing gradients are briefly discussed. Finally, experiments comparing conventional algorithms and alternative methods are presented. With advanced methods long time lag problems can be solved in reasonable time.},
  doi      = {10.1142/S0218488598000094},
  eprint   = {https://doi.org/10.1142/S0218488598000094},
  url      = {https://doi.org/10.1142/S0218488598000094},
}

@Article{Soydaner2022,
  author  = {Soydaner, Derya},
  journal = {Neural Computing and Applications},
  title   = {Attention mechanism in neural networks: where it comes and where it goes},
  year    = {2022},
  month   = {05},
  volume  = {34},
  doi     = {10.1007/s00521-022-07366-3},
}

@Article{Reis2023,
  author  = {Reis, Dillon and Kupec, Jordan and Hong, Jacqueline and Daoudi, Ahmad},
  journal = {arXiv preprint arXiv:2305.09972},
  title   = {Real-time flying object detection with YOLOv8},
  year    = {2023},
}

@Article{Achiam2023,
  author  = {Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal = {arXiv preprint arXiv:2303.08774},
  title   = {Gpt-4 technical report},
  year    = {2023},
}

@InProceedings{Li2023,
  author       = {Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  booktitle    = {International conference on machine learning},
  title        = {Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  year         = {2023},
  organization = {PMLR},
  pages        = {19730--19742},
}

@Article{Li2023a,
  author  = {Li, Zihao and Yang, Zhuoran and Wang, Mengdi},
  journal = {arXiv preprint arXiv:2305.18438},
  title   = {Reinforcement learning with human feedback: Learning dynamic choices via pessimism},
  year    = {2023},
}

@Article{Zhuang2020,
  author    = {Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
  journal   = {Proceedings of the IEEE},
  title     = {A comprehensive survey on transfer learning},
  year      = {2020},
  number    = {1},
  pages     = {43--76},
  volume    = {109},
  publisher = {Ieee},
}

@InProceedings{Liu2024,
  author    = {Liu, Bingyan and Wang, Chengyu and Cao, Tingfeng and Jia, Kui and Huang, Jun},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  title     = {Towards Understanding Cross and Self-Attention in Stable Diffusion for Text-Guided Image Editing},
  year      = {2024},
  pages     = {7817--7826},
}

@Article{Huang2020,
  author    = {Huang, Shih-Cheng and Pareek, Anuj and Seyyedi, Saeed and Banerjee, Imon and Lungren, Matthew P},
  journal   = {NPJ digital medicine},
  title     = {Fusion of medical imaging and electronic health records using deep learning: a systematic review and implementation guidelines},
  year      = {2020},
  number    = {1},
  pages     = {136},
  volume    = {3},
  publisher = {Nature Publishing Group UK London},
}

@Article{Ektefaie2023,
  author    = {Ektefaie, Yasha and Dasoulas, George and Noori, Ayush and Farhat, Maha and Zitnik, Marinka},
  journal   = {Nature Machine Intelligence},
  title     = {Multimodal learning with graphs},
  year      = {2023},
  number    = {4},
  pages     = {340--350},
  volume    = {5},
  publisher = {Nature Publishing Group UK London},
}

@Article{Zhu2024,
  author  = {Zhu, Rui-Jie and Zhang, Yu and Sifferman, Ethan and Sheaves, Tyler and Wang, Yiqiao and Richmond, Dustin and Zhou, Peng and Eshraghian, Jason K},
  journal = {arXiv preprint arXiv:2406.02528},
  title   = {Scalable MatMul-free Language Modeling},
  year    = {2024},
}

@Article{Li2024,
  author  = {Li, Junyan and Chen, Delin and Cai, Tianle and Chen, Peihao and Hong, Yining and Chen, Zhenfang and Shen, Yikang and Gan, Chuang},
  journal = {arXiv preprint arXiv:2407.20228},
  title   = {FlexAttention for Efficient High-Resolution Vision-Language Models},
  year    = {2024},
}

@InProceedings{Shen2021,
  author    = {Shen, Zhuoran and Zhang, Mingyuan and Zhao, Haiyu and Yi, Shuai and Li, Hongsheng},
  booktitle = {Proceedings of the IEEE/CVF winter conference on applications of computer vision},
  title     = {Efficient attention: Attention with linear complexities},
  year      = {2021},
  pages     = {3531--3539},
}

@Article{Riquelme2021,
  author  = {Riquelme, Carlos and Puigcerver, Joan and Mustafa, Basil and Neumann, Maxim and Jenatton, Rodolphe and Susano Pinto, Andr{\'e} and Keysers, Daniel and Houlsby, Neil},
  journal = {Advances in Neural Information Processing Systems},
  title   = {Scaling vision with sparse mixture of experts},
  year    = {2021},
  pages   = {8583--8595},
  volume  = {34},
}

@Article{Liu2024a,
  author  = {Liu, Tianlin and Blondel, Mathieu and Riquelme, Carlos and Puigcerver, Joan},
  journal = {arXiv preprint arXiv:2401.15969},
  title   = {Routers in vision mixture of experts: An empirical study},
  year    = {2024},
}

@Article{Han2022,
  author  = {Han, Kai and Wang, Yunhe and Guo, Jianyuan and Tang, Yehui and Wu, Enhua},
  journal = {Advances in neural information processing systems},
  title   = {Vision gnn: An image is worth graph of nodes},
  year    = {2022},
  pages   = {8291--8303},
  volume  = {35},
}

@InProceedings{Krzywda2022,
  author       = {Krzywda, Maciej and {\L}ukasik, Szymon and Gandomi, Amir H},
  booktitle    = {2022 International Joint Conference on Neural Networks (IJCNN)},
  title        = {Graph neural networks in computer vision-architectures, datasets and common approaches},
  year         = {2022},
  organization = {IEEE},
  pages        = {1--10},
}

@Article{Kipf2016,
  author  = {Kipf, Thomas N and Welling, Max},
  journal = {arXiv preprint arXiv:1609.02907},
  title   = {Semi-supervised classification with graph convolutional networks},
  year    = {2016},
}

@Article{Bai2021,
  author    = {Bai, Song and Zhang, Feihu and Torr, Philip HS},
  journal   = {Pattern Recognition},
  title     = {Hypergraph convolution and hypergraph attention},
  year      = {2021},
  pages     = {107637},
  volume    = {110},
  publisher = {Elsevier},
}

@Article{Zhou2020,
  author   = {Jie Zhou and Ganqu Cui and Shengding Hu and Zhengyan Zhang and Cheng Yang and Zhiyuan Liu and Lifeng Wang and Changcheng Li and Maosong Sun},
  journal  = {AI Open},
  title    = {Graph neural networks: A review of methods and applications},
  year     = {2020},
  issn     = {2666-6510},
  pages    = {57-81},
  volume   = {1},
  abstract = {Lots of learning tasks require dealing with graph data which contains rich relation information among elements. Modeling physics systems, learning molecular fingerprints, predicting protein interface, and classifying diseases demand a model to learn from graph inputs. In other domains such as learning from non-structural data like texts and images, reasoning on extracted structures (like the dependency trees of sentences and the scene graphs of images) is an important research topic which also needs graph reasoning models. Graph neural networks (GNNs) are neural models that capture the dependence of graphs via message passing between the nodes of graphs. In recent years, variants of GNNs such as graph convolutional network (GCN), graph attention network (GAT), graph recurrent network (GRN) have demonstrated ground-breaking performances on many deep learning tasks. In this survey, we propose a general design pipeline for GNN models and discuss the variants of each component, systematically categorize the applications, and propose four open problems for future research.},
  doi      = {https://doi.org/10.1016/j.aiopen.2021.01.001},
  keywords = {Deep learning, Graph neural network},
  url      = {https://www.sciencedirect.com/science/article/pii/S2666651021000012},
}

@Article{Liu2024b,
  author  = {Liu, Xiao and Zhang, Chenxu and Zhang, Lei},
  journal = {arXiv preprint arXiv:2405.04404},
  title   = {Vision mamba: A comprehensive survey and taxonomy},
  year    = {2024},
}

@Article{Gu2023,
  author  = {Gu, Albert and Dao, Tri},
  journal = {arXiv preprint arXiv:2312.00752},
  title   = {Mamba: Linear-time sequence modeling with selective state spaces},
  year    = {2023},
}

@Article{Liao2024,
  author  = {Liao, Weibin and Zhu, Yinghao and Wang, Xinyuan and Pan, Cehngwei and Wang, Yasha and Ma, Liantao},
  journal = {arXiv preprint arXiv:2403.05246},
  title   = {Lightm-unet: Mamba assists in lightweight unet for medical image segmentation},
  year    = {2024},
}

@Article{Liu2024c,
  author  = {Liu, Jiarun and Yang, Hao and Zhou, Hong-Yu and Xi, Yan and Yu, Lequan and Yu, Yizhou and Liang, Yong and Shi, Guangming and Zhang, Shaoting and Zheng, Hairong and others},
  journal = {arXiv preprint arXiv:2402.03302},
  title   = {Swin-umamba: Mamba-based unet with imagenet-based pretraining},
  year    = {2024},
}

@Article{Ye2024,
  author  = {Ye, Zi and Chen, Tianxiang},
  journal = {arXiv preprint arXiv:2402.08506},
  title   = {P-mamba: Marrying perona malik diffusion with mamba for efficient pediatric echocardiographic left ventricular segmentation},
  year    = {2024},
}

@InProceedings{Chen2023,
  author    = {Chen, Shoufa and Sun, Peize and Song, Yibing and Luo, Ping},
  booktitle = {Proceedings of the IEEE/CVF international conference on computer vision},
  title     = {Diffusiondet: Diffusion model for object detection},
  year      = {2023},
  pages     = {19830--19843},
}

@Article{Lewkowycz2020,
  author  = {Lewkowycz, Aitor and Gur-Ari, Guy},
  journal = {Advances in Neural Information Processing Systems},
  title   = {On the training dynamics of deep networks with $ L\_2 $ regularization},
  year    = {2020},
  pages   = {4790--4799},
  volume  = {33},
}

@Article{Golatkar2019,
  author  = {Golatkar, Aditya Sharad and Achille, Alessandro and Soatto, Stefano},
  journal = {Advances in Neural Information Processing Systems},
  title   = {Time matters in regularizing deep networks: Weight decay and data augmentation affect early learning dynamics, matter little near convergence},
  year    = {2019},
  volume  = {32},
}

@InProceedings{Dolhansky2018,
  author    = {Dolhansky, Brian and Ferrer, Cristian Canton},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  title     = {Eye in-painting with exemplar generative adversarial networks},
  year      = {2018},
  pages     = {7902--7911},
}

@InProceedings{Wang2018,
  author    = {Wang, Xintao and Yu, Ke and Wu, Shixiang and Gu, Jinjin and Liu, Yihao and Dong, Chao and Qiao, Yu and Change Loy, Chen},
  booktitle = {Proceedings of the European conference on computer vision (ECCV) workshops},
  title     = {Esrgan: Enhanced super-resolution generative adversarial networks},
  year      = {2018},
  pages     = {0--0},
}

@Article{Jabbar2021,
  author    = {Jabbar, Abdul and Li, Xi and Omar, Bourahla},
  journal   = {ACM Computing Surveys (CSUR)},
  title     = {A survey on generative adversarial networks: Variants, applications, and training},
  year      = {2021},
  number    = {8},
  pages     = {1--49},
  volume    = {54},
  publisher = {ACM New York, NY},
}

@Article{Gui2021,
  author    = {Gui, Jie and Sun, Zhenan and Wen, Yonggang and Tao, Dacheng and Ye, Jieping},
  journal   = {IEEE transactions on knowledge and data engineering},
  title     = {A review on generative adversarial networks: Algorithms, theory, and applications},
  year      = {2021},
  number    = {4},
  pages     = {3313--3332},
  volume    = {35},
  publisher = {IEEE},
}

@Article{Pan2019,
  author    = {Pan, Zhaoqing and Yu, Weijie and Yi, Xiaokai and Khan, Asifullah and Yuan, Feng and Zheng, Yuhui},
  journal   = {IEEE access},
  title     = {Recent progress on generative adversarial networks (GANs): A survey},
  year      = {2019},
  pages     = {36322--36333},
  volume    = {7},
  publisher = {IEEE},
}

@InProceedings{Mishra2019,
  author    = {Mishra, Purnendu and Sarawadekar, Kishor},
  booktitle = {TENCON 2019 - 2019 IEEE Region 10 Conference (TENCON)},
  title     = {Polynomial Learning Rate Policy with Warm Restart for Deep Neural Network},
  year      = {2019},
  pages     = {2087-2092},
  doi       = {10.1109/TENCON.2019.8929465},
  keywords  = {Training;Neural networks;Stochastic processes;Schedules;Convergence;Navigation;Mathematical model;Deep Neural Network;Learning Rate;SGDR;CLR;polynomial learning rate;warm restart},
}

@InProceedings{Smith2017,
  author    = {Smith, Leslie N.},
  booktitle = {2017 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  title     = {Cyclical Learning Rates for Training Neural Networks},
  year      = {2017},
  pages     = {464-472},
  doi       = {10.1109/WACV.2017.58},
  keywords  = {Training;Neural networks;Schedules;Computer architecture;Tuning;Computational efficiency},
}

@Article{Loshchilov2016,
  author  = {Loshchilov, Ilya and Hutter, Frank},
  journal = {arXiv preprint arXiv:1608.03983},
  title   = {Sgdr: Stochastic gradient descent with warm restarts},
  year    = {2016},
  comment = {CosineAnnealingLR},
}

@Article{Moshawrab2023,
  author         = {Moshawrab, Mohammad and Adda, Mehdi and Bouzouane, Abdenour and Ibrahim, Hussein and Raad, Ali},
  journal        = {Electronics},
  title          = {Reviewing Federated Learning Aggregation Algorithms; Strategies, Contributions, Limitations and Future Perspectives},
  year           = {2023},
  issn           = {2079-9292},
  number         = {10},
  volume         = {12},
  abstract       = {The success of machine learning (ML) techniques in the formerly difficult areas of data analysis and pattern extraction has led to their widespread incorporation into various aspects of human life. This success is due in part to the increasing computational power of computers and in part to the improved ability of ML algorithms to process large amounts of data in various forms. Despite these improvements, certain issues, such as privacy, continue to hinder the development of this field. In this context, a privacy-preserving, distributed, and collaborative machine learning technique called federated learning (FL) has emerged. The core idea of this technique is that, unlike traditional machine learning, user data is not collected on a central server. Nevertheless, models are sent to clients to be trained locally, and then only the models themselves, without associated data, are sent back to the server to combine the different locally trained models into a single global model. In this respect, the aggregation algorithms play a crucial role in the federated learning process, as they are responsible for integrating the knowledge of the participating clients, by integrating the locally trained models to train a global one. To this end, this paper explores and investigates several federated learning aggregation strategies and algorithms. At the beginning, a brief summary of federated learning is given so that the context of an aggregation algorithm within a FL system can be understood. This is followed by an explanation of aggregation strategies and a discussion of current aggregation algorithms implementations, highlighting the unique value that each brings to the knowledge. Finally, limitations and possible future directions are described to help future researchers determine the best place to begin their own investigations.},
  article-number = {2287},
  doi            = {10.3390/electronics12102287},
  url            = {https://www.mdpi.com/2079-9292/12/10/2287},
}

@Article{Mai2024,
  author  = {Mai, Peihua and Yan, Ran and Pang, Yan},
  journal = {arXiv preprint arXiv:2405.15182},
  title   = {Rflpa: A robust federated learning framework against poisoning attacks with secure aggregation},
  year    = {2024},
}

@InProceedings{McMahan2017,
  author       = {McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle    = {Artificial intelligence and statistics},
  title        = {Communication-efficient learning of deep networks from decentralized data},
  year         = {2017},
  organization = {PMLR},
  pages        = {1273--1282},
}

@Article{Yao2024,
  author    = {Yao, Wenjian and Bai, Jiajun and Liao, Wei and Chen, Yuheng and Liu, Mengjuan and Xie, Yao},
  journal   = {Journal of Imaging Informatics in Medicine},
  title     = {From cnn to transformer: A review of medical image segmentation models},
  year      = {2024},
  pages     = {1--19},
  publisher = {Springer},
}

@Article{Azad2024,
  author    = {Azad, Reza and Aghdam, Ehsan Khodapanah and Rauland, Amelie and Jia, Yiwei and Avval, Atlas Haddadi and Bozorgpour, Afshin and Karimijafarbigloo, Sanaz and Cohen, Joseph Paul and Adeli, Ehsan and Merhof, Dorit},
  journal   = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title     = {Medical image segmentation review: The success of u-net},
  year      = {2024},
  publisher = {IEEE},
}

@Article{Wardlaw2015,
  author    = {Wardlaw, Joanna M. and Valdés Hernández, Maria C. and Muñoz‐Maniega, Susana},
  journal   = {Journal of the American Heart Association},
  title     = {What are White Matter Hyperintensities Made of?: Relevance to Vascular Cognitive Impairment},
  year      = {2015},
  issn      = {2047-9980},
  month     = jun,
  number    = {6},
  volume    = {4},
  doi       = {10.1161/jaha.114.001140},
  publisher = {Ovid Technologies (Wolters Kluwer Health)},
}

@Article{Coenen2023,
  author   = {Mirthe Coenen and Geert Jan Biessels and Charles DeCarli and Evan F. Fletcher and Pauline M. Maillard and Frederik Barkhof and Josephine Barnes and Thomas Benke and Jooske M.F. Boomsma and Christopher {P.L.H. Chen} and Peter Dal-Bianco and Anna Dewenter and Marco Duering and Christian Enzinger and Michael Ewers and Lieza G. Exalto and Nicolai Franzmeier and Onno Groeneveld and Saima Hilal and Edith Hofer and Huiberdina L. Koek and Andrea B. Maier and Cheryl R. McCreary and Janne M. Papma and Ross W. Paterson and Yolande A.L. Pijnenburg and Anna Rubinski and Reinhold Schmidt and Jonathan M. Schott and Catherine F. Slattery and Eric E. Smith and Carole H. Sudre and Rebecca M.E. Steketee and Esther {van den Berg} and Wiesje M. {van der Flier} and Narayanaswamy Venketasubramanian and Meike W. Vernooij and Frank J. Wolters and Xu Xin and J. Matthijs Biesbroek and Hugo J. Kuijf},
  journal  = {NeuroImage: Clinical},
  title    = {Spatial distributions of white matter hyperintensities on brain MRI: A pooled analysis of individual participant data from 11 memory clinic cohorts},
  year     = {2023},
  issn     = {2213-1582},
  pages    = {103547},
  volume   = {40},
  abstract = {Introduction
The spatial distribution of white matter hyperintensities (WMH) on MRI is often considered in the diagnostic evaluation of patients with cognitive problems. In some patients, clinicians may classify WMH patterns as “unusual”, but this is largely based on expert opinion, because detailed quantitative information about WMH distribution frequencies in a memory clinic setting is lacking. Here we report voxel wise 3D WMH distribution frequencies in a large multicenter dataset and also aimed to identify individuals with unusual WMH patterns.
Methods
Individual participant data (N = 3525, including 777 participants with subjective cognitive decline, 1389 participants with mild cognitive impairment and 1359 patients with dementia) from eleven memory clinic cohorts, recruited through the Meta VCI Map Consortium, were used. WMH segmentations were provided by participating centers or performed in Utrecht and registered to the Montreal Neurological Institute (MNI)-152 brain template for spatial normalization. To determine WMH distribution frequencies, we calculated WMH probability maps at voxel level. To identify individuals with unusual WMH patterns, region-of-interest (ROI) based WMH probability maps, rule-based scores, and a machine learning method (Local Outlier Factor (LOF)), were implemented.
Results
WMH occurred in 82% of voxels from the white matter template with large variation between subjects. Only a small proportion of the white matter (1.7%), mainly in the periventricular areas, was affected by WMH in at least 20% of participants. A large portion of the total white matter was affected infrequently. Nevertheless, 93.8% of individual participants had lesions in voxels that were affected in less than 2% of the population, mainly located in subcortical areas. Only the machine learning method effectively identified individuals with unusual patterns, in particular subjects with asymmetric WMH distribution or with WMH at relatively rarely affected locations despite common locations not being affected.
Discussion
Aggregating data from several memory clinic cohorts, we provide a detailed 3D map of WMH lesion distribution frequencies, that informs on common as well as rare localizations. The use of data-driven analysis with LOF can be used to identify unusual patterns, which might serve as an alert that rare causes of WMH should be considered.},
  doi      = {https://doi.org/10.1016/j.nicl.2023.103547},
  keywords = {White matter hyperintensities, Brain MRI, Distribution frequencies, Lesion location},
  url      = {https://www.sciencedirect.com/science/article/pii/S2213158223002383},
}

@Article{Ranjbarzadeh2021,
  author    = {Ranjbarzadeh, Ramin and Bagherian Kasgari, Abbas and Jafarzadeh Ghoushchi, Saeid and Anari, Shokofeh and Naseri, Maryam and Bendechache, Malika},
  journal   = {Scientific Reports},
  title     = {Brain tumor segmentation based on deep learning and an attention mechanism using MRI multi-modalities brain images},
  year      = {2021},
  number    = {1},
  pages     = {1--17},
  volume    = {11},
  publisher = {Nature Publishing Group},
}

@Article{Martorell2012,
  author    = {Martorell, S Medrano and Bl{\'a}zquez, M Cuadrado and Figueredo, D Garc{\'\i}a and Ortiz, S Gonz{\'a}lez and Font, J Capellades},
  journal   = {Radiolog{\'\i}a},
  title     = {Im{\'a}genes puntiformes hiperintensas en la sustancia blanca: una aproximaci{\'o}n diagn{\'o}stica},
  year      = {2012},
  number    = {4},
  pages     = {321--335},
  volume    = {54},
  publisher = {Elsevier},
}

@Article{Khalighi2024,
  author    = {Khalighi, Sirvan and Reddy, Kartik and Midya, Abhishek and Pandav, Krunal Balvantbhai and Madabhushi, Anant and Abedalthagafi, Malak},
  journal   = {NPJ Precision Oncology},
  title     = {Artificial intelligence in neuro-oncology: advances and challenges in brain tumor diagnosis, prognosis, and precision treatment},
  year      = {2024},
  number    = {1},
  pages     = {80},
  volume    = {8},
  publisher = {Nature Publishing Group UK London},
}

@Article{Kodipalli2023,
  author         = {Kodipalli, Ashwini and Fernandes, Steven L. and Gururaj, Vaishnavi and Varada Rameshbabu, Shriya and Dasar, Santosh},
  journal        = {Diagnostics},
  title          = {Performance Analysis of Segmentation and Classification of CT-Scanned Ovarian Tumours Using U-Net and Deep Convolutional Neural Networks},
  year           = {2023},
  issn           = {2075-4418},
  number         = {13},
  volume         = {13},
  abstract       = {Difficulty in detecting tumours in early stages is the major cause of mortalities in patients, despite the advancements in treatment and research regarding ovarian cancer. Deep learning algorithms were applied to serve the purpose as a diagnostic tool and applied to CT scan images of the ovarian region. The images went through a series of pre-processing techniques and, further, the tumour was segmented using the UNet model. The instances were then classified into two categories—benign and malignant tumours. Classification was performed using deep learning models like CNN, ResNet, DenseNet, Inception-ResNet, VGG16 and Xception, along with machine learning models such as Random Forest, Gradient Boosting, AdaBoosting and XGBoosting. DenseNet 121 emerges as the best model on this dataset after applying optimization on the machine learning models by obtaining an accuracy of 95.7%. The current work demonstrates the comparison of multiple CNN architectures with common machine learning algorithms, with and without optimization techniques applied.},
  article-number = {2282},
  doi            = {10.3390/diagnostics13132282},
  pubmedid       = {37443676},
  url            = {https://www.mdpi.com/2075-4418/13/13/2282},
}

@Article{LabidiGaly2012,
  author    = {Labidi-Galy, Sana Intidhar and Treilleux, Isabelle and Goddard-Leon, Sophie and Combes, Jean-Damien and Blay, Jean-Yves and Ray-Coquard, Isabelle and Caux, Christophe and Bendriss-Vermare, Nathalie},
  journal   = {Oncoimmunology},
  title     = {Plasmacytoid dendritic cells infiltrating ovarian cancer are associated with poor prognosis},
  year      = {2012},
  number    = {3},
  pages     = {380--382},
  volume    = {1},
  publisher = {Taylor \& Francis},
}

@Article{Chen2023a,
  author         = {Chen, Lijiang and Qiao, Changkun and Wu, Meijing and Cai, Linghan and Yin, Cong and Yang, Mukun and Sang, Xiubo and Bai, Wenpei},
  journal        = {Bioengineering},
  title          = {Improving the Segmentation Accuracy of Ovarian-Tumor Ultrasound Images Using Image Inpainting},
  year           = {2023},
  issn           = {2306-5354},
  number         = {2},
  volume         = {10},
  abstract       = {Diagnostic results can be radically influenced by the quality of 2D ovarian-tumor ultrasound images. However, clinically processed 2D ovarian-tumor ultrasound images contain many artificially recognized symbols, such as fingers, crosses, dashed lines, and letters which assist artificial intelligence (AI) in image recognition. These symbols are widely distributed within the lesion’s boundary, which can also affect the useful feature-extraction-utilizing networks and thus decrease the accuracy of lesion classification and segmentation. Image inpainting techniques are used for noise and object elimination from images. To solve this problem, we observed the MMOTU dataset and built a 2D ovarian-tumor ultrasound image inpainting dataset by finely annotating the various symbols in the images. A novel framework called mask-guided generative adversarial network (MGGAN) is presented in this paper for 2D ovarian-tumor ultrasound images to remove various symbols from the images. The MGGAN performs to a high standard in corrupted regions by using an attention mechanism in the generator to pay more attention to valid information and ignore symbol information, making lesion boundaries more realistic. Moreover, fast Fourier convolutions (FFCs) and residual networks are used to increase the global field of perception; thus, our model can be applied to high-resolution ultrasound images. The greatest benefit of this algorithm is that it achieves pixel-level inpainting of distorted regions without clean images. Compared with other models, our model achieveed better results with only one stage in terms of objective and subjective evaluations. Our model obtained the best results for 256 × 256 and 512 × 512 resolutions. At a resolution of 256 × 256, our model achieved 0.9246 for SSIM, 22.66 for FID, and 0.07806 for LPIPS. At a resolution of 512 × 512, our model achieved 0.9208 for SSIM, 25.52 for FID, and 0.08300 for LPIPS. Our method can considerably improve the accuracy of computerized ovarian tumor diagnosis. The segmentation accuracy was improved from 71.51% to 76.06% for the Unet model and from 61.13% to 66.65% for the PSPnet model in clean images.},
  article-number = {184},
  doi            = {10.3390/bioengineering10020184},
  pubmedid       = {36829679},
  url            = {https://www.mdpi.com/2306-5354/10/2/184},
}

@Article{Feigin2023,
  author    = {Feigin, Valery L and Owolabi, Mayowa O and Abd-Allah, Foad and Akinyemi, Rufus O and Bhattacharjee, Natalia V and Brainin, Michael and Cao, Jackie and Caso, Valeria and Dalton, Bronte and Davis, Alan and others},
  journal   = {The Lancet Neurology},
  title     = {Pragmatic solutions to reduce the global burden of stroke: a World Stroke Organization--Lancet Neurology Commission},
  year      = {2023},
  number    = {12},
  pages     = {1160--1206},
  volume    = {22},
  publisher = {Elsevier},
}

@Article{Feigin2022,
  author    = {Feigin, Valery L and Brainin, Michael and Norrving, Bo and Martins, Sheila and Sacco, Ralph L and Hacke, Werner and Fisher, Marc and Pandian, Jeyaraj and Lindsay, Patrice},
  journal   = {International Journal of Stroke},
  title     = {World Stroke Organization (WSO): global stroke fact sheet 2022},
  year      = {2022},
  number    = {1},
  pages     = {18--29},
  volume    = {17},
  publisher = {SAGE Publications Sage UK: London, England},
}

@Article{Malik2024,
  author         = {Malik, Mishaim and Chong, Benjamin and Fernandez, Justin and Shim, Vickie and Kasabov, Nikola Kirilov and Wang, Alan},
  journal        = {Bioengineering},
  title          = {Stroke Lesion Segmentation and Deep Learning: A Comprehensive Review},
  year           = {2024},
  issn           = {2306-5354},
  number         = {1},
  volume         = {11},
  abstract       = {Stroke is a medical condition that affects around 15 million people annually. Patients and their families can face severe financial and emotional challenges as it can cause motor, speech, cognitive, and emotional impairments. Stroke lesion segmentation identifies the stroke lesion visually while providing useful anatomical information. Though different computer-aided software are available for manual segmentation, state-of-the-art deep learning makes the job much easier. This review paper explores the different deep-learning-based lesion segmentation models and the impact of different pre-processing techniques on their performance. It aims to provide a comprehensive overview of the state-of-the-art models and aims to guide future research and contribute to the development of more robust and effective stroke lesion segmentation models.},
  article-number = {86},
  doi            = {10.3390/bioengineering11010086},
  pubmedid       = {38247963},
  url            = {https://www.mdpi.com/2306-5354/11/1/86},
}

@Article{Luo2024,
  author   = {Jialin Luo and Peishan Dai and Zhuang He and Zhongchao Huang and Shenghui Liao and Kun Liu},
  journal  = {Computers in Biology and Medicine},
  title    = {Deep learning models for ischemic stroke lesion segmentation in medical images: A survey},
  year     = {2024},
  issn     = {0010-4825},
  pages    = {108509},
  volume   = {175},
  abstract = {This paper provides a comprehensive review of deep learning models for ischemic stroke lesion segmentation in medical images. Ischemic stroke is a severe neurological disease and a leading cause of death and disability worldwide. Accurate segmentation of stroke lesions in medical images such as MRI and CT scans is crucial for diagnosis, treatment planning and prognosis. This paper first introduces common imaging modalities used for stroke diagnosis, discussing their capabilities in imaging lesions at different disease stages from the acute to chronic stage. It then reviews three major public benchmark datasets for evaluating stroke segmentation algorithms: ATLAS, ISLES and AISD, highlighting their key characteristics. The paper proceeds to provide an overview of foundational deep learning architectures for medical image segmentation, including CNN-based and transformer-based models. It summarizes recent innovations in adapting these architectures to the task of stroke lesion segmentation across the three datasets, analyzing their motivations, modifications and results. A survey of loss functions and data augmentations employed for this task is also included. The paper discusses various aspects related to stroke segmentation tasks, including prior knowledge, small lesions, and multimodal fusion, and then concludes by outlining promising future research directions. Overall, this comprehensive review covers critical technical developments in the field to support continued progress in automated stroke lesion segmentation.},
  doi      = {https://doi.org/10.1016/j.compbiomed.2024.108509},
  keywords = {Ischemic stroke lesion segmentation, Segmentation techniques, Public datasets, Characteristics of stroke segmentation},
  url      = {https://www.sciencedirect.com/science/article/pii/S0010482524005936},
}

@Article{Galdran2022,
  author    = {Galdran, Adrian and Anjos, Andr{\'e} and Dolz, Jos{\'e} and Chakor, Hadi and Lombaert, Herv{\'e} and Ayed, Ismail Ben},
  journal   = {Scientific Reports},
  title     = {State-of-the-art retinal vessel segmentation with minimalistic models},
  year      = {2022},
  number    = {1},
  pages     = {6174},
  volume    = {12},
  publisher = {Nature Publishing Group UK London},
}

@Article{Abdushkour2023,
  author    = {Abdushkour, Hesham and Soomro, Toufique A and Ali, Ahmed and Ali Jandan, Fayyaz and Jelinek, Herbert and Memon, Farida and Althobiani, Faisal and Mohammed Ghonaim, Saleh and Irfan, Muhammad},
  journal   = {PloS One},
  title     = {Enhancing fine retinal vessel segmentation: Morphological reconstruction and double thresholds filtering strategy},
  year      = {2023},
  number    = {7},
  pages     = {e0288792},
  volume    = {18},
  publisher = {Public Library of Science San Francisco, CA USA},
}

@Article{Cervantes2023,
  author   = {Jair Cervantes and Jared Cervantes and Farid García-Lamont and Arturo Yee-Rendon and Josué Espejel Cabrera and Laura Domínguez Jalili},
  journal  = {Neurocomputing},
  title    = {A comprehensive survey on segmentation techniques for retinal vessel segmentation},
  year     = {2023},
  issn     = {0925-2312},
  pages    = {126626},
  volume   = {556},
  abstract = {In recent years, enormous research has been carried out on the segmentation of blood vessels. Segmentation of blood vessels in retinal images is crucial for diagnosing, treating, evaluating clinical results, and early detection of eye disorders. A successful segmentation accurately reflects the blood vessels’ structure and helps obtain patterns that can be used to identify retinal disorders and diseases. Most recent research on vessel segmentation employs multiple processes to determine the appropriate segmentation. Finding the best techniques for segmentation is a complex process. In certain circumstances, it requires a thorough understanding of every step that can only be acquired through years of training. Comprehension and expertise in segmentation procedures are essential for accurate segmentation. This paper briefly introduces the segmentation of blood vessels in retinal images, describes many preprocessing and segmentation techniques, and summarizes challenges and trends. Furthermore, the limitations of the current systems will be identified.},
  doi      = {https://doi.org/10.1016/j.neucom.2023.126626},
  keywords = {Vessel segmentation, Preprocessing techniques, Segmentation techniques},
  url      = {https://www.sciencedirect.com/science/article/pii/S092523122300749X},
}

@Article{Primakov2022,
  author    = {Primakov, Sergey P and Ibrahim, Abdalla and van Timmeren, Janita E and Wu, Guangyao and Keek, Simon A and Beuque, Manon and Granzier, Ren{\'e}e WY and Lavrova, Elizaveta and Scrivener, Madeleine and Sanduleanu, Sebastian and others},
  journal   = {Nature communications},
  title     = {Automated detection and segmentation of non-small cell lung cancer computed tomography images},
  year      = {2022},
  number    = {1},
  pages     = {3423},
  volume    = {13},
  publisher = {Nature Publishing Group UK London},
}

@Article{Bokhorst2023,
  author    = {Bokhorst, John-Melle and Nagtegaal, Iris D and Fraggetta, Filippo and Vatrano, Simona and Mesker, Wilma and Vieth, Michael and van der Laak, Jeroen and Ciompi, Francesco},
  journal   = {Scientific Reports},
  title     = {Deep learning for multi-class semantic segmentation enables colorectal cancer detection and classification in digital pathology images},
  year      = {2023},
  number    = {1},
  pages     = {8398},
  volume    = {13},
  publisher = {Nature Publishing Group UK London},
}

@Article{Karol2024,
  author    = {Karol, Michal and Tabakov, Martin and Markowska-Kaczmar, Urszula and Fulawka, Lukasz},
  journal   = {Artificial Intelligence Review},
  title     = {Deep learning for cancer cell detection: do we need dedicated models?},
  year      = {2024},
  number    = {3},
  pages     = {53},
  volume    = {57},
  publisher = {Springer},
}

@Article{Bhuiyan2022,
  author         = {Bhuiyan, Md Roman and Abdullah, Junaidi},
  journal        = {Sensors},
  title          = {Detection on Cell Cancer Using the Deep Transfer Learning and Histogram Based Image Focus Quality Assessment},
  year           = {2022},
  issn           = {1424-8220},
  number         = {18},
  volume         = {22},
  abstract       = {In recent years, the number of studies using whole-slide imaging (WSIs) of histopathology slides has expanded significantly. For the development and validation of artificial intelligence (AI) systems, glass slides from retrospective cohorts including patient follow-up data have been digitized. It has become crucial to determine that the quality of such resources meets the minimum requirements for the development of AI in the future. The need for automated quality control is one of the obstacles preventing the clinical implementation of digital pathology work processes. As a consequence of the inaccuracy of scanners in determining the focus of the image, the resulting visual blur can render the scanned slide useless. Moreover, when scanned at a resolution of 20× or higher, the resulting picture size of a scanned slide is often enormous. Therefore, for digital pathology to be clinically relevant, computational algorithms must be used to rapidly and reliably measure the picture’s focus quality and decide if an image requires re-scanning. We propose a metric for evaluating the quality of digital pathology images that uses a sum of even-derivative filter bases to generate a human visual-system-like kernel, which is described as the inverse of the lens’ point spread function. This kernel is then used for a digital pathology image to change high-frequency image data degraded by the scanner’s optics and assess the patch-level focus quality. Through several studies, we demonstrate that our technique correlates with ground-truth z-level data better than previous methods, and is computationally efficient. Using deep learning techniques, our suggested system is able to identify positive and negative cancer cells in images. We further expand our technique to create a local slide-level focus quality heatmap, which can be utilized for automated slide quality control, and we illustrate our method’s value in clinical scan quality control by comparing it to subjective slide quality ratings. The proposed method, GoogleNet, VGGNet, and ResNet had accuracy values of 98.5%, 94.5%, 94.00%, and 95.00% respectively.},
  article-number = {7007},
  doi            = {10.3390/s22187007},
  pubmedid       = {36146356},
  url            = {https://www.mdpi.com/1424-8220/22/18/7007},
}

@Article{Kim2024,
  author         = {Kim, Tae Hoon and Krichen, Moez and Ojo, Stephen and Alamro, Meznah A. and Sampedro, Gabriel Avelino},
  journal        = {Diagnostics},
  title          = {TSSG-CNN: A Tuberculosis Semantic Segmentation-Guided Model for Detecting and Diagnosis Using the Adaptive Convolutional Neural Network},
  year           = {2024},
  issn           = {2075-4418},
  number         = {11},
  volume         = {14},
  abstract       = {Tuberculosis (TB) is an infectious disease caused by Mycobacterium. It primarily impacts the lungs but can also endanger other organs, such as the renal system, spine, and brain. When an infected individual sneezes, coughs, or speaks, the virus can spread through the air, which contributes to its high contagiousness. The goal is to enhance detection recognition with an X-ray image dataset. This paper proposed a novel approach, named the Tuberculosis Segmentation-Guided Diagnosis Model (TSSG-CNN) for Detecting Tuberculosis, using a combined semantic segmentation and adaptive convolutional neural network (CNN) architecture. The proposed approach is distinguished from most of the previously proposed approaches in that it uses the combination of a deep learning segmentation model with a follow-up classification model based on CNN layers to segment chest X-ray images more precisely as well as to improve the diagnosis of TB. It contrasts with other approaches like ILCM, which is optimized for sequential learning, and explainable AI approaches, which focus on explanations. Moreover, our model is beneficial for the simplified procedure of feature optimization from the perspectives of approach using the Mayfly Algorithm (MA). Other models, including simple CNN, Batch Normalized CNN (BN-CNN), and Dense CNN (DCNN), are also evaluated on this dataset to evaluate the effectiveness of the proposed approach. The performance of the TSSG-CNN model outperformed all the models with an impressive accuracy of 98.75% and an F1 score of 98.70%. The evaluation findings demonstrate how well the deep learning segmentation model works and the potential for further research. The results suggest that this is the most accurate strategy and highlight the potential of the TSSG-CNN Model as a useful technique for precise and early diagnosis of TB.},
  article-number = {1174},
  doi            = {10.3390/diagnostics14111174},
  pubmedid       = {38893700},
  url            = {https://www.mdpi.com/2075-4418/14/11/1174},
}

@Book{Organization2020,
  author    = {World Health Organization},
  publisher = {World Health Organization},
  title     = {Global tuberculosis report 2020},
  year      = {2020},
  pages     = {xxi, 208 p.},
  type      = {Publications},
}

@Article{Rajaraman2021,
  author         = {Rajaraman, Sivaramakrishnan and Folio, Les R. and Dimperio, Jane and Alderson, Philip O. and Antani, Sameer K.},
  journal        = {Diagnostics},
  title          = {Improved Semantic Segmentation of Tuberculosis—Consistent Findings in Chest X-rays Using Augmented Training of Modality-Specific U-Net Models with Weak Localizations},
  year           = {2021},
  issn           = {2075-4418},
  number         = {4},
  volume         = {11},
  abstract       = {Deep learning (DL) has drawn tremendous attention for object localization and recognition in both natural and medical images. U-Net segmentation models have demonstrated superior performance compared to conventional hand-crafted feature-based methods. Medical image modality-specific DL models are better at transferring domain knowledge to a relevant target task than those pretrained on stock photography images. This character helps improve model adaptation, generalization, and class-specific region of interest (ROI) localization. In this study, we train chest X-ray (CXR) modality-specific U-Nets and other state-of-the-art U-Net models for semantic segmentation of tuberculosis (TB)-consistent findings. Automated segmentation of such manifestations could help radiologists reduce errors and supplement decision-making while improving patient care and productivity. Our approach uses the publicly available TBX11K CXR dataset with weak TB annotations, typically provided as bounding boxes, to train a set of U-Net models. Next, we improve the results by augmenting the training data with weak localization, postprocessed into an ROI mask, from a DL classifier trained to classify CXRs as showing normal lungs or suspected TB manifestations. Test data are individually derived from the TBX11K CXR training distribution and other cross-institutional collections, including the Shenzhen TB and Montgomery TB CXR datasets. We observe that our augmented training strategy helped the CXR modality-specific U-Net models achieve superior performance with test data derived from the TBX11K CXR training distribution and cross-institutional collections (p < 0.05). We believe that this is the first study to i) use CXR modality-specific U-Nets for semantic segmentation of TB-consistent ROIs and ii) evaluate the segmentation performance while augmenting the training data with weak TB-consistent localizations.},
  article-number = {616},
  doi            = {10.3390/diagnostics11040616},
  pubmedid       = {33808240},
  url            = {https://www.mdpi.com/2075-4418/11/4/616},
}

@Article{Xie2023,
  author  = {Xie, Yutong and Yang, Bing and Guan, Qingbiao and Zhang, Jianpeng and Wu, Qi and Xia, Yong},
  journal = {arXiv preprint arXiv:2305.17937},
  title   = {Attention mechanisms in medical image segmentation: A survey},
  year    = {2023},
}

@InProceedings{Hatamizadeh2022,
  author    = {Hatamizadeh, Ali and Tang, Yucheng and Nath, Vishwesh and Yang, Dong and Myronenko, Andriy and Landman, Bennett and Roth, Holger R and Xu, Daguang},
  booktitle = {Proceedings of the IEEE/CVF winter conference on applications of computer vision},
  title     = {Unetr: Transformers for 3d medical image segmentation},
  year      = {2022},
  pages     = {574--584},
}

@Article{Chen2021,
  author  = {Chen, Jieneng and Lu, Yongyi and Yu, Qihang and Luo, Xiangde and Adeli, Ehsan and Wang, Yan and Lu, Le and Yuille, Alan L and Zhou, Yuyin},
  journal = {arXiv preprint arXiv:2102.04306},
  title   = {Transunet: Transformers make strong encoders for medical image segmentation},
  year    = {2021},
}

@Article{Ma2024a,
  author  = {Ma, Jun and Li, Feifei and Wang, Bo},
  journal = {arXiv preprint arXiv:2401.04722},
  title   = {U-mamba: Enhancing long-range dependency for biomedical image segmentation},
  year    = {2024},
}

@Article{Zhou2019,
  author    = {Zhou, Zongwei and Siddiquee, Md Mahfuzur Rahman and Tajbakhsh, Nima and Liang, Jianming},
  journal   = {IEEE transactions on medical imaging},
  title     = {Unet++: Redesigning skip connections to exploit multiscale features in image segmentation},
  year      = {2019},
  number    = {6},
  pages     = {1856--1867},
  volume    = {39},
  publisher = {IEEE},
}

@InProceedings{Huang2020a,
  author       = {Huang, Huimin and Lin, Lanfen and Tong, Ruofeng and Hu, Hongjie and Zhang, Qiaowei and Iwamoto, Yutaro and Han, Xianhua and Chen, Yen-Wei and Wu, Jian},
  booktitle    = {ICASSP 2020-2020 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  title        = {Unet 3+: A full-scale connected unet for medical image segmentation},
  year         = {2020},
  organization = {IEEE},
  pages        = {1055--1059},
}

@Article{Li2019,
  author    = {Li, Haoming and Fang, Jinghui and Liu, Shengfeng and Liang, Xiaowen and Yang, Xin and Mai, Zixin and Van, Manh The and Wang, Tianfu and Chen, Zhiyi and Ni, Dong},
  journal   = {IEEE journal of biomedical and health informatics},
  title     = {Cr-unet: A composite network for ovary and follicle segmentation in ultrasound images},
  year      = {2019},
  number    = {4},
  pages     = {974--983},
  volume    = {24},
  publisher = {IEEE},
}

@InProceedings{Azad2019,
  author    = {Azad, Reza and Asadi-Aghbolaghi, Maryam and Fathy, Mahmood and Escalera, Sergio},
  booktitle = {Proceedings of the IEEE/CVF international conference on computer vision workshops},
  title     = {Bi-directional ConvLSTM U-Net with densley connected convolutions},
  year      = {2019},
  pages     = {0--0},
}

@InProceedings{Guo2021,
  author       = {Guo, Changlu and Szemenyei, M{\'a}rton and Yi, Yugen and Wang, Wenle and Chen, Buer and Fan, Changqi},
  booktitle    = {2020 25th international conference on pattern recognition (ICPR)},
  title        = {Sa-unet: Spatial attention u-net for retinal vessel segmentation},
  year         = {2021},
  organization = {IEEE},
  pages        = {1236--1242},
}

@Article{Hai2019,
  author    = {Hai, Jinjin and Qiao, Kai and Chen, Jian and Tan, Hongna and Xu, Jingbo and Zeng, Lei and Shi, Dapeng and Yan, Bin},
  journal   = {Journal of healthcare engineering},
  title     = {Fully convolutional densenet with multiscale context for automated breast tumor segmentation},
  year      = {2019},
  number    = {1},
  pages     = {8415485},
  volume    = {2019},
  publisher = {Wiley Online Library},
}

@Misc{Lin2024,
  author        = {Tianyu Lin and Zhiguang Chen and Zhonghao Yan and Weijiang Yu and Fudan Zheng},
  title         = {Stable Diffusion Segmentation for Biomedical Images with Single-step Reverse Process},
  year          = {2024},
  archiveprefix = {arXiv},
  eprint        = {2406.18361},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2406.18361},
}

@Misc{Wu2023,
  author        = {Junde Wu and Rao Fu and Huihui Fang and Yu Zhang and Yehui Yang and Haoyi Xiong and Huiying Liu and Yanwu Xu},
  title         = {MedSegDiff: Medical Image Segmentation with Diffusion Probabilistic Model},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2211.00611},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2211.00611},
}

@Article{Sun2020,
  author        = {Liyan Sun and Jianxiong Wu and Xinghao Ding and Yue Huang and Guisheng Wang and Yizhou Yu},
  journal       = {CoRR},
  title         = {A Teacher-Student Framework for Semi-supervised Medical Image Segmentation From Mixed Supervision},
  year          = {2020},
  volume        = {abs/2010.12219},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/abs-2010-12219.bib},
  eprint        = {2010.12219},
  timestamp     = {Tue, 27 Oct 2020 11:22:08 +0100},
  url           = {https://arxiv.org/abs/2010.12219},
}

@Article{Li2024a,
  author   = {Boliang Li and Yan Wang and Yaming Xu and Chen Wu},
  journal  = {Biomedical Signal Processing and Control},
  title    = {DSST: A dual student model guided student–teacher framework for semi-supervised medical image segmentation},
  year     = {2024},
  issn     = {1746-8094},
  pages    = {105890},
  volume   = {90},
  abstract = {Numerous semi-supervised learning methods are emerging in medical image segmentation to reduce the dependency of deep learning models on pixel-level annotation data. Consistency regularization methods based on the Student–Teacher structure have achieved brilliant results in this domain. However, the current structures are unable to resolve the tight weight coupling satisfactorily between the teacher and student model, which leads to a decrease in the segmentation performance. In this paper, we propose a novel and practical semi-supervised learning framework, Dual-Student-Single-Teacher (DSST), to alleviate this problem. Particularly, the DSST framework consists of three segmentation models with the identical structure but different initial parameters, one serves as the teacher model and others as the student models, which employs an alternating manner to update the teacher model parameters. For the DSST framework, we present different supervised modes to sufficiently explore the enhancement of consistency regularization for model segmentation performance. Furthermore, we also introduce abundant and efficient input and feature perturbations for the proposed method. Finally, we evaluate our framework in three public medical image segmentation tasks, including Pancreas-CT, LA dataset, and cardiac segmentation on the ACDC dataset. Extensive experiments demonstrate that, compared with eight other superior semi-supervised methods, the DSST method obtains state-of-the-art segmentation performance and is an effective and generalizable framework. Code is available at https://github.com/LBL0704/DSST.},
  doi      = {https://doi.org/10.1016/j.bspc.2023.105890},
  keywords = {Semi-supervised learning, Medical image segmentation, Student–teacher framework, Consistency regularization},
  url      = {https://www.sciencedirect.com/science/article/pii/S174680942301323X},
}

@InProceedings{Ashok2022,
  author    = {Ashok, Malvika and Gupta, Abhishek},
  booktitle = {2022 7th International Conference on Communication and Electronics Systems (ICCES)},
  title     = {Comparative Study of TRANS - GAN Architecture for Bio-Medical Image Semantic Segmentation},
  year      = {2022},
  pages     = {1564-1570},
  doi       = {10.1109/ICCES54183.2022.9835992},
  keywords  = {Deep learning;Image segmentation;Neural networks;Semantics;Bifurcation;Transformers;Generative adversarial networks;Bio-Medical Imaging;Medical modalities;CT segmentation;Transformers;Generative Adversarial Networks Convolutional Neural Network;Traditional Architecture;Supervised and Unsupervised Transformers;Automatic segmentation;Deep learning},
}

@Article{Iqbal2022,
  author   = {Iqbal, Ahmed and Sharif, Muhammad and Yasmin, Mussarat and Raza, Mudassar and Aftab, Shabib},
  journal  = {International Journal of Multimedia Information Retrieval},
  title    = {Generative adversarial networks and its applications in the biomedical image segmentation: a comprehensive survey},
  year     = {2022},
  issn     = {2192-662X},
  number   = {3},
  pages    = {333--368},
  volume   = {11},
  abstract = {Recent advancements with deep generative models have proven significant potential in the task of image synthesis, detection, segmentation, and classification. Segmenting the medical images is considered a primary challenge in the biomedical imaging field. There have been various GANs-based models proposed in the literature to resolve medical segmentation challenges. Our research outcome has identified 151 papers; after the twofold screening, 138 papers are selected for the final survey. A comprehensive survey is conducted on GANs network application to medical image segmentation, primarily focused on various GANs-based models, performance metrics, loss function, datasets, augmentation methods, paper implementation, and source codes. Secondly, this paper provides a detailed overview of GANs network application in different human diseases segmentation. We conclude our research with critical discussion, limitations of GANs, and suggestions for future directions. We hope this survey is beneficial and increases awareness of GANs network implementations for biomedical image segmentation tasks.},
  doi      = {10.1007/s13735-022-00240-x},
  refid    = {Iqbal2022},
  url      = {https://doi.org/10.1007/s13735-022-00240-x},
}

@Article{Mirza2014,
  author        = {Mehdi Mirza and Simon Osindero},
  journal       = {CoRR},
  title         = {Conditional Generative Adversarial Nets},
  year          = {2014},
  volume        = {abs/1411.1784},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/MirzaO14.bib},
  eprint        = {1411.1784},
  timestamp     = {Mon, 13 Aug 2018 16:48:15 +0200},
  url           = {http://arxiv.org/abs/1411.1784},
}

@Article{Abbasi2019,
  author        = {Sajjad Abbasi and Mohsen Hajabdollahi and Nader Karimi and Shadrokh Samavi},
  journal       = {CoRR},
  title         = {Modeling Teacher-Student Techniques in Deep Neural Networks for Knowledge Distillation},
  year          = {2019},
  volume        = {abs/1912.13179},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1912-13179.bib},
  eprint        = {1912.13179},
  timestamp     = {Sat, 04 Jan 2020 19:40:16 +0100},
  url           = {http://arxiv.org/abs/1912.13179},
}

@Article{Lou2023,
  author   = {Ange Lou and Shuyue Guan and Murray Loew},
  journal  = {Computers in Biology and Medicine},
  title    = {CFPNet-M: A light-weight encoder-decoder based network for multimodal biomedical image real-time segmentation},
  year     = {2023},
  issn     = {0010-4825},
  pages    = {106579},
  volume   = {154},
  abstract = {—Deep learning techniques are proving instrumental in identifying, classifying, and quantifying patterns in medical images. Segmentation is one of the important applications in medical image analysis. The U-Net has become the predominant deep-learning approach to medical image segmentation tasks. Existing U-Net based models have limitations in several respects, however, including: the requirement for millions of parameters in the U-Net, which consumes considerable computational resources and memory; the lack of global information; and incomplete segmentation in difficult cases. To remove some of those limitations, we built on our previous work and applied two modifications to improve the U-Net model: 1) we designed and added the dilated channel-wise CNN module and 2) we simplified the U-shape network. We then proposed a novel light-weight architecture, the Channel-wise Feature Pyramid Network for Medicine (CFPNet-M). To evaluate our method, we selected five datasets from different imaging modalities: thermography, electron microscopy, endoscopy, dermoscopy, and digital retinal images. We compared its performance with several models having a variety of complexities. We used the Tanimoto similarity instead of the Jaccard index for gray-level image comparisons. The CFPNet-M achieves segmentation results on all five medical datasets that are comparable to existing methods, yet require only 8.8 MB memory, and just 0.65 million parameters, which is about 2% of U-Net. Unlike other deep-learning segmentation methods, this new approach is suitable for real-time application: its inference speed can reach 80 frames per second when implemented on a single RTX 2070Ti GPU with an input image size of 256 × 192 pixels.},
  doi      = {https://doi.org/10.1016/j.compbiomed.2023.106579},
  keywords = {CFPNet-M, Light-weight network, Medical image, Real-time segmentation, Tanimoto similarity},
  url      = {https://www.sciencedirect.com/science/article/pii/S0010482523000446},
}

@Article{Ilesanmi2021,
  author   = {Ilesanmi, Ademola E. and Chaumrattanakul, Utairat and Makhanov, Stanislav S.},
  journal  = {Journal of Ultrasound},
  title    = {Methods for the segmentation and classification of breast ultrasound images: a review},
  year     = {2021},
  issn     = {1876-7931},
  number   = {4},
  pages    = {367--382},
  volume   = {24},
  abstract = {Breast ultrasound (BUS) is one of the imaging modalities for the diagnosis and treatment of breast cancer. However, the segmentation and classification of BUS images is a challenging task. In recent years, several methods for segmenting and classifying BUS images have been studied. These methods use BUS datasets for evaluation. In addition, semantic segmentation algorithms have gained prominence for segmenting medical images.},
  doi      = {10.1007/s40477-020-00557-5},
  refid    = {Ilesanmi2021},
  url      = {https://doi.org/10.1007/s40477-020-00557-5},
}

@Article{Chen2023b,
  author   = {Chen, Gongping and Li, Lei and Dai, Yu and Zhang, Jianxun and Yap, Moi Hoon},
  journal  = {IEEE Transactions on Medical Imaging},
  title    = {AAU-Net: An Adaptive Attention U-Net for Breast Lesions Segmentation in Ultrasound Images},
  year     = {2023},
  number   = {5},
  pages    = {1289-1300},
  volume   = {42},
  doi      = {10.1109/TMI.2022.3226268},
  keywords = {Image segmentation;Convolution;Breast;Lesions;Ultrasonic imaging;Kernel;Breast tumors;Ultrasound images;breast tumors segmentation;hybrid attention;adaptive learning;deep learning},
}

@InProceedings{Vianna2021,
  author    = {Vianna, P. O. and Farias, R. and Pereira, W. C. A.},
  booktitle = {2021 Global Medical Engineering Physics Exchanges/Pan American Health Care Exchanges (GMEPE/PAHCE)},
  title     = {Performance of the SegNet in the Segmentation of Breast Ultrasound Lesions},
  year      = {2021},
  pages     = {1-4},
  doi       = {10.1109/GMEPE/PAHCE50215.2021.9434877},
  keywords  = {Image segmentation;Ultrasonic imaging;Medical services;Breast;Feature extraction;Generative adversarial networks;Performance analysis;breast ultrasound;deep learning;SegNet},
}

@Misc{Simonyan2014a,
  author        = {Karen Simonyan and Andrea Vedaldi and Andrew Zisserman},
  title         = {Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps},
  year          = {2014},
  archiveprefix = {arXiv},
  eprint        = {1312.6034},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/1312.6034},
}

@Article{Ismail2021,
  author        = {Aya Abdelsalam Ismail and H{\'{e}}ctor Corrada Bravo and Soheil Feizi},
  journal       = {CoRR},
  title         = {Improving Deep Learning Interpretability by Saliency Guided Training},
  year          = {2021},
  volume        = {abs/2111.14338},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/abs-2111-14338.bib},
  eprint        = {2111.14338},
  timestamp     = {Wed, 01 Dec 2021 15:16:43 +0100},
  url           = {https://arxiv.org/abs/2111.14338},
}

@Article{Ning2022,
  author   = {Ning, Zhenyuan and Zhong, Shengzhou and Feng, Qianjin and Chen, Wufan and Zhang, Yu},
  journal  = {IEEE Transactions on Medical Imaging},
  title    = {SMU-Net: Saliency-Guided Morphology-Aware U-Net for Breast Lesion Segmentation in Ultrasound Image},
  year     = {2022},
  number   = {2},
  pages    = {476-490},
  volume   = {41},
  doi      = {10.1109/TMI.2021.3116087},
  keywords = {Lesions;Image segmentation;Streaming media;Breast;Ultrasonic imaging;Image edge detection;Shape;Breast lesion segmentation;ultrasound image;saliency-guided;morphology-aware;deep learning},
}

@Article{Vakanski2020,
  author   = {Aleksandar Vakanski and Min Xian and Phoebe E. Freer},
  journal  = {Ultrasound in Medicine \& Biology},
  title    = {Attention-Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images},
  year     = {2020},
  issn     = {0301-5629},
  number   = {10},
  pages    = {2819-2833},
  volume   = {46},
  abstract = {Incorporating human domain knowledge for breast tumor diagnosis is challenging because shape, boundary, curvature, intensity or other common medical priors vary significantly across patients and cannot be employed. This work proposes a new approach to integrating visual saliency into a deep learning model for breast tumor segmentation in ultrasound images. Visual saliency refers to image maps containing regions that are more likely to attract radiologists’ visual attention. The proposed approach introduces attention blocks into a U-Net architecture and learns feature representations that prioritize spatial regions with high saliency levels. The validation results indicate increased accuracy for tumor segmentation relative to models without salient attention layers. The approach achieved a Dice similarity coefficient (DSC) of 90.5% on a data set of 510 images. The salient attention model has the potential to enhance accuracy and robustness in processing medical images of other organs, by providing a means to incorporate task-specific knowledge into deep learning architectures.},
  doi      = {https://doi.org/10.1016/j.ultrasmedbio.2020.06.015},
  keywords = {Breast ultrasound, Medical image segmentation, Visual saliency, Domain knowledge-enriched learning},
  url      = {https://www.sciencedirect.com/science/article/pii/S0301562920302878},
}

@Article{Cao2020,
  author    = {Cao, Haichao and Pu, Shiliang and Tan, Wenming and Tong, Junyan and Zhang, Di},
  journal   = {IEEE Access},
  title     = {Multi-Tasking U-Shaped Network for Benign and Malignant Classification of Breast Masses},
  year      = {2020},
  issn      = {2169-3536},
  pages     = {223396--223404},
  volume    = {8},
  doi       = {10.1109/access.2020.3042889},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
}

@Article{Zhou2022a,
  author     = {Zhou, Quan and Wang, Qianwen and Bao, Yunchao and Kong, Lingjun and Jin, Xin and Ou, Weihua},
  journal    = {Comput. Electr. Eng.},
  title      = {LAEDNet: A Lightweight Attention Encoder–Decoder Network for ultrasound medical image segmentation},
  year       = {2022},
  issn       = {0045-7906},
  month      = {apr},
  number     = {C},
  volume     = {99},
  address    = {USA},
  doi        = {10.1016/j.compeleceng.2022.107777},
  issue_date = {Apr 2022},
  keywords   = {Lightweight network, Medical ultrasound image segmentation, Encoder–decoder network, Visual attention, EfficientNet},
  numpages   = {12},
  publisher  = {Pergamon Press, Inc.},
  url        = {https://doi.org/10.1016/j.compeleceng.2022.107777},
}

@Article{Cai2024,
  author   = {Cai, Fenglin and Wen, Jiaying and He, Fangzhou and Xia, Yulong and Xu, Weijun and Zhang, Yong and Jiang, Li and Li, Jie},
  journal  = {Journal of Imaging Informatics in Medicine},
  title    = {SC-Unext: A Lightweight Image Segmentation Model with Cellular Mechanism for Breast Ultrasound Tumor Diagnosis},
  year     = {2024},
  issn     = {2948-2933},
  number   = {4},
  pages    = {1505--1515},
  volume   = {37},
  abstract = {Automatic breast ultrasound image segmentation plays an important role in medical image processing. However, current methods for breast ultrasound segmentation suffer from high computational complexity and large model parameters, particularly when dealing with complex images. In this paper, we take the Unext network as a basis and utilize its encoder-decoder features. And taking inspiration from the mechanisms of cellular apoptosis and division, we design apoptosis and division algorithms to improve model performance. We propose a novel segmentation model which integrates the division and apoptosis algorithms and introduces spatial and channel convolution blocks into the model. Our proposed model not only improves the segmentation performance of breast ultrasound tumors, but also reduces the model parameters and computational resource consumption time. The model was evaluated on the breast ultrasound image dataset and our collected dataset. The experiments show that the SC-Unext model achieved Dice scores of 75.29% and accuracy of 97.09% on the BUSI dataset, and on the collected dataset, it reached Dice scores of 90.62% and accuracy of 98.37%. Meanwhile, we conducted a comparison of the model’s inference speed on CPUs to verify its efficiency in resource-constrained environments. The results indicated that the SC-Unext model achieved an inference speed of 92.72 ms per instance on devices equipped only with CPUs. The model’s number of parameters and computational resource consumption are 1.46M and 2.13 GFlops, respectively, which are lower compared to other network models. Due to its lightweight nature, the model holds significant value for various practical applications in the medical field.},
  doi      = {10.1007/s10278-024-01042-9},
  refid    = {Cai2024},
  url      = {https://doi.org/10.1007/s10278-024-01042-9},
}

@Misc{Valanarasu2022,
  author        = {Jeya Maria Jose Valanarasu and Vishal M. Patel},
  title         = {UNeXt: MLP-based Rapid Medical Image Segmentation Network},
  year          = {2022},
  archiveprefix = {arXiv},
  eprint        = {2203.04967},
  primaryclass  = {eess.IV},
  url           = {https://arxiv.org/abs/2203.04967},
}

@Comment{jabref-meta: databaseType:bibtex;}
