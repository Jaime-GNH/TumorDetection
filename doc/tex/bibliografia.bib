@IEEEtranBSTCTL{Hu2020,
  author   = {Hu, Xuegang and Wang, Haibo},
  comment  = {EFSNet.},
  doi      = {10.1109/ACCESS.2020.2987080},
  journal  = {IEEE Access},
  keywords = {Semantics;Image segmentation;Feature extraction;Decoding;Real-time systems;Image resolution;Performance evaluation;Resource-constrained;semantic segmentation;continuous shuffle dilated convolution;real-time},
  pages    = {70913-70924},
  title    = {Efficient Fast Semantic Segmentation Using Continuous Shuffle Dilated Convolutions},
  volume   = {8},
  year     = {2020},
}

@IEEEtranBSTCTL{paszke2016enet,
  author  = {Paszke, Adam and Chaurasia, Abhishek and Kim, Sangpil and Culurciello, Eugenio},
  comment = {ENet},
  journal = {arXiv preprint arXiv:1606.02147},
  title   = {Enet: A deep neural network architecture for real-time semantic segmentation},
  year    = {2016},
}

@IEEEtranBSTCTL{Zhang2019,
  abstract = {Semantic segmentation is a challenging problem in computer vision. Many applications, such as autonomous driving and robot navigation with urban road scene, need accurate and efficient segmentation. Most state-of-the-art methods focus on accuracy, rather than efficiency. In this paper, we propose a more efficient neural network architecture, which has fewer parameters, for semantic segmentation in the urban road scene. An asymmetric encoder-decoder structure based on ResNet is used in our model. In the first stage of encoder, we use continuous factorized block to extract low-level features. Continuous dilated block is applied in the second stage, which ensures that the model has a larger view field, while keeping the model small-scale and shallow. The down sampled features from encoder are up sampled with decoder to the same-size output as the input image and the details refined. Our model can achieve end-to-end and pixel-to-pixel training without pretraining from scratch. The parameters of our model are only 0.2M, 100× less than those of others such as SegNet, etc. Experiments are conducted on five public road scene datasets (CamVid, CityScapes, Gatech, KITTI Road Detection, and KITTI Semantic Segmentation), and the results demonstrate that our model can achieve better performance.},
  author   = {Zhang, Xuetao and Chen, Zhenxue and Wu, Q. M. Jonathan and Cai, Lei and Lu, Dan and Li, Xianming},
  comment  = {FSSNet},
  doi      = {10.1109/TII.2018.2849348},
  issn     = {1941-0050},
  journal  = {IEEE Transactions on Industrial Informatics},
  keywords = {Semantics;Convolution;Image segmentation;Standards;Informatics;Computer vision;Computer architecture;Convolutional neural network (CNN);real-time;ResNet;scene perception;semantic segmentation},
  month    = {Feb},
  number   = {2},
  pages    = {1183-1192},
  title    = {Fast Semantic Segmentation for Scene Perception},
  volume   = {15},
  year     = {2019},
}

@Article{7803544,
  author   = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title    = {SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation},
  year     = {2017},
  issn     = {1939-3539},
  month    = {Dec},
  number   = {12},
  pages    = {2481-2495},
  volume   = {39},
  abstract = {We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1] . The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN [2] and also with the well known DeepLab-LargeFOV [3] , DeconvNet [4] architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures and can be trained end-to-end using stochastic gradient descent. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. These quantitative assessments show that SegNet provides good performance with competitive inference time and most efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.},
  comment  = {SegNet. Benchmark. History.},
  doi      = {10.1109/TPAMI.2016.2644615},
  keywords = {Decoding;Neural networks;Training;Computer architecture;Image segmentation;Semantics;Convolutional codes;Deep convolutional neural networks;semantic pixel-wise segmentation;indoor scenes;road scenes;encoder;decoder;pooling;upsampling},
}

@InProceedings{7780459,
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Deep Residual Learning for Image Recognition},
  year      = {2016},
  month     = {June},
  pages     = {770-778},
  abstract  = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  comment   = {Bottleneck unit},
  doi       = {10.1109/CVPR.2016.90},
  issn      = {1063-6919},
  keywords  = {Training;Degradation;Complexity theory;Image recognition;Neural networks;Visualization;Image segmentation},
}

@InProceedings{8578814,
  author    = {Zhang, Xiangyu and Zhou, Xinyu and Lin, Mengxiao and Sun, Jian},
  booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  title     = {ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices},
  year      = {2018},
  month     = {June},
  pages     = {6848-6856},
  abstract  = {We introduce an extremely computation-efficient CNN architecture named ShuffleNet, which is designed specially for mobile devices with very limited computing power (e.g., 10-150 MFLOPs). The new architecture utilizes two new operations, pointwise group convolution and channel shuffle, to greatly reduce computation cost while maintaining accuracy. Experiments on ImageNet classification and MS COCO object detection demonstrate the superior performance of ShuffleNet over other structures, e.g. lower top-1 error (absolute 7.8%) than recent MobileNet [12] on ImageNet classification task, under the computation budget of 40 MFLOPs. On an ARM-based mobile device, ShuffleNet achieves ~13× actual speedup over AlexNet while maintaining comparable accuracy.},
  doi       = {10.1109/CVPR.2018.00716},
  issn      = {2575-7075},
  keywords  = {Convolution;Complexity theory;Computer architecture;Mobile handsets;Computational modeling;Task analysis;Neural networks},
}

@InProceedings{Ma2018,
  author    = {Ma, Ningning and Zhang, Xiangyu and Zheng, Hai-Tao and Sun, Jian},
  booktitle = {Computer Vision -- ECCV 2018},
  title     = {ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design},
  year      = {2018},
  address   = {Cham},
  editor    = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
  pages     = {122--138},
  publisher = {Springer International Publishing},
  abstract  = {Currently, the neural network architecture design is mostly guided by the indirect metric of computation complexity, i.e., FLOPs. However, the direct metric, e.g., speed, also depends on the other factors such as memory access cost and platform characterics. Thus, this work proposes to evaluate the direct metric on the target platform, beyond only considering FLOPs. Based on a series of controlled experiments, this work derives several practical guidelines for efficient network design. Accordingly, a new architecture is presented, called ShuffleNet V2. Comprehensive ablation experiments verify that our model is the state-of-the-art in terms of speed and accuracy tradeoff.},
  isbn      = {978-3-030-01264-9},
}

@InProceedings{Loni2019,
  author    = {Mohammad Loni and Ali Zoljodi and Sima Sinaei and Masoud Daneshtalab and Mikael Sj{\"o}din},
  booktitle = {International Conference on Artificial Neural Networks},
  title     = {NeuroPower: Designing Energy Efficient Convolutional Neural Network Architecture for Embedded Systems},
  year      = {2019},
  comment   = {Sobre consumo de energía por tamaño de los modelos},
  url       = {https://api.semanticscholar.org/CorpusID:202402304},
}

@Article{Shin2016,
  author        = {Hoo{-}Chang Shin and Holger R. Roth and Mingchen Gao and Le Lu and Ziyue Xu and Isabella Nogues and Jianhua Yao and Daniel J. Mollura and Ronald M. Summers},
  journal       = {CoRR},
  title         = {Deep Convolutional Neural Networks for Computer-Aided Detection: {CNN} Architectures, Dataset Characteristics and Transfer Learning},
  year          = {2016},
  volume        = {abs/1602.03409},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/ShinRGLXNYMS16.bib},
  eprint        = {1602.03409},
  timestamp     = {Fri, 07 Oct 2022 14:19:05 +0200},
  url           = {http://arxiv.org/abs/1602.03409},
}

@Article{Chenna2023,
  author  = {Chenna, Dwith},
  journal = {arXiv preprint arXiv:2311.12816},
  title   = {Evolution of Convolutional Neural Network (CNN): Compute vs Memory bandwidth for Edge AI},
  year    = {2023},
  comment = {Ojo preprint!
Evolucion de las CNNs},
}

@Article{Krizhevsky2012,
  author  = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal = {Advances in neural information processing systems},
  title   = {Imagenet classification with deep convolutional neural networks},
  year    = {2012},
  volume  = {25},
  comment = {AlexNet. History DCNN},
}

@Article{Network1989,
  author    = {Network, Back-Propagation},
  title     = {Handwritten digit recognition with},
  year      = {1989},
  comment   = {Densa clasificacion. Historia.},
  publisher = {Citeseer},
}

@Article{turaga2010convolutional,
  author    = {Turaga, Srinivas C and Murray, Joseph F and Jain, Viren and Roth, Fabian and Helmstaedter, Moritz and Briggman, Kevin and Denk, Winfried and Seung, H Sebastian},
  journal   = {Neural computation},
  title     = {Convolutional networks can learn to generate affinity graphs for image segmentation},
  year      = {2010},
  number    = {2},
  pages     = {511--538},
  volume    = {22},
  comment   = {Image Segmentation},
  publisher = {MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…},
}

@Article{Siddique2021,
  author   = {Siddique, Nahian and Paheding, Sidike and Elkin, Colin P. and Devabhaktuni, Vijay},
  journal  = {IEEE Access},
  title    = {U-Net and Its Variants for Medical Image Segmentation: A Review of Theory and Applications},
  year     = {2021},
  pages    = {82031-82057},
  volume   = {9},
  comment  = {U-Net history in Image segmentation},
  doi      = {10.1109/ACCESS.2021.3086020},
  keywords = {Image segmentation;Convolution;Biomedical imaging;Three-dimensional displays;Logic gates;Deep learning;Computer architecture;Biomedical imaging;deep learning;neural network architecture;segmentation;U-net},
}

@Article{Du2020,
  author  = {Du, Getao and Cao, Xu and Liang, Jimin and Chen, Xueli and Zhan, Yonghua},
  journal = {Journal of Imaging Science \& Technology},
  title   = {Medical Image Segmentation based on U-Net: A Review.},
  year    = {2020},
  number  = {2},
  volume  = {64},
  comment = {U-Net Medical Image Segmentation},
}

@Article{Guo2018,
  author    = {Guo, Yanming and Liu, Yu and Georgiou, Theodoros and Lew, Michael S},
  journal   = {International journal of multimedia information retrieval},
  title     = {A review of semantic segmentation using deep neural networks},
  year      = {2018},
  pages     = {87--93},
  volume    = {7},
  comment   = {Semantic Segmentation History},
  publisher = {Springer},
}

@Article{Islam2022,
  author  = {Islam, Khawar},
  journal = {arXiv preprint arXiv:2203.01536},
  title   = {Recent advances in vision transformer: A survey and outlook of recent work},
  year    = {2022},
  comment = {Vision Transformer History},
}

@Article{Jiang2022,
  author  = {Baode Jiang and Xiaoya An and Shaofen Xu and Zhanlong Chen},
  journal = {Journal of the Indian Society of Remote Sensing},
  title   = {Intelligent Image Semantic Segmentation: A Review Through Deep Learning Techniques for Remote Sensing Image Analysis},
  year    = {2022},
  pages   = {1865-1878},
  volume  = {51},
  url     = {https://api.semanticscholar.org/CorpusID:246075398},
}

@Article{Yang2022,
  author  = {Cheng Yang and Hongjun Guo},
  journal = {Mathematical Problems in Engineering},
  title   = {A Method of Image Semantic Segmentation Based on PSPNet},
  year    = {2022},
  comment = {Pyramid Semantic Segmentation History},
  url     = {https://api.semanticscholar.org/CorpusID:251511103},
}

@InBook{Rojas1996,
  author    = {Rojas, Ra{\'u}l},
  pages     = {149--182},
  publisher = {Springer Berlin Heidelberg},
  title     = {The Backpropagation Algorithm},
  year      = {1996},
  address   = {Berlin, Heidelberg},
  isbn      = {978-3-642-61068-4},
  abstract  = {We saw in the last chapter that multilayered networks are capable of computing a wider range of Boolean functions than networks with a single layer of computing units. However the computational effort needed for finding the correct combination of weights increases substantially when more parameters and more complicated topologies are considered. In this chapter we discuss a popular learning method capable of handling such large learning problems---the backpropagation algorithm. This numerical method was used by different research communities in different contexts, was discovered and rediscovered, until in 1985 it found its way into connectionist AI mainly through the work of the PDP group [382]. It has been one of the most studied and used algorithms for neural networks learning ever since.},
  booktitle = {Neural Networks: A Systematic Introduction},
  comment   = {History NNs},
  doi       = {10.1007/978-3-642-61068-4_7},
  url       = {https://doi.org/10.1007/978-3-642-61068-4_7},
}

@Article{LeCun1989,
  author  = {LeCun, Yann and Boser, Bernhard and Denker, John and Henderson, Donnie and Howard, Richard and Hubbard, Wayne and Jackel, Lawrence},
  journal = {Advances in neural information processing systems},
  title   = {Handwritten digit recognition with a back-propagation network},
  year    = {1989},
  volume  = {2},
  comment = {First CNN

This 1989 paper, published in the proceedings of the Neural Information Processing Systems (NIPS) conference, is considered the first work to use backpropagation to train the convolution kernels of a CNN for image recognition tasks like handwritten digit classification

The CNN architecture used in this paper was later named LeNet-5 and published in 1995.},
}

@Article{LeCun1995,
  author  = {LeCun, Yann and Jackel, Lawrence D and Bottou, L{\'e}on and Cortes, Corinna and Denker, John S and Drucker, Harris and Guyon, Isabelle and Muller, Urs A and Sackinger, Eduard and Simard, Patrice and others},
  journal = {Neural networks: the statistical mechanics perspective},
  title   = {Learning algorithms for classification: A comparison on handwritten digit recognition},
  year    = {1995},
  number  = {276},
  pages   = {2},
  volume  = {261},
  comment = {LeNet primeras CNNs.},
}

@InProceedings{Ronneberger2015,
  author       = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  booktitle    = {Medical image computing and computer-assisted intervention--MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18},
  title        = {U-net: Convolutional networks for biomedical image segmentation},
  year         = {2015},
  organization = {Springer},
  pages        = {234--241},
  comment      = {Primera U-Net en medicina. History DCNN.

U-Net benchmark},
}

@Article{Krogh1991,
  author  = {Krogh, Anders and Hertz, John},
  journal = {Advances in neural information processing systems},
  title   = {A simple weight decay can improve generalization},
  year    = {1991},
  volume  = {4},
  comment = {Weight Decay},
}

@InProceedings{Nair2010,
  author    = {Nair, Vinod and Hinton, Geoffrey E},
  booktitle = {Proceedings of the 27th international conference on machine learning (ICML-10)},
  title     = {Rectified linear units improve restricted boltzmann machines},
  year      = {2010},
  pages     = {807--814},
  comment   = {ReLU activation},
}

@Article{Lee2020,
  author    = {Lee, Sanghun and Lee, Chulhee},
  journal   = {Multimedia Tools and Applications},
  title     = {Revisiting spatial dropout for regularizing convolutional neural networks},
  year      = {2020},
  number    = {45},
  pages     = {34195--34207},
  volume    = {79},
  comment   = {Spatial Dropout paper.

"...dropout between channels in the CNNs can be functionally similar to dropout in the FCNNs, and spatial dropout can be an effective way to take advantage of the dropout technique for regularizing."},
  publisher = {Springer},
}

@InProceedings{Ioffe2015,
  author       = {Ioffe, Sergey and Szegedy, Christian},
  booktitle    = {International conference on machine learning},
  title        = {Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  year         = {2015},
  organization = {pmlr},
  pages        = {448--456},
  comment      = {BatchNormalization paper},
}

@Article{Ba2016,
  author  = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal = {arXiv preprint arXiv:1607.06450},
  title   = {Layer normalization},
  year    = {2016},
  comment = {Layer Normalization},
}

@InProceedings{He2015,
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle = {Proceedings of the IEEE international conference on computer vision},
  title     = {Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
  year      = {2015},
  pages     = {1026--1034},
  comment   = {Parametrized Rectified Linear Unit, Kaiming Uniform, PReLU},
}

@Article{Kingma2014,
  author  = {Kingma, Diederik P and Ba, Jimmy},
  journal = {arXiv preprint arXiv:1412.6980},
  title   = {Adam: A method for stochastic optimization},
  year    = {2014},
}

@InProceedings{Redmon2016,
  author    = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  title     = {You only look once: Unified, real-time object detection},
  year      = {2016},
  pages     = {779--788},
  comment   = {YOLO history},
}

@InProceedings{Long2015,
  author    = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  title     = {Fully convolutional networks for semantic segmentation},
  year      = {2015},
  pages     = {3431--3440},
  comment   = {Semantic Segmentation History.

FCN.},
}

@Article{Ren2015,
  author  = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  journal = {Advances in neural information processing systems},
  title   = {Faster r-cnn: Towards real-time object detection with region proposal networks},
  year    = {2015},
  volume  = {28},
  comment = {Faster-R-CNN. Real time object detection.},
}

@InProceedings{Girshick2015,
  author    = {Girshick, Ross},
  booktitle = {Proceedings of the IEEE international conference on computer vision},
  title     = {Fast r-cnn},
  year      = {2015},
  pages     = {1440--1448},
  comment   = {Fast R-CNN. History.},
}

@InProceedings{Girshick2014,
  author    = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  title     = {Rich feature hierarchies for accurate object detection and semantic segmentation},
  year      = {2014},
  pages     = {580--587},
  comment   = {R-CNN. History.},
}

@Article{He2015a,
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  journal   = {IEEE transactions on pattern analysis and machine intelligence},
  title     = {Spatial pyramid pooling in deep convolutional networks for visual recognition},
  year      = {2015},
  number    = {9},
  pages     = {1904--1916},
  volume    = {37},
  comment   = {Spatial Pyramid Pooling. HIstory.},
  publisher = {IEEE},
}

@Article{Chen2014,
  author  = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L},
  journal = {arXiv preprint arXiv:1412.7062},
  title   = {Semantic image segmentation with deep convolutional nets and fully connected crfs},
  year    = {2014},
  comment = {Semantic Image Segmentation DCNN. History.},
}

@Article{Farabet2012,
  author    = {Farabet, Clement and Couprie, Camille and Najman, Laurent and LeCun, Yann},
  journal   = {IEEE transactions on pattern analysis and machine intelligence},
  title     = {Learning hierarchical features for scene labeling},
  year      = {2012},
  number    = {8},
  pages     = {1915--1929},
  volume    = {35},
  comment   = {Learning hierarchical features.},
  publisher = {IEEE},
}

@Article{Srivastava2014,
  author    = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal   = {The journal of machine learning research},
  title     = {Dropout: a simple way to prevent neural networks from overfitting},
  year      = {2014},
  number    = {1},
  pages     = {1929--1958},
  volume    = {15},
  comment   = {Dropout first paper. History.},
  publisher = {JMLR. org},
}

@InProceedings{Szegedy2016,
  author    = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  title     = {Rethinking the inception architecture for computer vision},
  year      = {2016},
  pages     = {2818--2826},
  comment   = {Rethinking Inception. Influencia clave para la EFSNet.},
}

@InProceedings{Szegedy2017,
  author    = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander},
  booktitle = {Proceedings of the AAAI conference on artificial intelligence},
  title     = {Inception-v4, inception-resnet and the impact of residual connections on learning},
  year      = {2017},
  number    = {1},
  volume    = {31},
  comment   = {Inception v-4. Influencia para las residual connections de EFSNet.},
}

@InProceedings{He2016,
  author       = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle    = {Computer Vision--ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11--14, 2016, Proceedings, Part IV 14},
  title        = {Identity mappings in deep residual networks},
  year         = {2016},
  organization = {Springer},
  pages        = {630--645},
  comment      = {Identity mappings. Influencia para los residual blocks de EFSNet.},
}

@Article{Jaderberg2015,
  author  = {Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and others},
  journal = {Advances in neural information processing systems},
  title   = {Spatial transformer networks},
  year    = {2015},
  volume  = {28},
  comment = {Spatial transformers. Vía de desarrollo descartada.},
}

@InProceedings{Szegedy2015,
  author    = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  title     = {Going deeper with convolutions},
  year      = {2015},
  pages     = {1--9},
  comment   = {GoogLeNet. Residual Blocks influence.},
}

@Article{Simonyan2014,
  author  = {Simonyan, Karen and Zisserman, Andrew},
  journal = {arXiv preprint arXiv:1409.1556},
  title   = {Very deep convolutional networks for large-scale image recognition},
  year    = {2014},
  comment = {ConvNet. History. VGGNet},
}

@InProceedings{Mehta2018,
  author    = {Mehta, Sachin and Rastegari, Mohammad and Caspi, Anat and Shapiro, Linda and Hajishirzi, Hannaneh},
  booktitle = {Proceedings of the european conference on computer vision (ECCV)},
  title     = {Espnet: Efficient spatial pyramid of dilated convolutions for semantic segmentation},
  year      = {2018},
  pages     = {552--568},
  comment   = {ESPNet. Influencia EFSNet.},
}

@Article{Howard2017,
  author  = {Howard, Andrew G and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  journal = {arXiv preprint arXiv:1704.04861},
  title   = {Mobilenets: Efficient convolutional neural networks for mobile vision applications},
  year    = {2017},
  comment = {MobileNet. Real-time history.},
}

@Article{Chen2024,
  author  = {Chen, Dong and Liu, Ning and Zhu, Yichen and Che, Zhengping and Ma, Rui and Zhang, Fachao and Mou, Xiaofeng and Chang, Yi and Tang, Jian},
  journal = {arXiv preprint arXiv:2402.00084},
  title   = {EPSD: Early Pruning with Self-Distillation for Efficient Model Compression},
  year    = {2024},
  comment = {EPSD. Introducción.},
}

@InProceedings{Zhou2022,
  author    = {Zhou, ZhaoJing and Zhou, Yun and Jiang, Zhuqing and Men, Aidong and Wang, Haiying},
  booktitle = {ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  title     = {An Efficient Method for Model Pruning Using Knowledge Distillation with Few Samples},
  year      = {2022},
  pages     = {2515-2519},
  comment   = {PFDD. Introducción},
  doi       = {10.1109/ICASSP43922.2022.9746024},
  keywords  = {Training;Measurement;Deep learning;Image coding;Convolution;Conferences;Neural networks;Network compression;knowledge distillation;few samples},
}

@InProceedings{Park2022,
  author       = {Park, Jinhyuk and No, Albert},
  booktitle    = {European Conference on Computer Vision},
  title        = {Prune your model before distill it},
  year         = {2022},
  organization = {Springer},
  pages        = {120--136},
  comment      = {Prune before distill. Introduction.},
}

@Article{Liao2023,
  author  = {Liao, Baohao and Meng, Yan and Monz, Christof},
  journal = {arXiv preprint arXiv:2305.16742},
  title   = {Parameter-efficient fine-tuning without introducing new latency},
  year    = {2023},
  comment = {PaFi. Introducción.},
}

@Article{Hu2021,
  author  = {Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal = {arXiv preprint arXiv:2106.09685},
  title   = {Lora: Low-rank adaptation of large language models},
  year    = {2021},
  comment = {LoRA. Introducción},
}

@Article{Dettmers2024,
  author  = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal = {Advances in Neural Information Processing Systems},
  title   = {Qlora: Efficient finetuning of quantized llms},
  year    = {2024},
  volume  = {36},
  comment = {QLoRA. Introducción.},
}

@Article{Wang2023,
  author  = {Wang, Hongyu and Ma, Shuming and Dong, Li and Huang, Shaohan and Wang, Huaijie and Ma, Lingxiao and Yang, Fan and Wang, Ruiping and Wu, Yi and Wei, Furu},
  journal = {arXiv preprint arXiv:2310.11453},
  title   = {Bitnet: Scaling 1-bit transformers for large language models},
  year    = {2023},
  comment = {BitNet. Introducción.},
}

@Article{AlDhabyani2020,
  author    = {Al-Dhabyani, Walid and Gomaa, Mohammed and Khaled, Hussien and Fahmy, Aly},
  journal   = {Data in brief},
  title     = {Dataset of breast ultrasound images},
  year      = {2020},
  pages     = {104863},
  volume    = {28},
  comment   = {BUSI Dataset},
  publisher = {Elsevier},
}

@Article{Pawlowska2024,
  author   = {Pawłowska, Anna and Ćwierz-Pieńkowska, Anna and Domalik, Agnieszka and Jaguś, Dominika and Kasprzak, Piotr and Matkowski, Rafał and Fura, Łukasz and Nowicki, Andrzej and Żołek, Norbert},
  journal  = {Scientific Data},
  title    = {Curated benchmark dataset for ultrasound based breast lesion analysis},
  year     = {2024},
  issn     = {2052-4463},
  number   = {1},
  pages    = {148},
  volume   = {11},
  abstract = {A new detailed dataset of breast ultrasound scans (BrEaST) containing images of benign and malignant lesions as well as normal tissue examples, is presented. The dataset consists of 256 breast scans collected from 256 patients. Each scan was manually annotated and labeled by a radiologist experienced in breast ultrasound examination. In particular, each tumor was identified in the image using a freehand annotation and labeled according to BIRADS features and lexicon. The histopathological classification of the tumor was also provided for patients who underwent a biopsy. The BrEaST dataset is the first breast ultrasound dataset containing patient-level labels, image-level annotations, and tumor-level labels with all cases confirmed by follow-up care or core needle biopsy result. To enable research into breast disease detection, tumor segmentation and classification, the BrEaST dataset is made publicly available with the CC-BY 4.0 license.},
  comment  = {BrEaST},
  doi      = {10.1038/s41597-024-02984-z},
  refid    = {Pawłowska2024},
  url      = {https://doi.org/10.1038/s41597-024-02984-z},
}

@Article{Pawlowska2023,
  author    = {Paw{\l}owska, Anna and Karwat, Piotr and {\.Z}o{\l}ek, Norbert},
  journal   = {Data in Brief},
  title     = {re:“[dataset of breast ultrasound images by w. al-dhabyani, m. gomaa, h. khaled \& a. fahmy, data in brief, 2020, 28, 104863]”},
  year      = {2023},
  volume    = {48},
  comment   = {BUSI-BrEaST beef},
  publisher = {Elsevier},
}

@Article{PiotrzkowskaWroblewska2017,
  author   = {Piotrzkowska-Wróblewska, Hanna and Dobruch-Sobczak, Katarzyna and Byra, Michał and Nowicki, Andrzej},
  journal  = {Medical Physics},
  title    = {Open access database of raw ultrasonic signals acquired from malignant and benign breast lesions},
  year     = {2017},
  number   = {11},
  pages    = {6105-6109},
  volume   = {44},
  abstract = {Purpose The aim of this paper is to provide access to a database consisting of the raw radio-frequency ultrasonic echoes acquired from malignant and benign breast lesions. The database is freely available for study and signal analysis. Acquisition and validation methods The ultrasonic radio-frequency echoes were recorded from breast focal lesions of patients of the Institute of Oncology in Warsaw. The data were collected between 11/2013 and 10/2015. Patients were examined by a radiologist with 18 yr' experience in the ultrasonic examination of breast lesions. The set of data includes scans from 52 malignant and 48 benign breast lesions recorded in a group of 78 women. For each lesion, two individual orthogonal scans from the pathological region were acquired with the Ultrasonix SonixTouch Research ultrasound scanner using the L14-5/38 linear array transducer. All malignant lesions were histologically assessed by core needle biopsy. In the case of benign lesions, part of them was histologically assessed and another part was observed over a 2-year period. Data format and usage notes The radio-frequency echoes were stored in Matlab file format. For each scan, the region of interest was provided to correctly indicate the lesion area. Moreover, for each lesion, the BI-RADS category and the lesion class were included. Two code examples of data manipulation are presented. The data can be downloaded via the Zenodo repository (https://doi.org/10.5281/zenodo.545928) or the website http://bluebox.ippt.gov.pl/~hpiotrzk. Potential applications The database can be used to test quantitative ultrasound techniques and ultrasound image processing algorithms, or to develop computer-aided diagnosis systems.},
  comment  = {OASBUD},
  doi      = {https://doi.org/10.1002/mp.12538},
  eprint   = {https://aapm.onlinelibrary.wiley.com/doi/pdf/10.1002/mp.12538},
  keywords = {breast lesions, dataset, ultrasonic signals, ultrasonography},
  url      = {https://aapm.onlinelibrary.wiley.com/doi/abs/10.1002/mp.12538},
}

@InProceedings{Ribeiro2016,
  author    = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle = {Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},
  title     = {" Why should i trust you?" Explaining the predictions of any classifier},
  year      = {2016},
  pages     = {1135--1144},
  comment   = {Wolf-Husky image prediction snow bias.},
}

@InProceedings{Shah2016,
  author    = {Shah, Anish and Kadam, Eashan and Shah, Hena and Shinde, Sameer and Shingade, Sandip},
  booktitle = {Proceedings of the third international symposium on computer vision and the internet},
  title     = {Deep residual networks with exponential linear unit},
  year      = {2016},
  pages     = {59--65},
  comment   = {BatchNorm ELU. Vanishing Gradients.},
}

@Article{Yu2015,
  author  = {Yu, Fisher and Koltun, Vladlen},
  journal = {arXiv preprint arXiv:1511.07122},
  title   = {Multi-scale context aggregation by dilated convolutions},
  year    = {2015},
  comment = {Dilated convolution.},
}

@InProceedings{Chollet2017,
  author    = {Chollet, Fran{\c{c}}ois},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  title     = {Xception: Deep learning with depthwise separable convolutions},
  year      = {2017},
  pages     = {1251--1258},
  comment   = {DepthWise Convolution},
}

@InProceedings{Mannor2005,
  author  = {Mannor, Shie and Peleg, Dori and Rubinstein, Reuven},
  title   = {The cross entropy method for classification},
  year    = {2005},
  month   = {01},
  pages   = {561-568},
  comment = {CrossEntropy Loss},
  doi     = {10.1145/1102351.1102422},
}

@Article{Reddi2019,
  author  = {Reddi, Sashank J and Kale, Satyen and Kumar, Sanjiv},
  journal = {arXiv preprint arXiv:1904.09237},
  title   = {On the convergence of adam and beyond},
  year    = {2019},
  comment = {AMSGRAD. Adam Variant.},
}

@Article{Bansal2023,
  author  = {Bansal, Satish},
  journal = {International Journal on Recent and Innovation Trends in Computing and Communication},
  title   = {Breast Tumor Recognition by Semantic Segmentation of Multiclass Ultrasound Images},
  year    = {2023},
  month   = {10},
  pages   = {938-946},
  volume  = {11},
  comment = {V-Net. Mal paper. Decir que no es fiable.},
  doi     = {10.17762/ijritcc.v11i9.8986},
}

@Article{Oktay2018,
  author  = {Oktay, Ozan and Schlemper, Jo and Folgoc, Loic Le and Lee, Matthew and Heinrich, Mattias and Misawa, Kazunari and Mori, Kensaku and McDonagh, Steven and Hammerla, Nils Y and Kainz, Bernhard and others},
  journal = {arXiv preprint arXiv:1804.03999},
  title   = {Attention u-net: Learning where to look for the pancreas},
  year    = {2018},
  comment = {Attention U-Net},
}

@Article{Chen2017,
  author  = {Chen, Liang-Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig},
  journal = {arXiv preprint arXiv:1706.05587},
  title   = {Rethinking atrous convolution for semantic image segmentation},
  year    = {2017},
  comment = {DeepLabv3+},
}

@Article{Xu2023,
  author   = {Xu, Meng and Huang, Kuan and Qi, Xiaojun},
  journal  = {IEEE Access},
  title    = {A Regional-Attentive Multi-Task Learning Framework for Breast Ultrasound Image Segmentation and Classification},
  year     = {2023},
  pages    = {5377-5392},
  volume   = {11},
  comment  = {Comparación de métricas BUSI},
  doi      = {10.1109/ACCESS.2023.3236693},
  keywords = {Tumors;Image segmentation;Multitasking;Ultrasonic imaging;Feature extraction;Breast;Biomedical imaging;Multitasking;Regional attention;multi-task learning;segmentation;classification;breast ultrasound},
}

@Article{Amiri2020,
  author    = {Amiri, Mina and Brooks, Rupert and Behboodi, Bahareh and Rivaz, Hassan},
  journal   = {International journal of computer assisted radiology and surgery},
  title     = {Two-stage ultrasound image segmentation using U-Net and test time augmentation},
  year      = {2020},
  pages     = {981--988},
  volume    = {15},
  comment   = {Two Stage. State of the art. Historia.},
  publisher = {Springer},
}

@InProceedings{Xu2021,
  author       = {Xu, Meng and Huang, Kuan and Chen, Qiuxiao and Qi, Xiaojun},
  booktitle    = {2021 IEEE 18th international symposium on biomedical imaging (ISBI)},
  title        = {Mssa-net: Multi-scale self-attention network for breast ultrasound image segmentation},
  year         = {2021},
  organization = {IEEE},
  pages        = {827--831},
  comment      = {MSSA-Net Comparison},
}

@Article{Zhang2023,
  author         = {Zhang, Shuai and Niu, Yanmin},
  journal        = {Bioengineering},
  title          = {LcmUNet: A Lightweight Network Combining CNN and MLP for Real-Time Medical Image Segmentation},
  year           = {2023},
  issn           = {2306-5354},
  number         = {6},
  volume         = {10},
  abstract       = {In recent years, UNet and its improved variants have become the main methods for medical image segmentation. Although these models have achieved excellent results in segmentation accuracy, their large number of network parameters and high computational complexity make it difficult to achieve medical image segmentation in real-time therapy and diagnosis rapidly. To address this problem, we introduce a lightweight medical image segmentation network (LcmUNet) based on CNN and MLP. We designed LcmUNet’s structure in terms of model performance, parameters, and computational complexity. The first three layers are convolutional layers, and the last two layers are MLP layers. In the convolution part, we propose an LDA module that combines asymmetric convolution, depth-wise separable convolution, and an attention mechanism to reduce the number of network parameters while maintaining a strong feature-extraction capability. In the MLP part, we propose an LMLP module that helps enhance contextual information while focusing on local information and improves segmentation accuracy while maintaining high inference speed. This network also covers skip connections between the encoder and decoder at various levels. Our network achieves real-time segmentation results accurately in extensive experiments. With only 1.49 million model parameters and without pre-training, LcmUNet demonstrated impressive performance on different datasets. On the ISIC2018 dataset, it achieved an IoU of 85.19%, 92.07% recall, and 92.99% precision. On the BUSI dataset, it achieved an IoU of 63.99%, 79.96% recall, and 76.69% precision. Lastly, on the Kvasir-SEG dataset, LcmUNet achieved an IoU of 81.89%, 88.93% recall, and 91.79% precision.},
  article-number = {712},
  comment        = {LCM Unet.},
  doi            = {10.3390/bioengineering10060712},
  pubmedid       = {37370643},
  url            = {https://www.mdpi.com/2306-5354/10/6/712},
}

@Article{Byra2020,
  author    = {Byra, Michal and Jarosik, Piotr and Szubert, Aleksandra and Galperin, Michael and Ojeda-Fournier, Haydee and Olson, Linda and O’Boyle, Mary and Comstock, Christopher and Andre, Michael},
  journal   = {Biomedical Signal Processing and Control},
  title     = {Breast mass segmentation in ultrasound with selective kernel U-Net convolutional neural network},
  year      = {2020},
  pages     = {102027},
  volume    = {61},
  comment   = {Sk-U-Net},
  publisher = {Elsevier},
}

@Article{Shareef2022,
  author         = {Shareef, Bryar and Vakanski, Aleksandar and Freer, Phoebe E. and Xian, Min},
  journal        = {Healthcare},
  title          = {ESTAN: Enhanced Small Tumor-Aware Network for Breast Ultrasound Image Segmentation},
  year           = {2022},
  issn           = {2227-9032},
  number         = {11},
  volume         = {10},
  abstract       = {Breast tumor segmentation is a critical task in computer-aided diagnosis (CAD) systems for breast cancer detection because accurate tumor size, shape, and location are important for further tumor quantification and classification. However, segmenting small tumors in ultrasound images is challenging due to the speckle noise, varying tumor shapes and sizes among patients, and the existence of tumor-like image regions. Recently, deep learning-based approaches have achieved great success in biomedical image analysis, but current state-of-the-art approaches achieve poor performance for segmenting small breast tumors. In this paper, we propose a novel deep neural network architecture, namely the Enhanced Small Tumor-Aware Network (ESTAN), to accurately and robustly segment breast tumors. The Enhanced Small Tumor-Aware Network introduces two encoders to extract and fuse image context information at different scales, and utilizes row-column-wise kernels to adapt to the breast anatomy. We compare ESTAN and nine state-of-the-art approaches using seven quantitative metrics on three public breast ultrasound datasets, i.e., BUSIS, Dataset B, and BUSI. The results demonstrate that the proposed approach achieves the best overall performance and outperforms all other approaches on small tumor segmentation. Specifically, the Dice similarity coefficient (DSC) of ESTAN on the three datasets is 0.92, 0.82, and 0.78, respectively; and the DSC of ESTAN on the three datasets of small tumors is 0.89, 0.80, and 0.81, respectively.},
  article-number = {2262},
  comment        = {ESTAN Net},
  doi            = {10.3390/healthcare10112262},
  pubmedid       = {36421586},
  url            = {https://www.mdpi.com/2227-9032/10/11/2262},
}

@Article{Zhang2023a,
  author   = {Zhang, Jie and Zhang, Zhichao and Liu, Hua and Xu, Shiqiang},
  journal  = {IET Image Processing},
  title    = {SaTransformer: Semantic-aware transformer for breast cancer classification and segmentation},
  year     = {2023},
  number   = {13},
  pages    = {3789-3800},
  volume   = {17},
  abstract = {Abstract Breast cancer classification and segmentation play an important role in identifying and detecting benign and malignant breast lesions. However, segmentation and classification still face many challenges: 1) The characteristics of cancer itself, such as fuzzy edges, complex backgrounds, and significant changes in size, shape, and intensity distribution make accurate segment and classification challenges. 2) Existing methods ignore the potential relationship between classification and segmentation tasks, due to the classification and segmentation being treated as two separate tasks. To overcome these challenges, in this paper, a novel Semantic-aware transformer (SaTransformer) for breast cancer classification and segmentation is proposed. Specifically, the SaTransformer enables doing the two takes simultaneously through one unified framework. Unlike existing well-known methods, the segmentation and classification information are semantically interactive, reinforcing each other during feature representation learning and improving the ability of feature representation learning while consuming less memory and computational complexity. The SaTransformer is validated on two publicly available breast cancer datasets – BUSI and UDIAT. Experimental results and quantitative evaluations (accuracy: 97.97\%, precision: 98.20\%, DSC: 86.34\%) demonstrate that the SaTransformer outperforms other state-of-the-art methods.},
  comment  = {SaTransformer benchmark},
  doi      = {https://doi.org/10.1049/ipr2.12897},
  eprint   = {https://ietresearch.onlinelibrary.wiley.com/doi/pdf/10.1049/ipr2.12897},
  keywords = {biomedical imaging, cancer, computer vision, convolutional neural nets, diseases, image classification, image segmentation},
  url      = {https://ietresearch.onlinelibrary.wiley.com/doi/abs/10.1049/ipr2.12897},
}

@Article{Cho2022,
  author   = {Se Woon Cho and Na Rae Baek and Kang Ryoung Park},
  journal  = {Journal of King Saud University - Computer and Information Sciences},
  title    = {Deep Learning-based Multi-stage segmentation method using ultrasound images for breast cancer diagnosis},
  year     = {2022},
  issn     = {1319-1578},
  number   = {10, Part B},
  pages    = {10273-10292},
  volume   = {34},
  abstract = {Globally, breast cancer occurs frequently in women and has the highest mortality rate. Owing to the increased need for a rapid and reliable initial diagnosis of breast cancer, several breast tumor segmentation methods based on ultrasound images have attracted research attention. Most conventional methods use a single network and demonstrate high performance by accurately classifying tumor-containing and normal image pixels. However, tests performed using normal images have revealed the occurrence of many false-positive errors. To address this limitation, this study proposes a multistage-based breast tumor segmentation technique based on the classification and segmentation of ultrasound images. In our method, a breast tumor ensemble classification network (BTEC-Net) is designed to classify whether an ultrasound image contains breast tumors or not. In the segmentation stage, a residual feature selection UNet (RFS-UNet) is used to exclusively segment images classified as abnormal by the BTEC-Net. The proposed multistage segmentation method can be adopted as a fully automated diagnosis system because it can classify images as tumor-containing or normal and effectively specify the breast tumor regions.},
  comment  = {Attention U-Net. Benchmark},
  doi      = {https://doi.org/10.1016/j.jksuci.2022.10.020},
  keywords = {Breast cancer, Ultrasound image, Breast tumor segmentation, BTEC-Net, RFS-UNet},
  url      = {https://www.sciencedirect.com/science/article/pii/S1319157822003718},
}

@InProceedings{Chen2018,
  author    = {Chen, Liang-Chieh and Zhu, Yukun and Papandreou, George and Schroff, Florian and Adam, Hartwig},
  booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
  title     = {Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation},
  year      = {2018},
  month     = {September},
  comment   = {DeepLab v3+},
}

@InProceedings{Zhao2017,
  author    = {Zhao, Hengshuang and Shi, Jianping and Qi, Xiaojuan and Wang, Xiaogang and Jia, Jiaya},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  title     = {Pyramid scene parsing network},
  year      = {2017},
  pages     = {2881--2890},
  comment   = {PSP Net.},
}

@Article{Cho2022a,
  author    = {Cho, Se Woon and Baek, Na Rae and Park, Kang Ryoung},
  journal   = {Journal of King Saud University-Computer and Information Sciences},
  title     = {Deep Learning-based Multi-stage segmentation method using ultrasound images for breast cancer diagnosis},
  year      = {2022},
  number    = {10},
  pages     = {10273--10292},
  volume    = {34},
  comment   = {Benchmark base. RFS-UNet.},
  publisher = {Elsevier},
}

@InProceedings{Kirillov2023,
  author    = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C and Lo, Wan-Yen and others},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
  title     = {Segment anything},
  year      = {2023},
  pages     = {4015--4026},
  comment   = {Segment Anything Model. SAM},
}

@InProceedings{Guo2023,
  author       = {Guo, Hongjiang and Wang, Shengwen and Dang, Hao and Xiao, Kangle and Yang, Yaru and Liu, Wenpei and Liu, Tongtong and Wan, Yiying},
  booktitle    = {2023 China Automation Congress (CAC)},
  title        = {LightBTSeg: A lightweight breast tumor segmentation model using ultrasound images via dual-path joint knowledge distillation},
  year         = {2023},
  organization = {IEEE},
  pages        = {3841--3847},
  comment      = {Lightweight. Student Teacher model.},
}

@InProceedings{Han2020,
  author    = {Han, Kai and Wang, Yunhe and Tian, Qi and Guo, Jianyuan and Xu, Chunjing and Xu, Chang},
  booktitle = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  title     = {Ghostnet: More features from cheap operations},
  year      = {2020},
  pages     = {1580--1589},
}

@InProceedings{Hu2018,
  author    = {Hu, Jie and Shen, Li and Sun, Gang},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  title     = {Squeeze-and-excitation networks},
  year      = {2018},
  pages     = {7132--7141},
  comment   = {Reducción de parámetros.},
}

@Article{Huang2022,
  author  = {Huang, Huimin and Xie, Shiao and Lin, Lanfen and Iwamoto, Yutaro and Han, Xianhua and Chen, Yen-Wei and Tong, Ruofeng},
  journal = {arXiv preprint arXiv:2207.14552},
  title   = {ScaleFormer: revisiting the transformer-based backbones from a scale-wise perspective for medical image segmentation},
  year    = {2022},
  comment = {Transformers en bio medicina. Atención},
}

@Article{Yuan2023,
  author         = {Yuan, Sheng and Qiu, Zhao and Li, Peipei and Hong, Yuqi},
  journal        = {Applied Sciences},
  title          = {RMAU-Net: Breast Tumor Segmentation Network Based on Residual Depthwise Separable Convolution and Multiscale Channel Attention Gates},
  year           = {2023},
  issn           = {2076-3417},
  number         = {20},
  volume         = {13},
  abstract       = {Breast cancer is one of the most common female diseases, posing a great threat to women’s health, and breast ultrasound imaging is a common method for breast cancer diagnosis. In recent years, U-Net and its variants have dominated the medical image segmentation field with their excellent performance. However, the existing U-type segmentation networks have the following problems: (1) the design of the feature extractor is complicated, and the calculation difficulty is increased; (2) the skip connection operation simply combines the features of the encoder and the decoder, without considering both spatial and channel dimensions; (3) during the downsampling phase, the pooling operation results in the loss of feature information. To address the above deficiencies, this paper proposes a breast tumor segmentation network, RMAU-Net, that combines residual depthwise separable convolution and a multi-scale channel attention gate. Specifically, we designed the RDw block, which has a simple structure and a larger sensory field, to overcome the localization problem of convolutional operations. Meanwhile, the MCAG module is designed to correct the low-level features in both spatial and channel dimensions and assist the high-level features to recover the up-sampling and pinpoint non-regular breast tumor features. In addition, this paper used the Patch Merging operation instead of the pooling method to prevent the loss of breast ultrasound image information. Experiments were conducted on two breast ultrasound datasets, Dataset B and BUSI, and the results show that the method in this paper has superior segmentation performance and better generalization.},
  article-number = {11362},
  comment        = {Reducción de parámetros.},
  doi            = {10.3390/app132011362},
  url            = {https://www.mdpi.com/2076-3417/13/20/11362},
}

@Article{Xiao2023,
  author   = {Xiao, Peng and Qin, Zhen and Chen, Dajiang and Zhang, Ning and Ding, Yi and Deng, Fuhu and Qin, Zhiguang and Pang, Minghui},
  journal  = {IEEE Internet of Things Journal},
  title    = {FastNet: A Lightweight Convolutional Neural Network for Tumors Fast Identification in Mobile-Computer-Assisted Devices},
  year     = {2023},
  number   = {11},
  pages    = {9878-9891},
  volume   = {10},
  comment  = {FastNet. Histopatología.},
  doi      = {10.1109/JIOT.2023.3235651},
  keywords = {Histopathology;Tumors;Neural networks;Internet of Things;Deep learning;Computational modeling;Transformers;Histopathology;Internet of Things (IoT);lightweight;mobile-computer-assisted devices;tumor detection},
}

@Article{Siegel2019,
  author   = {Siegel, Rebecca L. and Miller, Kimberly D. and Jemal, Ahmedin},
  journal  = {CA: A Cancer Journal for Clinicians},
  title    = {Cancer statistics, 2019},
  year     = {2019},
  number   = {1},
  pages    = {7-34},
  volume   = {69},
  abstract = {Abstract Each year, the American Cancer Society estimates the numbers of new cancer cases and deaths that will occur in the United States and compiles the most recent data on cancer incidence, mortality, and survival. Incidence data, available through 2015, were collected by the Surveillance, Epidemiology, and End Results Program; the National Program of Cancer Registries; and the North American Association of Central Cancer Registries. Mortality data, available through 2016, were collected by the National Center for Health Statistics. In 2019, 1,762,450 new cancer cases and 606,880 cancer deaths are projected to occur in the United States. Over the past decade of data, the cancer incidence rate (2006-2015) was stable in women and declined by approximately 2\% per year in men, whereas the cancer death rate (2007-2016) declined annually by 1.4\% and 1.8\%, respectively. The overall cancer death rate dropped continuously from 1991 to 2016 by a total of 27\%, translating into approximately 2,629,200 fewer cancer deaths than would have been expected if death rates had remained at their peak. Although the racial gap in cancer mortality is slowly narrowing, socioeconomic inequalities are widening, with the most notable gaps for the most preventable cancers. For example, compared with the most affluent counties, mortality rates in the poorest counties were 2-fold higher for cervical cancer and 40\% higher for male lung and liver cancers during 2012-2016. Some states are home to both the wealthiest and the poorest counties, suggesting the opportunity for more equitable dissemination of effective cancer prevention, early detection, and treatment strategies. A broader application of existing cancer control knowledge with an emphasis on disadvantaged groups would undoubtedly accelerate progress against cancer.},
  comment  = {Cancer stats.},
  doi      = {https://doi.org/10.3322/caac.21551},
  eprint   = {https://acsjournals.onlinelibrary.wiley.com/doi/pdf/10.3322/caac.21551},
  keywords = {cancer cases, cancer statistics, death rates, incidence, mortality},
  url      = {https://acsjournals.onlinelibrary.wiley.com/doi/abs/10.3322/caac.21551},
}

@Article{RegistrosdelCancer2019,
  author  = {Red Espa{\~n}ola de Registros del C{\'a}ncer},
  journal = {Redecan},
  title   = {Estimaciones de la incidencia del c{\'a}ncer en Espa{\~n}a 2019},
  year    = {2019},
  pages   = {1--14},
  volume  = {19},
  comment = {Cancer statistics},
}

@Article{Eigen2013,
  author  = {Eigen, David and Ranzato, Marc'Aurelio and Sutskever, Ilya},
  journal = {arXiv preprint arXiv:1312.4314},
  title   = {Learning factored representations in a deep mixture of experts},
  year    = {2013},
  comment = {Mixture of Experts MoE},
}

@Article{Seele2022,
  author    = {Seele, Peter and Schultz, Mario D},
  journal   = {Journal of Business Ethics},
  title     = {From greenwashing to machinewashing: a model and future directions derived from reasoning by analogy},
  year      = {2022},
  number    = {4},
  pages     = {1063--1089},
  volume    = {178},
  comment   = {greenwashing},
  publisher = {Springer},
}

@Article{Paszke2016,
  author  = {Paszke, Adam and Chaurasia, Abhishek and Kim, Sangpil and Culurciello, Eugenio},
  journal = {arXiv preprint arXiv:1606.02147},
  title   = {Enet: A deep neural network architecture for real-time semantic segmentation},
  year    = {2016},
}

@Article{Vaswani2017,
  author  = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal = {Advances in neural information processing systems},
  title   = {Attention is all you need},
  year    = {2017},
  volume  = {30},
}

@Article{Dosovitskiy2020,
  author  = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal = {arXiv preprint arXiv:2010.11929},
  title   = {An image is worth 16x16 words: Transformers for image recognition at scale},
  year    = {2020},
  comment = {Vision Transformer, ViT},
}

@Article{Hassani2022,
  author  = {Hassani, Ali and Shi, Humphrey},
  journal = {arXiv preprint arXiv:2209.15001},
  title   = {Dilated neighborhood attention transformer},
  year    = {2022},
}

@InProceedings{Cao2023,
  author    = {Cao, Hu and Wang, Yueyue and Chen, Joy and Jiang, Dongsheng and Zhang, Xiaopeng and Tian, Qi and Wang, Manning},
  booktitle = {Computer Vision -- ECCV 2022 Workshops},
  title     = {Swin-Unet: Unet-Like Pure Transformer for Medical Image Segmentation},
  year      = {2023},
  address   = {Cham},
  editor    = {Karlinsky, Leonid and Michaeli, Tomer and Nishino, Ko},
  pages     = {205--218},
  publisher = {Springer Nature Switzerland},
  abstract  = {In the past few years, convolutional neural networks (CNNs) have achieved milestones in medical image analysis. In particular, deep neural networks based on U-shaped architecture and skip-connections have been widely applied in various medical image tasks. However, although CNN has achieved excellent performance, it cannot learn global semantic information interaction well due to the locality of convolution operation. In this paper, we propose Swin-Unet, which is an Unet-like pure Transformer for medical image segmentation. The tokenized image patches are fed into the Transformer-based U-shaped Encoder-Decoder architecture with skip-connections for local-global semantic feature learning. Specifically, we use a hierarchical Swin Transformer with shifted windows as the encoder to extract context features. And a symmetric Swin Transformer-based decoder with a patch expanding layer is designed to perform the up-sampling operation to restore the spatial resolution of the feature maps. Under the direct down-sampling and up-sampling of the inputs and outputs by {\$}{\$}4{\{}{\backslash}times {\}}{\$}{\$}4{\texttimes}, experiments on multi-organ and cardiac segmentation tasks demonstrate that the pure Transformer-based U-shaped Encoder-Decoder network outperforms those methods with full-convolution or the combination of transformer and convolution. The codes have been publicly available at the link (https://github.com/HuCaoFighting/Swin-Unet).},
  isbn      = {978-3-031-25066-8},
}

@Article{Xie2021,
  author  = {Xie, Enze and Wang, Wenhai and Yu, Zhiding and Anandkumar, Anima and Alvarez, Jose M and Luo, Ping},
  journal = {Advances in neural information processing systems},
  title   = {SegFormer: Simple and efficient design for semantic segmentation with transformers},
  year    = {2021},
  pages   = {12077--12090},
  volume  = {34},
}

@Article{Li2018,
  author  = {Li, Hanchao and Xiong, Pengfei and An, Jie and Wang, Lingxue},
  journal = {arXiv preprint arXiv:1805.10180},
  title   = {Pyramid attention network for semantic segmentation},
  year    = {2018},
  comment = {Pyramid Vit},
}

@Article{Wu2024,
  author  = {Wu, Weiyi and Gao, Chongyang and Xu, Xinwen and Li, Siting and Gui, Jiang},
  journal = {arXiv preprint arXiv:2406.09333},
  title   = {Memory-Efficient Sparse Pyramid Attention Networks for Whole Slide Image Analysis},
  year    = {2024},
}

@Article{Hatamizadeh2023,
  author  = {Hatamizadeh, Ali and Heinrich, Greg and Yin, Hongxu and Tao, Andrew and Alvarez, Jose M and Kautz, Jan and Molchanov, Pavlo},
  journal = {arXiv preprint arXiv:2306.06189},
  title   = {Fastervit: Fast vision transformers with hierarchical attention},
  year    = {2023},
  comment = {Vit. Efficient. Fast. Attention},
}

@Article{Akkus2023,
  author  = {Akkus, Cem and Chu, Luyang and Djakovic, Vladana and Jauch-Walser, Steffen and Koch, Philipp and Loss, Giacomo and Marquardt, Christopher and Moldovan, Marco and Sauter, Nadja and Schneider, Maximilian and others},
  journal = {arXiv preprint arXiv:2301.04856},
  title   = {Multimodal deep learning},
  year    = {2023},
}

@Article{Shang2024,
  author  = {Shang, Chenming and Zhang, Hengyuan and Wen, Hao and Yang, Yujiu},
  journal = {arXiv preprint arXiv:2404.08964},
  title   = {Understanding Multimodal Deep Neural Networks: A Concept Selection View},
  year    = {2024},
}

@Article{Park2023,
  author    = {Park, Eunil},
  journal   = {Journal of big Data},
  title     = {CRNet: a multimodal deep convolutional neural network for customer revisit prediction},
  year      = {2023},
  number    = {1},
  pages     = {1},
  volume    = {10},
  publisher = {Springer},
}

@Article{Camilus2012,
  author    = {Camilus, K Santle and Govindan, VK},
  journal   = {International Journal of Image, Graphics and Signal Processing},
  title     = {A review on graph based segmentation},
  year      = {2012},
  number    = {5},
  pages     = {1},
  volume    = {4},
  publisher = {Modern Education and Computer Science Press},
}

@InProceedings{Yang2020,
  author    = {Yang, Fengting and Sun, Qian and Jin, Hailin and Zhou, Zihan},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Superpixel Segmentation With Fully Convolutional Networks},
  year      = {2020},
  month     = {June},
}

@InProceedings{Pavlitskaya2020,
  author    = {Pavlitskaya, Svetlana and Hubschneider, Christian and Weber, Michael and Moritz, Ruby and Huger, Fabian and Schlicht, Peter and Zollner, Marius},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops},
  title     = {Using mixture of expert models to gain insights into semantic segmentation},
  year      = {2020},
  pages     = {342--343},
  comment   = {Segmentation MoE},
}

@InProceedings{He2019,
  author    = {He, Junjun and Deng, Zhongying and Zhou, Lei and Wang, Yali and Qiao, Yu},
  booktitle = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  title     = {Adaptive pyramid context network for semantic segmentation},
  year      = {2019},
  pages     = {7519--7528},
}

@InProceedings{Huo2023,
  author    = {Huo, Xinyue and Xie, Lingxi and Zhou, Wengang and Li, Houqiang and Tian, Qi},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  title     = {Focus on Your Target: A Dual Teacher-Student Framework for Domain-Adaptive Semantic Segmentation},
  year      = {2023},
  month     = {October},
  pages     = {19027-19038},
}

@Article{Ho2020,
  author  = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  journal = {Advances in neural information processing systems},
  title   = {Denoising diffusion probabilistic models},
  year    = {2020},
  pages   = {6840--6851},
  volume  = {33},
}

@InProceedings{Esser2024,
  author    = {Esser, Patrick and Kulal, Sumith and Blattmann, Andreas and Entezari, Rahim and M{\"u}ller, Jonas and Saini, Harry and Levi, Yam and Lorenz, Dominik and Sauer, Axel and Boesel, Frederic and others},
  booktitle = {Forty-first International Conference on Machine Learning},
  title     = {Scaling rectified flow transformers for high-resolution image synthesis},
  year      = {2024},
  comment   = {Stable-Diffusion},
}

@Article{Betker2023,
  author  = {Betker, James and Goh, Gabriel and Jing, Li and Brooks, Tim and Wang, Jianfeng and Li, Linjie and Ouyang, Long and Zhuang, Juntang and Lee, Joyce and Guo, Yufei and others},
  journal = {Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf},
  title   = {Improving image generation with better captions},
  year    = {2023},
  number  = {3},
  pages   = {8},
  volume  = {2},
  comment = {Dalle-3},
}

@Article{Goodfellow2014,
  author  = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  journal = {Advances in neural information processing systems},
  title   = {Generative adversarial nets},
  year    = {2014},
  volume  = {27},
}

@Article{Loshchilov2017,
  author  = {Loshchilov, Ilya and Hutter, Frank},
  journal = {arXiv preprint arXiv:1711.05101},
  title   = {Decoupled weight decay regularization},
  year    = {2017},
}

@Article{Defazio2023,
  author  = {Defazio, Aaron and Cutkosky, Ashok and Mehta, Harsh and Mishchenko, Konstantin},
  journal = {arXiv preprint arXiv:2310.07831},
  title   = {When, why and how much? adaptive learning rate scheduling by refinement},
  year    = {2023},
}

@Article{Bharati2022,
  author    = {Bharati, Subrato and Mondal, M and Podder, Prajoy and Prasath, VB},
  journal   = {International Journal of Hybrid Intelligent Systems},
  title     = {Federated learning: Applications, challenges and future directions},
  year      = {2022},
  number    = {1-2},
  pages     = {19--35},
  volume    = {18},
  publisher = {IOS Press},
}

@InProceedings{Bretto2005,
  author  = {Bretto, Alain and Gillibert, Luc},
  title   = {Hypergraph-Based Image Representation},
  year    = {2005},
  month   = {04},
  pages   = {1-11},
  volume  = {3434},
  doi     = {10.1007/978-3-540-31988-7_1},
  isbn    = {978-3-540-25270-2},
  journal = {Lecture Notes in Computer Science},
}

@Article{BuenestadoCortes2022,
  author    = {Buenestado Cort{\'e}s, Miguel},
  title     = {Aprendizaje Federado Aplicado al Diagn{\'o}stico de Tumores Mamarios En Im{\'a}genes de Ultrasonido},
  year      = {2022},
  month     = feb,
  abstract  = {En el campo de la medicina, el aprendizaje autom{\'a}tico se erige como una herramienta poderosa y eficaz en la automatizaci{\'o}n de la tarea del diagn{\'o}stico por imagen, mediante la creaci{\'o}n de modelos inform{\'a}ticos predictivos. Para lograr resultados con un alto grado de fiabilidad, dichos modelos requieren de grandes vol{\'u}menes de datos para su aprendizaje, caracter{\'i}stica poco frecuente en contextos reales, pues organismos e instituciones m{\'e}dicas, individualmente, no suelen disponer de tales cantidades de informaci{\'o}n. Idealmente, {\'e}sta podr{\'i}a ser compartida y cedida mutuamente en busca de un fin com{\'u}n, pero esto resulta altamente improbable, dadas las regulaciones imperantes en la actualidad en torno a la propiedad intelectual y la privacidad de datos m{\'e}dicos de pacientes. A fin de superar tales escollos, surge el paradigma del aprendizaje federado (federated learning). En este Trabajo Fin de Master, aplicamos dicho paradigma en un caso de aplicaci{\'o}n concreto (detecci{\'o}n de tumores mamarios en im{\'a}genes de ultrasonidos), empleando tecnolog{\'i}a y t{\'e}cnicas espec{\'i}ficas de federaci{\'o}n (la librer{\'i}a TensorFlow Federated; la agregaci{\'o}n federada est{\'a}ndar, Federated Average, y variantes de la misma), con las que entrenamos modelos predictivos sobre diferentes combinaciones de datasets. Evaluaremos la idoneidad y eficacia de dichos modelos federados, tanto por s{\'i} mismos como en comparaci{\'o}n con resultados an{\'a}logos derivados de un aprendizaje cl{\'a}sico. Exponemos que el aprendizaje federado puede alcanzar, bajo ciertas configuraciones, cotas de eficacia similares a las de un aprendizaje cl{\'a}sico. El presente documento se articula, primero, con la introducci{\'o}n de los conceptos clave tocantes al propio caso de estudio: el problema a resolver en cuesti{\'o}n (segmentaci{\'o}n de im{\'a}genes de ultrasonido), los aspectos concretos del aprendizaje autom{\'a}tico que han ayudado a resolverlo (modelo de aprendizaje profundo, una red neuronal convolutiva U-Net) y profundizaremos en los fundamentos del paradigma de aprendizaje federado. Posteriormente, se detallar{\'a} la metodolog{\'i}a aplicada durante el desarrollo del proyecto y se expondr{\'a}n los resultados derivados del mismo. Finalmente, abordaremos las conclusiones de dichos resultados, as{\'i} como las limitaciones y posibles mejoras en el {\'a}mbito del presente trabajo y del aprendizaje federado, en particular.},
  publisher = {Universidad Nacional de Educaci{\'o}n a Distancia (Espa{\~n}a). Escuela T{\'e}cnica Superior de Ingenier{\'i}a Inform{\'a}tica. Departamento de Inteligencia Artificial},
  urldate   = {2022-11-20},
}

@Article{Ma2024,
  author    = {Ma, Jun and He, Yuting and Li, Feifei and Han, Lin and You, Chenyu and Wang, Bo},
  journal   = {Nature Communications},
  title     = {Segment anything in medical images},
  year      = {2024},
  number    = {1},
  pages     = {654},
  volume    = {15},
  comment   = {MedSAM},
  publisher = {Nature Publishing Group UK London},
}

@Article{Hinton2006,
  author    = {Hinton, G. E. and Salakhutdinov, R. R.},
  journal   = {Science},
  title     = {Reducing the Dimensionality of Data with Neural Networks},
  year      = {2006},
  issn      = {1095-9203},
  month     = jul,
  number    = {5786},
  pages     = {504--507},
  volume    = {313},
  comment   = {Fine-tuning},
  doi       = {10.1126/science.1127647},
  publisher = {American Association for the Advancement of Science (AAAS)},
}

@Article{Tan2023,
  author  = {Tan, Weimin and Chen, Siyuan and Yan, Bo},
  journal = {arXiv preprint arXiv:2307.00773},
  title   = {Diffss: Diffusion model for few-shot semantic segmentation},
  year    = {2023},
  comment = {Diffusion in segmentation},
}

@Article{Sarker2021,
  author    = {Sarker, Md Mostafa Kamal and Rashwan, Hatem A and Akram, Farhan and Singh, Vivek Kumar and Banu, Syeda Furruka and Chowdhury, Forhad UH and Choudhury, Kabir Ahmed and Chambon, Sylvie and Radeva, Petia and Puig, Domenec and others},
  journal   = {Expert Systems with Applications},
  title     = {SLSNet: Skin lesion segmentation using a lightweight generative adversarial network},
  year      = {2021},
  pages     = {115433},
  volume    = {183},
  comment   = {GAN lightweight},
  publisher = {Elsevier},
}

@Comment{jabref-meta: databaseType:bibtex;}
