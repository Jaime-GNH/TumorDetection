@IEEEtranBSTCTL{Hu2020,
  author   = {Hu, Xuegang and Wang, Haibo},
  doi      = {10.1109/ACCESS.2020.2987080},
  journal  = {IEEE Access},
  keywords = {Semantics;Image segmentation;Feature extraction;Decoding;Real-time systems;Image resolution;Performance evaluation;Resource-constrained;semantic segmentation;continuous shuffle dilated convolution;real-time},
  pages    = {70913-70924},
  title    = {Efficient Fast Semantic Segmentation Using Continuous Shuffle Dilated Convolutions},
  volume   = {8},
  year     = {2020},
}

@IEEEtranBSTCTL{paszke2016enet,
  author  = {Paszke, Adam and Chaurasia, Abhishek and Kim, Sangpil and Culurciello, Eugenio},
  journal = {arXiv preprint arXiv:1606.02147},
  title   = {Enet: A deep neural network architecture for real-time semantic segmentation},
  year    = {2016},
}

@IEEEtranBSTCTL{Zhang2019,
  abstract = {Semantic segmentation is a challenging problem in computer vision. Many applications, such as autonomous driving and robot navigation with urban road scene, need accurate and efficient segmentation. Most state-of-the-art methods focus on accuracy, rather than efficiency. In this paper, we propose a more efficient neural network architecture, which has fewer parameters, for semantic segmentation in the urban road scene. An asymmetric encoder-decoder structure based on ResNet is used in our model. In the first stage of encoder, we use continuous factorized block to extract low-level features. Continuous dilated block is applied in the second stage, which ensures that the model has a larger view field, while keeping the model small-scale and shallow. The down sampled features from encoder are up sampled with decoder to the same-size output as the input image and the details refined. Our model can achieve end-to-end and pixel-to-pixel training without pretraining from scratch. The parameters of our model are only 0.2M, 100× less than those of others such as SegNet, etc. Experiments are conducted on five public road scene datasets (CamVid, CityScapes, Gatech, KITTI Road Detection, and KITTI Semantic Segmentation), and the results demonstrate that our model can achieve better performance.},
  author   = {Zhang, Xuetao and Chen, Zhenxue and Wu, Q. M. Jonathan and Cai, Lei and Lu, Dan and Li, Xianming},
  doi      = {10.1109/TII.2018.2849348},
  issn     = {1941-0050},
  journal  = {IEEE Transactions on Industrial Informatics},
  keywords = {Semantics;Convolution;Image segmentation;Standards;Informatics;Computer vision;Computer architecture;Convolutional neural network (CNN);real-time;ResNet;scene perception;semantic segmentation},
  month    = {Feb},
  number   = {2},
  pages    = {1183-1192},
  title    = {Fast Semantic Segmentation for Scene Perception},
  volume   = {15},
  year     = {2019},
}

@Article{ronneberger2015u,
  author  = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  journal = {arXiv preprint arXiv:1505.04597},
  title   = {U-Net: Convolutional networks for biomedical image segmentation. arXiv 2015},
  year    = {2015},
}

@Article{7803544,
  author   = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title    = {SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation},
  year     = {2017},
  issn     = {1939-3539},
  month    = {Dec},
  number   = {12},
  pages    = {2481-2495},
  volume   = {39},
  abstract = {We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1] . The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN [2] and also with the well known DeepLab-LargeFOV [3] , DeconvNet [4] architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures and can be trained end-to-end using stochastic gradient descent. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. These quantitative assessments show that SegNet provides good performance with competitive inference time and most efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.},
  doi      = {10.1109/TPAMI.2016.2644615},
  keywords = {Decoding;Neural networks;Training;Computer architecture;Image segmentation;Semantics;Convolutional codes;Deep convolutional neural networks;semantic pixel-wise segmentation;indoor scenes;road scenes;encoder;decoder;pooling;upsampling},
}

@InProceedings{7780459,
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Deep Residual Learning for Image Recognition},
  year      = {2016},
  month     = {June},
  pages     = {770-778},
  abstract  = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  doi       = {10.1109/CVPR.2016.90},
  issn      = {1063-6919},
  keywords  = {Training;Degradation;Complexity theory;Image recognition;Neural networks;Visualization;Image segmentation},
}

@InProceedings{8578814,
  author    = {Zhang, Xiangyu and Zhou, Xinyu and Lin, Mengxiao and Sun, Jian},
  booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  title     = {ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices},
  year      = {2018},
  month     = {June},
  pages     = {6848-6856},
  abstract  = {We introduce an extremely computation-efficient CNN architecture named ShuffleNet, which is designed specially for mobile devices with very limited computing power (e.g., 10-150 MFLOPs). The new architecture utilizes two new operations, pointwise group convolution and channel shuffle, to greatly reduce computation cost while maintaining accuracy. Experiments on ImageNet classification and MS COCO object detection demonstrate the superior performance of ShuffleNet over other structures, e.g. lower top-1 error (absolute 7.8%) than recent MobileNet [12] on ImageNet classification task, under the computation budget of 40 MFLOPs. On an ARM-based mobile device, ShuffleNet achieves ~13× actual speedup over AlexNet while maintaining comparable accuracy.},
  doi       = {10.1109/CVPR.2018.00716},
  issn      = {2575-7075},
  keywords  = {Convolution;Complexity theory;Computer architecture;Mobile handsets;Computational modeling;Task analysis;Neural networks},
}

@InProceedings{Ma2018,
  author    = {Ma, Ningning and Zhang, Xiangyu and Zheng, Hai-Tao and Sun, Jian},
  booktitle = {Computer Vision -- ECCV 2018},
  title     = {ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design},
  year      = {2018},
  address   = {Cham},
  editor    = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
  pages     = {122--138},
  publisher = {Springer International Publishing},
  abstract  = {Currently, the neural network architecture design is mostly guided by the indirect metric of computation complexity, i.e., FLOPs. However, the direct metric, e.g., speed, also depends on the other factors such as memory access cost and platform characterics. Thus, this work proposes to evaluate the direct metric on the target platform, beyond only considering FLOPs. Based on a series of controlled experiments, this work derives several practical guidelines for efficient network design. Accordingly, a new architecture is presented, called ShuffleNet V2. Comprehensive ablation experiments verify that our model is the state-of-the-art in terms of speed and accuracy tradeoff.},
  isbn      = {978-3-030-01264-9},
}

@Comment{jabref-meta: databaseType:bibtex;}
